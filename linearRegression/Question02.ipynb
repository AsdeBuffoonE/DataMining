{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "((50, 500), (50, 1))"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 导入数据\n",
    "x_all = pd.read_csv('data/random_data_regression_X.csv', header=None)\n",
    "y_all = pd.read_csv('data/random_data_regression_y.csv', header=None)\n",
    "train_x = x_all[:40]\n",
    "train_y = y_all[:40]\n",
    "test_x = x_all[40:]\n",
    "test_y = y_all[40:]\n",
    "(x_all.shape, y_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# 封装梯度下降法(包含正则化项)\n",
    "class GradientDescentLinerRegressionWithRegularization:\n",
    "    def __init__(self, learning_rate=0.01, max_iter=1000, _lambda=0.1, early_stop=True, seed=None):\n",
    "        np.random.seed(seed)\n",
    "        self.early_stop = early_stop\n",
    "        self._lambda = _lambda\n",
    "        self.lr = learning_rate  # 设置学习率\n",
    "        self.max_iter = max_iter\n",
    "        self.loss_arr = []\n",
    "        self.intercept_ = None\n",
    "        self.coef_ = None\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.x = x\n",
    "        self.x_b = np.hstack([np.ones((len(x), 1)), x])\n",
    "        self.x_b_dim = np.size(self.x_b, 1)\n",
    "        self.x_sample = np.size(self.x_b, 0)\n",
    "        self.y = y\n",
    "        self.w = np.random.normal(1, 0.001, (self.x_b_dim, 1))\n",
    "        for i in range(self.max_iter):\n",
    "            self._train_step()\n",
    "            self.loss_arr.append(self.loss())\n",
    "            print(\"iter: {}, loss_in_train: {:.5f}, loss_in_test: {:.5f}\".format(i + 1, self.loss(is_test=False), self.loss_arr[-1]))\n",
    "\n",
    "            # 连续十次不下降则退出\n",
    "            if self.early_stop and len(self.loss_arr) > 10:\n",
    "                arr = np.array(self.loss_arr[:-10])\n",
    "                if np.argmin(arr) == 0:\n",
    "                    return\n",
    "\n",
    "    def _f(self, x, w):\n",
    "        return x.dot(w)\n",
    "\n",
    "    def predict(self, x=None):\n",
    "        if x is None:\n",
    "            x = self.x_b\n",
    "        x = np.hstack([np.ones((len(x), 1)), x])\n",
    "        y_pred = self._f(x, self.w)\n",
    "        return y_pred\n",
    "\n",
    "    def loss(self, is_test=True, y_true=None, y_pred=None):\n",
    "        if y_true is None or y_pred is None:\n",
    "            y_true = self.y\n",
    "        if is_test:\n",
    "            return np.sqrt(mean_squared_error(test_y, self.predict(test_x)))\n",
    "        else:\n",
    "            return np.sqrt(mean_squared_error(y_true, self.predict(self.x)))\n",
    "\n",
    "    def _calc_gradient(self):\n",
    "        d_w = np.empty(self.x_b_dim).reshape(-1, 1)\n",
    "        d_w[0] = np.sum(self.x_b.dot(self.w) - self.y)\n",
    "        for i in range(1, self.x_b_dim):\n",
    "            # print(\"{} {} {} {}\".format(self.x_b.shape, self.w.shape, self.y.shape, self.x_b[:, i].T.shape))\n",
    "            d_w[i] = np.squeeze((self.x_b.dot(self.w) - self.y)).dot(self.x_b[:, i].T)\n",
    "        return d_w * 2 / self.x_sample + (self._lambda / self.x_sample * self.w)\n",
    "\n",
    "    def _train_step(self):\n",
    "        d_w = self._calc_gradient()\n",
    "        self.w = self.w - self.lr * d_w\n",
    "        self.intercept_ = self.w[0]\n",
    "        self.coef_ = self.w[1:]\n",
    "        return self.w"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1, loss_in_train: 30.80939, loss_in_test: 35.08037\n",
      "iter: 2, loss_in_train: 30.80125, loss_in_test: 35.08077\n",
      "iter: 3, loss_in_train: 30.79312, loss_in_test: 35.08118\n",
      "iter: 4, loss_in_train: 30.78499, loss_in_test: 35.08158\n",
      "iter: 5, loss_in_train: 30.77686, loss_in_test: 35.08198\n",
      "iter: 6, loss_in_train: 30.76874, loss_in_test: 35.08238\n",
      "iter: 7, loss_in_train: 30.76061, loss_in_test: 35.08278\n",
      "iter: 8, loss_in_train: 30.75249, loss_in_test: 35.08319\n",
      "iter: 9, loss_in_train: 30.74437, loss_in_test: 35.08359\n",
      "iter: 10, loss_in_train: 30.73626, loss_in_test: 35.08399\n",
      "iter: 11, loss_in_train: 30.72815, loss_in_test: 35.08439\n",
      "运行时间:  1.5480129718780518\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()  # 记录开始时间\n",
    "reg = GradientDescentLinerRegressionWithRegularization(learning_rate=1e-5, max_iter=30, _lambda=100, early_stop=True, seed=1024)\n",
    "reg.fit(train_x, train_y)\n",
    "print(\"运行时间: \", time.time() - time_start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1, loss_in_train: 15989449595.48327, loss_in_test: 19107178862.43552\n",
      "iter: 2, loss_in_train: 2754287787120579.50000, loss_in_test: 3291337144231060.50000\n",
      "iter: 3, loss_in_train: 474444174515207274496.00000, loss_in_test: 566954456156685139968.00000\n",
      "iter: 4, loss_in_train: 81726127452621944486100992.00000, loss_in_test: 97661631510259090965135360.00000\n",
      "iter: 5, loss_in_train: 14077862617297495002779842772992.00000, loss_in_test: 16822857930954745121414521552896.00000\n",
      "iter: 6, loss_in_train: 2425004365787357994633132608973701120.00000, loss_in_test: 2897847850671608891980680261953126400.00000\n",
      "iter: 7, loss_in_train: 417722940900288747644712018932248529076224.00000, loss_in_test: 499173339043081383236297770412926434279424.00000\n",
      "iter: 8, loss_in_train: 71955522149227679295953406822923933966047117312.00000, loss_in_test: 85985888580613404241229189804493876341613002752.00000\n",
      "iter: 9, loss_in_train: 12394811634259514926228986790270247729448618234478592.00000, loss_in_test: 14811634469844070180216588336522641421453646289174528.00000\n",
      "iter: 10, loss_in_train: 2135087771723215521788934570310579932503942857201226350592.00000, loss_in_test: 2551401390271100699179036295709004287319342092019268845568.00000\n",
      "iter: 11, loss_in_train: 367782902029905916621925991745011046844763357766623203055108096.00000, loss_in_test: 439495659140840290904777868431651247692750832304815460921114624.00000\n",
      "iter: 12, loss_in_train: 63353022211526421711192289342704961349240659623810524333272767397888.00000, loss_in_test: 75706015972311401455426645543666017831784877321390462652966760349696.00000\n",
      "iter: 13, loss_in_train: 10912974478100670100770795117150434286653248419341552814069169806372765696.00000, loss_in_test: 13040858846260398563658636318694672351302901468918579127076645524199178240.00000\n",
      "iter: 14, loss_in_train: 1879831581862701582188551449023785795959653042635928937133430391640424598994944.00000, loss_in_test: 2246373650282786519328681135369180883486431672146731930134075420600398818312192.00000\n",
      "iter: 15, loss_in_train: 323813345596997562740286797067456727707055188315855841960254222200726375040339148800.00000, loss_in_test: 386952626063571012244451831203259996362829644209031610919718734424105011665000464384.00000\n",
      "iter: 16, loss_in_train: 55778977116035604035919129206516270486713289544255425077971268715692186250449594559234048.00000, loss_in_test: 66655133173701788402459259303517842327519342439530920433266218610193411665272145002364928.00000\n",
      "iter: 17, loss_in_train: 9608295428266222872218593392613522681607548200539372292241885097604714685338558224792031330304.00000, loss_in_test: 11481784795211639907165930242711590697700349583397272180132806389559026777625271434251276910592.00000\n",
      "iter: 18, loss_in_train: 1655092040228560823778951911141813913433453130501772084763056449203153571212309087328225641401155584.00000, loss_in_test: 1977812897620404139444675977776281459523040439626003734286364689256561389044195986343932229010325504.00000\n",
      "iter: 19, loss_in_train: 285100482398701873379282398526769512791448629124104963357076715612344509982052356328562308996091074314240.00000, loss_in_test: 340691271240772118736522816425248096958102269490707748579740790724626618138263690904105404938026758438912.00000\n",
      "iter: 20, loss_in_train: 49110431981020056208126511023811896135868485396241067044015085542772232103244988067834835659363393011355484160.00000, loss_in_test: 58686310742185471123194788324344728210151726223771750699374999183536036021780754716864080523181006082552102912.00000\n",
      "iter: 21, loss_in_train: 8459594698228335973834780186028381602429634146191107590267819844284725331133981576723301452805577779125823054282752.00000, loss_in_test: 10109102754482856057824021371398948957117974441387237278626233593578391745961425111944213044841913622949791658934272.00000\n",
      "iter: 22, loss_in_train: 1457220789382404726591259169968824425816076372472610897230859136469335659207071679774527108384733745605558913154664103936.00000, loss_in_test: 1741359393839571063118269743356275755678567601469257970129048800589580852657006774387261806302943889982919502489853100032.00000\n",
      "iter: 23, loss_in_train: 251015858886595896132555770833036717965253244191151890256675364696528429655664989624803644131054259018342988706634901844131840.00000, loss_in_test: 299960601070024687023968471652248423581848271110520060616340313186526893720913291712249454514167365161927462195934320608149504.00000\n",
      "iter: 24, loss_in_train: 43239131552110034313025550963611288194649483147060507003624289020184431194017815612363496386864619164683288419351139040662841720832.00000, loss_in_test: 51670185093669345665449223335297348720294964817885938212428855688612326524509267932559697508501307010668619030797896259797826666496.00000\n",
      "iter: 25, loss_in_train: 7448224608889500392531475443762179290514244383497935302268095180883363321198827162315781703615109935484668190952626811096603127770513408.00000, loss_in_test: 8900528996442415513852856776048407766048289090583433834956332909584274551794005752797473618543466290214396584888402007101079547537260544.00000\n",
      "iter: 26, loss_in_train: 1283005644033568462673194695874160228079010308912282530069625617547621701803925831249999276459837285220762740338991946120938799779573055619072.00000, loss_in_test: 1533174620390863970047836969871597681949588485631632201784518565165434325926323745075418169591623016668980525872365829591389623863106118615040.00000\n",
      "iter: 27, loss_in_train: 221006155031543735440642869700133516740539003175820159775962453509914117934881142325454641042517835747547172509607746091644965017518795343311929344.00000, loss_in_test: 264099405501653461245115419414009804542023382871425235748846148588683301968777075588415329256601316410414435580659084543384533427539394440185511936.00000\n",
      "iter: 28, loss_in_train: 38069762817464901509289729991242199040604503873396417532375118184264015666492872463441813099016198853837997236247923418593231614392340780533825769832448.00000, loss_in_test: 45492858451143215631058691947356783614774873305046982475056364238063625988022926972732269553500030231239740037479378426550341465791132667378470469238784.00000\n",
      "iter: 29, loss_in_train: inf, loss_in_test: inf\n",
      "iter: 30, loss_in_train: inf, loss_in_test: inf\n",
      "运行时间:  4.113682985305786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owemshu/opt/anaconda3/envs/deeplearning/lib/python3.9/site-packages/sklearn/metrics/_regression.py:446: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n",
      "/Users/owemshu/opt/anaconda3/envs/deeplearning/lib/python3.9/site-packages/sklearn/metrics/_regression.py:446: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n",
      "/Users/owemshu/opt/anaconda3/envs/deeplearning/lib/python3.9/site-packages/sklearn/metrics/_regression.py:446: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n",
      "/Users/owemshu/opt/anaconda3/envs/deeplearning/lib/python3.9/site-packages/sklearn/metrics/_regression.py:446: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()  # 记录开始时间\n",
    "reg = GradientDescentLinerRegressionWithRegularization(learning_rate=1e-5, max_iter=30, _lambda=100, early_stop=False, seed=1024)\n",
    "rate = np.diag([1] * 500)\n",
    "rate[0][0] = 100000\n",
    "train_x = train_x.dot(rate)\n",
    "test_x = test_x.dot(rate)\n",
    "reg.fit(train_x, train_y)\n",
    "print(\"运行时间: \", time.time() - time_start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1, loss_in_train: 22.27634, loss_in_test: 30.74151\n",
      "iter: 2, loss_in_train: 22.27634, loss_in_test: 30.74151\n",
      "iter: 3, loss_in_train: 22.27634, loss_in_test: 30.74151\n",
      "iter: 4, loss_in_train: 22.27634, loss_in_test: 30.74151\n",
      "iter: 5, loss_in_train: 22.27634, loss_in_test: 30.74151\n",
      "iter: 6, loss_in_train: 22.27635, loss_in_test: 30.74151\n",
      "iter: 7, loss_in_train: 22.27635, loss_in_test: 30.74151\n",
      "iter: 8, loss_in_train: 22.27635, loss_in_test: 30.74151\n",
      "iter: 9, loss_in_train: 22.27635, loss_in_test: 30.74151\n",
      "iter: 10, loss_in_train: 22.27635, loss_in_test: 30.74151\n",
      "iter: 11, loss_in_train: 22.27635, loss_in_test: 30.74151\n",
      "iter: 12, loss_in_train: 22.27635, loss_in_test: 30.74151\n",
      "iter: 13, loss_in_train: 22.27635, loss_in_test: 30.74151\n",
      "iter: 14, loss_in_train: 22.27635, loss_in_test: 30.74152\n",
      "iter: 15, loss_in_train: 22.27635, loss_in_test: 30.74152\n",
      "iter: 16, loss_in_train: 22.27635, loss_in_test: 30.74152\n",
      "iter: 17, loss_in_train: 22.27635, loss_in_test: 30.74152\n",
      "iter: 18, loss_in_train: 22.27635, loss_in_test: 30.74152\n",
      "iter: 19, loss_in_train: 22.27635, loss_in_test: 30.74152\n",
      "iter: 20, loss_in_train: 22.27636, loss_in_test: 30.74152\n",
      "iter: 21, loss_in_train: 22.27636, loss_in_test: 30.74152\n",
      "iter: 22, loss_in_train: 22.27636, loss_in_test: 30.74152\n",
      "iter: 23, loss_in_train: 22.27636, loss_in_test: 30.74152\n",
      "iter: 24, loss_in_train: 22.27636, loss_in_test: 30.74152\n",
      "iter: 25, loss_in_train: 22.27636, loss_in_test: 30.74152\n",
      "iter: 26, loss_in_train: 22.27636, loss_in_test: 30.74152\n",
      "iter: 27, loss_in_train: 22.27636, loss_in_test: 30.74152\n",
      "iter: 28, loss_in_train: 22.27636, loss_in_test: 30.74152\n",
      "iter: 29, loss_in_train: 22.27636, loss_in_test: 30.74152\n",
      "iter: 30, loss_in_train: 22.27636, loss_in_test: 30.74152\n",
      "运行时间:  4.076153039932251\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "time_start = time.time()  # 记录开始时间\n",
    "reg = GradientDescentLinerRegressionWithRegularization(learning_rate=1e-5, max_iter=30, _lambda=100, early_stop=False, seed=1024)\n",
    "\n",
    "nor = Normalizer()\n",
    "train_x = nor.fit_transform(train_x)\n",
    "test_x = nor.transform(test_x)\n",
    "\n",
    "reg.fit(train_x, train_y)\n",
    "print(\"运行时间: \", time.time() - time_start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}