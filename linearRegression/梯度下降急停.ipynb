{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 导入数据\n",
    "x_all = pd.read_csv('data/random_data_regression_X.csv', header=None)\n",
    "y_all = pd.read_csv('data/random_data_regression_y.csv', header=None)\n",
    "train_x = x_all[:40]\n",
    "train_y = y_all[:40]\n",
    "test_x = x_all[40:]\n",
    "test_y = y_all[40:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# 封装梯度下降法(包含正则化项)\n",
    "class GradientDescentLinerRegressionWithRegularization:\n",
    "    def __init__(self, learning_rate=0.01, _lambda=0.1, min_grad=0.001, max_item=5000, seed=None):\n",
    "        np.random.seed(seed)\n",
    "        self._lambda = _lambda\n",
    "        self.lr = learning_rate  # 设置学习率\n",
    "        self.loss_arr = []\n",
    "        self.intercept_ = None\n",
    "        self.coef_ = None\n",
    "        self.min_grad = min_grad\n",
    "        self.max_item = max_item\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.x = x\n",
    "        self.x_b = np.hstack([np.ones((len(x), 1)), x])\n",
    "        self.x_b_dim = np.size(self.x_b, 1)\n",
    "        self.x_sample = np.size(self.x_b, 0)\n",
    "        self.y = y\n",
    "        self.w = np.random.normal(1, 0.001, (self.x_b_dim, 1))\n",
    "\n",
    "        for i in range(self.max_item):\n",
    "            d_w = self._train_step()\n",
    "            self.loss_arr.append(self.loss())\n",
    "            print(\"iter: {}, grad: {:.5f}, loss_in_train: {:.5f}, loss_in_test: {:.5f}\".format(i + 1, np.average(d_w),\n",
    "                                                                                               self.loss(is_test=False),\n",
    "                                                                                               self.loss_arr[-1]))\n",
    "            i += 1\n",
    "\n",
    "            if np.average(d_w) < self.min_grad:\n",
    "                break\n",
    "\n",
    "    def _f(self, x, w):\n",
    "        return x.dot(w)\n",
    "\n",
    "    def predict(self, x=None):\n",
    "        if x is None:\n",
    "            x = self.x_b\n",
    "        x = np.hstack([np.ones((len(x), 1)), x])\n",
    "        y_pred = self._f(x, self.w)\n",
    "        return y_pred\n",
    "\n",
    "    def loss(self, is_test=True, y_true=None, y_pred=None):\n",
    "        if y_true is None or y_pred is None:\n",
    "            y_true = self.y\n",
    "        if is_test:\n",
    "            return np.sqrt(mean_squared_error(test_y, self.predict(test_x)))\n",
    "        else:\n",
    "            return np.sqrt(mean_squared_error(y_true, self.predict(self.x)))\n",
    "\n",
    "    def _calc_gradient(self):\n",
    "        d_w = np.empty(self.x_b_dim).reshape(-1, 1)\n",
    "        d_w[0] = np.sum(self.x_b.dot(self.w) - self.y)\n",
    "        for i in range(1, self.x_b_dim):\n",
    "            d_w[i] = np.squeeze((self.x_b.dot(self.w) - self.y)).dot(self.x_b[:, i].T)\n",
    "        return d_w * 2 / self.x_sample + (self._lambda / self.x_sample * self.w)\n",
    "\n",
    "    def _train_step(self):\n",
    "        d_w = self._calc_gradient()\n",
    "        self.w = self.w - self.lr * d_w\n",
    "        self.intercept_ = self.w[0]\n",
    "        self.coef_ = self.w[1:]\n",
    "        return self.w"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1, grad: 0.99524, loss_in_train: 30.00431, loss_in_test: 35.12043\n",
      "iter: 2, grad: 0.99052, loss_in_train: 29.21537, loss_in_test: 35.16077\n",
      "iter: 3, grad: 0.98587, loss_in_train: 28.44995, loss_in_test: 35.20093\n",
      "iter: 4, grad: 0.98129, loss_in_train: 27.70729, loss_in_test: 35.24087\n",
      "iter: 5, grad: 0.97679, loss_in_train: 26.98667, loss_in_test: 35.28055\n",
      "iter: 6, grad: 0.97235, loss_in_train: 26.28739, loss_in_test: 35.31994\n",
      "iter: 7, grad: 0.96799, loss_in_train: 25.60878, loss_in_test: 35.35899\n",
      "iter: 8, grad: 0.96369, loss_in_train: 24.95019, loss_in_test: 35.39768\n",
      "iter: 9, grad: 0.95945, loss_in_train: 24.31099, loss_in_test: 35.43598\n",
      "iter: 10, grad: 0.95527, loss_in_train: 23.69056, loss_in_test: 35.47386\n",
      "iter: 11, grad: 0.95116, loss_in_train: 23.08833, loss_in_test: 35.51129\n",
      "iter: 12, grad: 0.94710, loss_in_train: 22.50373, loss_in_test: 35.54827\n",
      "iter: 13, grad: 0.94310, loss_in_train: 21.93619, loss_in_test: 35.58476\n",
      "iter: 14, grad: 0.93916, loss_in_train: 21.38520, loss_in_test: 35.62075\n",
      "iter: 15, grad: 0.93527, loss_in_train: 20.85025, loss_in_test: 35.65623\n",
      "iter: 16, grad: 0.93144, loss_in_train: 20.33082, loss_in_test: 35.69118\n",
      "iter: 17, grad: 0.92765, loss_in_train: 19.82645, loss_in_test: 35.72559\n",
      "iter: 18, grad: 0.92392, loss_in_train: 19.33667, loss_in_test: 35.75945\n",
      "iter: 19, grad: 0.92024, loss_in_train: 18.86103, loss_in_test: 35.79275\n",
      "iter: 20, grad: 0.91660, loss_in_train: 18.39910, loss_in_test: 35.82547\n",
      "iter: 21, grad: 0.91301, loss_in_train: 17.95045, loss_in_test: 35.85763\n",
      "iter: 22, grad: 0.90946, loss_in_train: 17.51469, loss_in_test: 35.88920\n",
      "iter: 23, grad: 0.90596, loss_in_train: 17.09142, loss_in_test: 35.92019\n",
      "iter: 24, grad: 0.90251, loss_in_train: 16.68025, loss_in_test: 35.95059\n",
      "iter: 25, grad: 0.89909, loss_in_train: 16.28083, loss_in_test: 35.98040\n",
      "iter: 26, grad: 0.89572, loss_in_train: 15.89279, loss_in_test: 36.00962\n",
      "iter: 27, grad: 0.89238, loss_in_train: 15.51580, loss_in_test: 36.03825\n",
      "iter: 28, grad: 0.88909, loss_in_train: 15.14952, loss_in_test: 36.06628\n",
      "iter: 29, grad: 0.88583, loss_in_train: 14.79362, loss_in_test: 36.09372\n",
      "iter: 30, grad: 0.88261, loss_in_train: 14.44780, loss_in_test: 36.12058\n",
      "iter: 31, grad: 0.87943, loss_in_train: 14.11175, loss_in_test: 36.14684\n",
      "iter: 32, grad: 0.87628, loss_in_train: 13.78519, loss_in_test: 36.17251\n",
      "iter: 33, grad: 0.87317, loss_in_train: 13.46783, loss_in_test: 36.19761\n",
      "iter: 34, grad: 0.87009, loss_in_train: 13.15939, loss_in_test: 36.22212\n",
      "iter: 35, grad: 0.86704, loss_in_train: 12.85962, loss_in_test: 36.24606\n",
      "iter: 36, grad: 0.86402, loss_in_train: 12.56825, loss_in_test: 36.26943\n",
      "iter: 37, grad: 0.86104, loss_in_train: 12.28504, loss_in_test: 36.29223\n",
      "iter: 38, grad: 0.85809, loss_in_train: 12.00975, loss_in_test: 36.31447\n",
      "iter: 39, grad: 0.85516, loss_in_train: 11.74214, loss_in_test: 36.33615\n",
      "iter: 40, grad: 0.85227, loss_in_train: 11.48199, loss_in_test: 36.35728\n",
      "iter: 41, grad: 0.84940, loss_in_train: 11.22908, loss_in_test: 36.37787\n",
      "iter: 42, grad: 0.84656, loss_in_train: 10.98320, loss_in_test: 36.39792\n",
      "iter: 43, grad: 0.84375, loss_in_train: 10.74415, loss_in_test: 36.41743\n",
      "iter: 44, grad: 0.84097, loss_in_train: 10.51172, loss_in_test: 36.43642\n",
      "iter: 45, grad: 0.83821, loss_in_train: 10.28573, loss_in_test: 36.45489\n",
      "iter: 46, grad: 0.83548, loss_in_train: 10.06599, loss_in_test: 36.47285\n",
      "iter: 47, grad: 0.83277, loss_in_train: 9.85232, loss_in_test: 36.49031\n",
      "iter: 48, grad: 0.83008, loss_in_train: 9.64454, loss_in_test: 36.50726\n",
      "iter: 49, grad: 0.82742, loss_in_train: 9.44248, loss_in_test: 36.52372\n",
      "iter: 50, grad: 0.82478, loss_in_train: 9.24599, loss_in_test: 36.53970\n",
      "iter: 51, grad: 0.82217, loss_in_train: 9.05489, loss_in_test: 36.55520\n",
      "iter: 52, grad: 0.81958, loss_in_train: 8.86905, loss_in_test: 36.57024\n",
      "iter: 53, grad: 0.81700, loss_in_train: 8.68830, loss_in_test: 36.58481\n",
      "iter: 54, grad: 0.81445, loss_in_train: 8.51250, loss_in_test: 36.59892\n",
      "iter: 55, grad: 0.81192, loss_in_train: 8.34152, loss_in_test: 36.61259\n",
      "iter: 56, grad: 0.80942, loss_in_train: 8.17521, loss_in_test: 36.62582\n",
      "iter: 57, grad: 0.80693, loss_in_train: 8.01345, loss_in_test: 36.63861\n",
      "iter: 58, grad: 0.80446, loss_in_train: 7.85611, loss_in_test: 36.65098\n",
      "iter: 59, grad: 0.80201, loss_in_train: 7.70306, loss_in_test: 36.66293\n",
      "iter: 60, grad: 0.79957, loss_in_train: 7.55418, loss_in_test: 36.67447\n",
      "iter: 61, grad: 0.79716, loss_in_train: 7.40935, loss_in_test: 36.68561\n",
      "iter: 62, grad: 0.79476, loss_in_train: 7.26847, loss_in_test: 36.69635\n",
      "iter: 63, grad: 0.79238, loss_in_train: 7.13142, loss_in_test: 36.70671\n",
      "iter: 64, grad: 0.79002, loss_in_train: 6.99809, loss_in_test: 36.71668\n",
      "iter: 65, grad: 0.78768, loss_in_train: 6.86839, loss_in_test: 36.72628\n",
      "iter: 66, grad: 0.78535, loss_in_train: 6.74221, loss_in_test: 36.73551\n",
      "iter: 67, grad: 0.78304, loss_in_train: 6.61945, loss_in_test: 36.74438\n",
      "iter: 68, grad: 0.78074, loss_in_train: 6.50002, loss_in_test: 36.75290\n",
      "iter: 69, grad: 0.77846, loss_in_train: 6.38383, loss_in_test: 36.76107\n",
      "iter: 70, grad: 0.77620, loss_in_train: 6.27079, loss_in_test: 36.76891\n",
      "iter: 71, grad: 0.77395, loss_in_train: 6.16081, loss_in_test: 36.77641\n",
      "iter: 72, grad: 0.77171, loss_in_train: 6.05381, loss_in_test: 36.78358\n",
      "iter: 73, grad: 0.76949, loss_in_train: 5.94971, loss_in_test: 36.79044\n",
      "iter: 74, grad: 0.76728, loss_in_train: 5.84842, loss_in_test: 36.79699\n",
      "iter: 75, grad: 0.76509, loss_in_train: 5.74987, loss_in_test: 36.80323\n",
      "iter: 76, grad: 0.76291, loss_in_train: 5.65399, loss_in_test: 36.80917\n",
      "iter: 77, grad: 0.76074, loss_in_train: 5.56070, loss_in_test: 36.81482\n",
      "iter: 78, grad: 0.75859, loss_in_train: 5.46993, loss_in_test: 36.82018\n",
      "iter: 79, grad: 0.75644, loss_in_train: 5.38162, loss_in_test: 36.82526\n",
      "iter: 80, grad: 0.75432, loss_in_train: 5.29569, loss_in_test: 36.83006\n",
      "iter: 81, grad: 0.75220, loss_in_train: 5.21209, loss_in_test: 36.83460\n",
      "iter: 82, grad: 0.75010, loss_in_train: 5.13075, loss_in_test: 36.83888\n",
      "iter: 83, grad: 0.74800, loss_in_train: 5.05160, loss_in_test: 36.84290\n",
      "iter: 84, grad: 0.74592, loss_in_train: 4.97459, loss_in_test: 36.84667\n",
      "iter: 85, grad: 0.74386, loss_in_train: 4.89967, loss_in_test: 36.85019\n",
      "iter: 86, grad: 0.74180, loss_in_train: 4.82676, loss_in_test: 36.85347\n",
      "iter: 87, grad: 0.73975, loss_in_train: 4.75583, loss_in_test: 36.85652\n",
      "iter: 88, grad: 0.73772, loss_in_train: 4.68682, loss_in_test: 36.85934\n",
      "iter: 89, grad: 0.73569, loss_in_train: 4.61967, loss_in_test: 36.86194\n",
      "iter: 90, grad: 0.73368, loss_in_train: 4.55433, loss_in_test: 36.86432\n",
      "iter: 91, grad: 0.73168, loss_in_train: 4.49077, loss_in_test: 36.86649\n",
      "iter: 92, grad: 0.72968, loss_in_train: 4.42892, loss_in_test: 36.86845\n",
      "iter: 93, grad: 0.72770, loss_in_train: 4.36874, loss_in_test: 36.87020\n",
      "iter: 94, grad: 0.72573, loss_in_train: 4.31019, loss_in_test: 36.87176\n",
      "iter: 95, grad: 0.72376, loss_in_train: 4.25322, loss_in_test: 36.87312\n",
      "iter: 96, grad: 0.72181, loss_in_train: 4.19780, loss_in_test: 36.87430\n",
      "iter: 97, grad: 0.71987, loss_in_train: 4.14387, loss_in_test: 36.87529\n",
      "iter: 98, grad: 0.71793, loss_in_train: 4.09140, loss_in_test: 36.87610\n",
      "iter: 99, grad: 0.71601, loss_in_train: 4.04036, loss_in_test: 36.87674\n",
      "iter: 100, grad: 0.71409, loss_in_train: 3.99069, loss_in_test: 36.87720\n",
      "iter: 101, grad: 0.71218, loss_in_train: 3.94237, loss_in_test: 36.87750\n",
      "iter: 102, grad: 0.71028, loss_in_train: 3.89536, loss_in_test: 36.87764\n",
      "iter: 103, grad: 0.70839, loss_in_train: 3.84961, loss_in_test: 36.87762\n",
      "iter: 104, grad: 0.70651, loss_in_train: 3.80511, loss_in_test: 36.87745\n",
      "iter: 105, grad: 0.70464, loss_in_train: 3.76181, loss_in_test: 36.87712\n",
      "iter: 106, grad: 0.70277, loss_in_train: 3.71969, loss_in_test: 36.87665\n",
      "iter: 107, grad: 0.70092, loss_in_train: 3.67870, loss_in_test: 36.87604\n",
      "iter: 108, grad: 0.69907, loss_in_train: 3.63883, loss_in_test: 36.87529\n",
      "iter: 109, grad: 0.69723, loss_in_train: 3.60003, loss_in_test: 36.87440\n",
      "iter: 110, grad: 0.69540, loss_in_train: 3.56229, loss_in_test: 36.87338\n",
      "iter: 111, grad: 0.69357, loss_in_train: 3.52557, loss_in_test: 36.87223\n",
      "iter: 112, grad: 0.69175, loss_in_train: 3.48984, loss_in_test: 36.87096\n",
      "iter: 113, grad: 0.68994, loss_in_train: 3.45508, loss_in_test: 36.86957\n",
      "iter: 114, grad: 0.68814, loss_in_train: 3.42126, loss_in_test: 36.86807\n",
      "iter: 115, grad: 0.68635, loss_in_train: 3.38835, loss_in_test: 36.86644\n",
      "iter: 116, grad: 0.68456, loss_in_train: 3.35634, loss_in_test: 36.86471\n",
      "iter: 117, grad: 0.68278, loss_in_train: 3.32519, loss_in_test: 36.86287\n",
      "iter: 118, grad: 0.68101, loss_in_train: 3.29489, loss_in_test: 36.86092\n",
      "iter: 119, grad: 0.67924, loss_in_train: 3.26540, loss_in_test: 36.85887\n",
      "iter: 120, grad: 0.67748, loss_in_train: 3.23672, loss_in_test: 36.85672\n",
      "iter: 121, grad: 0.67573, loss_in_train: 3.20880, loss_in_test: 36.85448\n",
      "iter: 122, grad: 0.67398, loss_in_train: 3.18165, loss_in_test: 36.85214\n",
      "iter: 123, grad: 0.67225, loss_in_train: 3.15522, loss_in_test: 36.84971\n",
      "iter: 124, grad: 0.67051, loss_in_train: 3.12951, loss_in_test: 36.84719\n",
      "iter: 125, grad: 0.66879, loss_in_train: 3.10450, loss_in_test: 36.84459\n",
      "iter: 126, grad: 0.66707, loss_in_train: 3.08015, loss_in_test: 36.84190\n",
      "iter: 127, grad: 0.66536, loss_in_train: 3.05647, loss_in_test: 36.83914\n",
      "iter: 128, grad: 0.66365, loss_in_train: 3.03342, loss_in_test: 36.83630\n",
      "iter: 129, grad: 0.66195, loss_in_train: 3.01100, loss_in_test: 36.83338\n",
      "iter: 130, grad: 0.66026, loss_in_train: 2.98917, loss_in_test: 36.83039\n",
      "iter: 131, grad: 0.65857, loss_in_train: 2.96794, loss_in_test: 36.82732\n",
      "iter: 132, grad: 0.65689, loss_in_train: 2.94727, loss_in_test: 36.82419\n",
      "iter: 133, grad: 0.65521, loss_in_train: 2.92717, loss_in_test: 36.82100\n",
      "iter: 134, grad: 0.65354, loss_in_train: 2.90760, loss_in_test: 36.81773\n",
      "iter: 135, grad: 0.65188, loss_in_train: 2.88855, loss_in_test: 36.81441\n",
      "iter: 136, grad: 0.65022, loss_in_train: 2.87002, loss_in_test: 36.81103\n",
      "iter: 137, grad: 0.64857, loss_in_train: 2.85198, loss_in_test: 36.80759\n",
      "iter: 138, grad: 0.64692, loss_in_train: 2.83442, loss_in_test: 36.80409\n",
      "iter: 139, grad: 0.64528, loss_in_train: 2.81734, loss_in_test: 36.80054\n",
      "iter: 140, grad: 0.64364, loss_in_train: 2.80070, loss_in_test: 36.79693\n",
      "iter: 141, grad: 0.64201, loss_in_train: 2.78452, loss_in_test: 36.79328\n",
      "iter: 142, grad: 0.64039, loss_in_train: 2.76876, loss_in_test: 36.78958\n",
      "iter: 143, grad: 0.63877, loss_in_train: 2.75343, loss_in_test: 36.78583\n",
      "iter: 144, grad: 0.63716, loss_in_train: 2.73850, loss_in_test: 36.78203\n",
      "iter: 145, grad: 0.63555, loss_in_train: 2.72396, loss_in_test: 36.77819\n",
      "iter: 146, grad: 0.63395, loss_in_train: 2.70982, loss_in_test: 36.77431\n",
      "iter: 147, grad: 0.63235, loss_in_train: 2.69604, loss_in_test: 36.77039\n",
      "iter: 148, grad: 0.63076, loss_in_train: 2.68264, loss_in_test: 36.76643\n",
      "iter: 149, grad: 0.62917, loss_in_train: 2.66958, loss_in_test: 36.76244\n",
      "iter: 150, grad: 0.62759, loss_in_train: 2.65687, loss_in_test: 36.75840\n",
      "iter: 151, grad: 0.62601, loss_in_train: 2.64450, loss_in_test: 36.75434\n",
      "iter: 152, grad: 0.62444, loss_in_train: 2.63245, loss_in_test: 36.75024\n",
      "iter: 153, grad: 0.62288, loss_in_train: 2.62071, loss_in_test: 36.74611\n",
      "iter: 154, grad: 0.62131, loss_in_train: 2.60929, loss_in_test: 36.74195\n",
      "iter: 155, grad: 0.61976, loss_in_train: 2.59816, loss_in_test: 36.73776\n",
      "iter: 156, grad: 0.61821, loss_in_train: 2.58733, loss_in_test: 36.73354\n",
      "iter: 157, grad: 0.61666, loss_in_train: 2.57677, loss_in_test: 36.72930\n",
      "iter: 158, grad: 0.61512, loss_in_train: 2.56650, loss_in_test: 36.72503\n",
      "iter: 159, grad: 0.61358, loss_in_train: 2.55649, loss_in_test: 36.72074\n",
      "iter: 160, grad: 0.61205, loss_in_train: 2.54674, loss_in_test: 36.71642\n",
      "iter: 161, grad: 0.61052, loss_in_train: 2.53724, loss_in_test: 36.71209\n",
      "iter: 162, grad: 0.60900, loss_in_train: 2.52799, loss_in_test: 36.70773\n",
      "iter: 163, grad: 0.60748, loss_in_train: 2.51898, loss_in_test: 36.70336\n",
      "iter: 164, grad: 0.60596, loss_in_train: 2.51020, loss_in_test: 36.69896\n",
      "iter: 165, grad: 0.60445, loss_in_train: 2.50165, loss_in_test: 36.69455\n",
      "iter: 166, grad: 0.60295, loss_in_train: 2.49332, loss_in_test: 36.69012\n",
      "iter: 167, grad: 0.60145, loss_in_train: 2.48520, loss_in_test: 36.68568\n",
      "iter: 168, grad: 0.59995, loss_in_train: 2.47729, loss_in_test: 36.68122\n",
      "iter: 169, grad: 0.59846, loss_in_train: 2.46958, loss_in_test: 36.67675\n",
      "iter: 170, grad: 0.59698, loss_in_train: 2.46207, loss_in_test: 36.67227\n",
      "iter: 171, grad: 0.59549, loss_in_train: 2.45475, loss_in_test: 36.66777\n",
      "iter: 172, grad: 0.59402, loss_in_train: 2.44761, loss_in_test: 36.66327\n",
      "iter: 173, grad: 0.59254, loss_in_train: 2.44066, loss_in_test: 36.65875\n",
      "iter: 174, grad: 0.59107, loss_in_train: 2.43388, loss_in_test: 36.65423\n",
      "iter: 175, grad: 0.58961, loss_in_train: 2.42728, loss_in_test: 36.64970\n",
      "iter: 176, grad: 0.58815, loss_in_train: 2.42084, loss_in_test: 36.64516\n",
      "iter: 177, grad: 0.58669, loss_in_train: 2.41457, loss_in_test: 36.64061\n",
      "iter: 178, grad: 0.58524, loss_in_train: 2.40845, loss_in_test: 36.63606\n",
      "iter: 179, grad: 0.58379, loss_in_train: 2.40248, loss_in_test: 36.63150\n",
      "iter: 180, grad: 0.58235, loss_in_train: 2.39667, loss_in_test: 36.62693\n",
      "iter: 181, grad: 0.58091, loss_in_train: 2.39100, loss_in_test: 36.62237\n",
      "iter: 182, grad: 0.57947, loss_in_train: 2.38547, loss_in_test: 36.61780\n",
      "iter: 183, grad: 0.57804, loss_in_train: 2.38008, loss_in_test: 36.61323\n",
      "iter: 184, grad: 0.57661, loss_in_train: 2.37483, loss_in_test: 36.60865\n",
      "iter: 185, grad: 0.57519, loss_in_train: 2.36970, loss_in_test: 36.60407\n",
      "iter: 186, grad: 0.57377, loss_in_train: 2.36470, loss_in_test: 36.59950\n",
      "iter: 187, grad: 0.57236, loss_in_train: 2.35983, loss_in_test: 36.59492\n",
      "iter: 188, grad: 0.57095, loss_in_train: 2.35508, loss_in_test: 36.59034\n",
      "iter: 189, grad: 0.56954, loss_in_train: 2.35044, loss_in_test: 36.58577\n",
      "iter: 190, grad: 0.56814, loss_in_train: 2.34591, loss_in_test: 36.58119\n",
      "iter: 191, grad: 0.56674, loss_in_train: 2.34150, loss_in_test: 36.57662\n",
      "iter: 192, grad: 0.56534, loss_in_train: 2.33720, loss_in_test: 36.57205\n",
      "iter: 193, grad: 0.56395, loss_in_train: 2.33300, loss_in_test: 36.56748\n",
      "iter: 194, grad: 0.56256, loss_in_train: 2.32890, loss_in_test: 36.56292\n",
      "iter: 195, grad: 0.56118, loss_in_train: 2.32491, loss_in_test: 36.55836\n",
      "iter: 196, grad: 0.55980, loss_in_train: 2.32101, loss_in_test: 36.55380\n",
      "iter: 197, grad: 0.55842, loss_in_train: 2.31720, loss_in_test: 36.54925\n",
      "iter: 198, grad: 0.55705, loss_in_train: 2.31349, loss_in_test: 36.54471\n",
      "iter: 199, grad: 0.55568, loss_in_train: 2.30986, loss_in_test: 36.54017\n",
      "iter: 200, grad: 0.55432, loss_in_train: 2.30633, loss_in_test: 36.53563\n",
      "iter: 201, grad: 0.55296, loss_in_train: 2.30288, loss_in_test: 36.53111\n",
      "iter: 202, grad: 0.55160, loss_in_train: 2.29951, loss_in_test: 36.52659\n",
      "iter: 203, grad: 0.55025, loss_in_train: 2.29622, loss_in_test: 36.52207\n",
      "iter: 204, grad: 0.54890, loss_in_train: 2.29301, loss_in_test: 36.51757\n",
      "iter: 205, grad: 0.54755, loss_in_train: 2.28988, loss_in_test: 36.51307\n",
      "iter: 206, grad: 0.54621, loss_in_train: 2.28682, loss_in_test: 36.50858\n",
      "iter: 207, grad: 0.54487, loss_in_train: 2.28384, loss_in_test: 36.50409\n",
      "iter: 208, grad: 0.54353, loss_in_train: 2.28092, loss_in_test: 36.49962\n",
      "iter: 209, grad: 0.54220, loss_in_train: 2.27808, loss_in_test: 36.49516\n",
      "iter: 210, grad: 0.54087, loss_in_train: 2.27530, loss_in_test: 36.49070\n",
      "iter: 211, grad: 0.53955, loss_in_train: 2.27259, loss_in_test: 36.48626\n",
      "iter: 212, grad: 0.53823, loss_in_train: 2.26994, loss_in_test: 36.48182\n",
      "iter: 213, grad: 0.53691, loss_in_train: 2.26736, loss_in_test: 36.47740\n",
      "iter: 214, grad: 0.53560, loss_in_train: 2.26483, loss_in_test: 36.47298\n",
      "iter: 215, grad: 0.53429, loss_in_train: 2.26236, loss_in_test: 36.46858\n",
      "iter: 216, grad: 0.53298, loss_in_train: 2.25996, loss_in_test: 36.46419\n",
      "iter: 217, grad: 0.53168, loss_in_train: 2.25760, loss_in_test: 36.45981\n",
      "iter: 218, grad: 0.53038, loss_in_train: 2.25531, loss_in_test: 36.45544\n",
      "iter: 219, grad: 0.52908, loss_in_train: 2.25306, loss_in_test: 36.45108\n",
      "iter: 220, grad: 0.52779, loss_in_train: 2.25087, loss_in_test: 36.44673\n",
      "iter: 221, grad: 0.52650, loss_in_train: 2.24873, loss_in_test: 36.44240\n",
      "iter: 222, grad: 0.52521, loss_in_train: 2.24664, loss_in_test: 36.43807\n",
      "iter: 223, grad: 0.52393, loss_in_train: 2.24460, loss_in_test: 36.43376\n",
      "iter: 224, grad: 0.52265, loss_in_train: 2.24260, loss_in_test: 36.42947\n",
      "iter: 225, grad: 0.52138, loss_in_train: 2.24065, loss_in_test: 36.42518\n",
      "iter: 226, grad: 0.52010, loss_in_train: 2.23875, loss_in_test: 36.42091\n",
      "iter: 227, grad: 0.51884, loss_in_train: 2.23688, loss_in_test: 36.41665\n",
      "iter: 228, grad: 0.51757, loss_in_train: 2.23507, loss_in_test: 36.41241\n",
      "iter: 229, grad: 0.51631, loss_in_train: 2.23329, loss_in_test: 36.40818\n",
      "iter: 230, grad: 0.51505, loss_in_train: 2.23155, loss_in_test: 36.40396\n",
      "iter: 231, grad: 0.51379, loss_in_train: 2.22985, loss_in_test: 36.39975\n",
      "iter: 232, grad: 0.51254, loss_in_train: 2.22820, loss_in_test: 36.39556\n",
      "iter: 233, grad: 0.51129, loss_in_train: 2.22658, loss_in_test: 36.39139\n",
      "iter: 234, grad: 0.51005, loss_in_train: 2.22499, loss_in_test: 36.38723\n",
      "iter: 235, grad: 0.50880, loss_in_train: 2.22344, loss_in_test: 36.38308\n",
      "iter: 236, grad: 0.50756, loss_in_train: 2.22193, loss_in_test: 36.37895\n",
      "iter: 237, grad: 0.50633, loss_in_train: 2.22045, loss_in_test: 36.37483\n",
      "iter: 238, grad: 0.50510, loss_in_train: 2.21901, loss_in_test: 36.37072\n",
      "iter: 239, grad: 0.50387, loss_in_train: 2.21759, loss_in_test: 36.36663\n",
      "iter: 240, grad: 0.50264, loss_in_train: 2.21621, loss_in_test: 36.36256\n",
      "iter: 241, grad: 0.50142, loss_in_train: 2.21486, loss_in_test: 36.35850\n",
      "iter: 242, grad: 0.50020, loss_in_train: 2.21354, loss_in_test: 36.35445\n",
      "iter: 243, grad: 0.49898, loss_in_train: 2.21225, loss_in_test: 36.35042\n",
      "iter: 244, grad: 0.49777, loss_in_train: 2.21099, loss_in_test: 36.34641\n",
      "iter: 245, grad: 0.49656, loss_in_train: 2.20975, loss_in_test: 36.34241\n",
      "iter: 246, grad: 0.49535, loss_in_train: 2.20855, loss_in_test: 36.33843\n",
      "iter: 247, grad: 0.49414, loss_in_train: 2.20737, loss_in_test: 36.33446\n",
      "iter: 248, grad: 0.49294, loss_in_train: 2.20622, loss_in_test: 36.33050\n",
      "iter: 249, grad: 0.49174, loss_in_train: 2.20509, loss_in_test: 36.32656\n",
      "iter: 250, grad: 0.49055, loss_in_train: 2.20399, loss_in_test: 36.32264\n",
      "iter: 251, grad: 0.48936, loss_in_train: 2.20291, loss_in_test: 36.31874\n",
      "iter: 252, grad: 0.48817, loss_in_train: 2.20185, loss_in_test: 36.31484\n",
      "iter: 253, grad: 0.48698, loss_in_train: 2.20082, loss_in_test: 36.31097\n",
      "iter: 254, grad: 0.48580, loss_in_train: 2.19981, loss_in_test: 36.30711\n",
      "iter: 255, grad: 0.48462, loss_in_train: 2.19883, loss_in_test: 36.30327\n",
      "iter: 256, grad: 0.48345, loss_in_train: 2.19786, loss_in_test: 36.29944\n",
      "iter: 257, grad: 0.48227, loss_in_train: 2.19692, loss_in_test: 36.29563\n",
      "iter: 258, grad: 0.48110, loss_in_train: 2.19599, loss_in_test: 36.29183\n",
      "iter: 259, grad: 0.47993, loss_in_train: 2.19509, loss_in_test: 36.28805\n",
      "iter: 260, grad: 0.47877, loss_in_train: 2.19421, loss_in_test: 36.28428\n",
      "iter: 261, grad: 0.47761, loss_in_train: 2.19334, loss_in_test: 36.28054\n",
      "iter: 262, grad: 0.47645, loss_in_train: 2.19250, loss_in_test: 36.27680\n",
      "iter: 263, grad: 0.47529, loss_in_train: 2.19167, loss_in_test: 36.27309\n",
      "iter: 264, grad: 0.47414, loss_in_train: 2.19086, loss_in_test: 36.26939\n",
      "iter: 265, grad: 0.47299, loss_in_train: 2.19007, loss_in_test: 36.26570\n",
      "iter: 266, grad: 0.47185, loss_in_train: 2.18930, loss_in_test: 36.26203\n",
      "iter: 267, grad: 0.47070, loss_in_train: 2.18854, loss_in_test: 36.25838\n",
      "iter: 268, grad: 0.46956, loss_in_train: 2.18780, loss_in_test: 36.25474\n",
      "iter: 269, grad: 0.46842, loss_in_train: 2.18708, loss_in_test: 36.25112\n",
      "iter: 270, grad: 0.46729, loss_in_train: 2.18637, loss_in_test: 36.24752\n",
      "iter: 271, grad: 0.46616, loss_in_train: 2.18567, loss_in_test: 36.24393\n",
      "iter: 272, grad: 0.46503, loss_in_train: 2.18499, loss_in_test: 36.24036\n",
      "iter: 273, grad: 0.46390, loss_in_train: 2.18433, loss_in_test: 36.23680\n",
      "iter: 274, grad: 0.46278, loss_in_train: 2.18368, loss_in_test: 36.23326\n",
      "iter: 275, grad: 0.46166, loss_in_train: 2.18304, loss_in_test: 36.22974\n",
      "iter: 276, grad: 0.46054, loss_in_train: 2.18242, loss_in_test: 36.22623\n",
      "iter: 277, grad: 0.45943, loss_in_train: 2.18181, loss_in_test: 36.22274\n",
      "iter: 278, grad: 0.45831, loss_in_train: 2.18121, loss_in_test: 36.21927\n",
      "iter: 279, grad: 0.45720, loss_in_train: 2.18063, loss_in_test: 36.21581\n",
      "iter: 280, grad: 0.45610, loss_in_train: 2.18006, loss_in_test: 36.21236\n",
      "iter: 281, grad: 0.45500, loss_in_train: 2.17950, loss_in_test: 36.20893\n",
      "iter: 282, grad: 0.45389, loss_in_train: 2.17895, loss_in_test: 36.20552\n",
      "iter: 283, grad: 0.45280, loss_in_train: 2.17842, loss_in_test: 36.20213\n",
      "iter: 284, grad: 0.45170, loss_in_train: 2.17789, loss_in_test: 36.19875\n",
      "iter: 285, grad: 0.45061, loss_in_train: 2.17738, loss_in_test: 36.19538\n",
      "iter: 286, grad: 0.44952, loss_in_train: 2.17688, loss_in_test: 36.19204\n",
      "iter: 287, grad: 0.44843, loss_in_train: 2.17639, loss_in_test: 36.18871\n",
      "iter: 288, grad: 0.44735, loss_in_train: 2.17590, loss_in_test: 36.18539\n",
      "iter: 289, grad: 0.44627, loss_in_train: 2.17543, loss_in_test: 36.18209\n",
      "iter: 290, grad: 0.44519, loss_in_train: 2.17497, loss_in_test: 36.17880\n",
      "iter: 291, grad: 0.44411, loss_in_train: 2.17452, loss_in_test: 36.17554\n",
      "iter: 292, grad: 0.44304, loss_in_train: 2.17408, loss_in_test: 36.17228\n",
      "iter: 293, grad: 0.44197, loss_in_train: 2.17365, loss_in_test: 36.16905\n",
      "iter: 294, grad: 0.44090, loss_in_train: 2.17322, loss_in_test: 36.16583\n",
      "iter: 295, grad: 0.43984, loss_in_train: 2.17281, loss_in_test: 36.16262\n",
      "iter: 296, grad: 0.43878, loss_in_train: 2.17240, loss_in_test: 36.15943\n",
      "iter: 297, grad: 0.43772, loss_in_train: 2.17200, loss_in_test: 36.15626\n",
      "iter: 298, grad: 0.43666, loss_in_train: 2.17162, loss_in_test: 36.15310\n",
      "iter: 299, grad: 0.43561, loss_in_train: 2.17123, loss_in_test: 36.14995\n",
      "iter: 300, grad: 0.43456, loss_in_train: 2.17086, loss_in_test: 36.14683\n",
      "iter: 301, grad: 0.43351, loss_in_train: 2.17050, loss_in_test: 36.14371\n",
      "iter: 302, grad: 0.43246, loss_in_train: 2.17014, loss_in_test: 36.14062\n",
      "iter: 303, grad: 0.43142, loss_in_train: 2.16979, loss_in_test: 36.13753\n",
      "iter: 304, grad: 0.43038, loss_in_train: 2.16945, loss_in_test: 36.13447\n",
      "iter: 305, grad: 0.42934, loss_in_train: 2.16911, loss_in_test: 36.13142\n",
      "iter: 306, grad: 0.42830, loss_in_train: 2.16878, loss_in_test: 36.12838\n",
      "iter: 307, grad: 0.42727, loss_in_train: 2.16846, loss_in_test: 36.12536\n",
      "iter: 308, grad: 0.42624, loss_in_train: 2.16814, loss_in_test: 36.12236\n",
      "iter: 309, grad: 0.42521, loss_in_train: 2.16783, loss_in_test: 36.11937\n",
      "iter: 310, grad: 0.42419, loss_in_train: 2.16753, loss_in_test: 36.11639\n",
      "iter: 311, grad: 0.42316, loss_in_train: 2.16724, loss_in_test: 36.11343\n",
      "iter: 312, grad: 0.42214, loss_in_train: 2.16695, loss_in_test: 36.11048\n",
      "iter: 313, grad: 0.42113, loss_in_train: 2.16666, loss_in_test: 36.10755\n",
      "iter: 314, grad: 0.42011, loss_in_train: 2.16638, loss_in_test: 36.10464\n",
      "iter: 315, grad: 0.41910, loss_in_train: 2.16611, loss_in_test: 36.10174\n",
      "iter: 316, grad: 0.41809, loss_in_train: 2.16584, loss_in_test: 36.09885\n",
      "iter: 317, grad: 0.41708, loss_in_train: 2.16558, loss_in_test: 36.09598\n",
      "iter: 318, grad: 0.41608, loss_in_train: 2.16533, loss_in_test: 36.09313\n",
      "iter: 319, grad: 0.41508, loss_in_train: 2.16508, loss_in_test: 36.09029\n",
      "iter: 320, grad: 0.41408, loss_in_train: 2.16483, loss_in_test: 36.08746\n",
      "iter: 321, grad: 0.41308, loss_in_train: 2.16459, loss_in_test: 36.08465\n",
      "iter: 322, grad: 0.41209, loss_in_train: 2.16436, loss_in_test: 36.08185\n",
      "iter: 323, grad: 0.41109, loss_in_train: 2.16412, loss_in_test: 36.07907\n",
      "iter: 324, grad: 0.41010, loss_in_train: 2.16390, loss_in_test: 36.07630\n",
      "iter: 325, grad: 0.40912, loss_in_train: 2.16368, loss_in_test: 36.07354\n",
      "iter: 326, grad: 0.40813, loss_in_train: 2.16346, loss_in_test: 36.07080\n",
      "iter: 327, grad: 0.40715, loss_in_train: 2.16325, loss_in_test: 36.06808\n",
      "iter: 328, grad: 0.40617, loss_in_train: 2.16304, loss_in_test: 36.06536\n",
      "iter: 329, grad: 0.40519, loss_in_train: 2.16284, loss_in_test: 36.06267\n",
      "iter: 330, grad: 0.40422, loss_in_train: 2.16264, loss_in_test: 36.05998\n",
      "iter: 331, grad: 0.40325, loss_in_train: 2.16244, loss_in_test: 36.05731\n",
      "iter: 332, grad: 0.40228, loss_in_train: 2.16225, loss_in_test: 36.05466\n",
      "iter: 333, grad: 0.40131, loss_in_train: 2.16206, loss_in_test: 36.05202\n",
      "iter: 334, grad: 0.40035, loss_in_train: 2.16188, loss_in_test: 36.04939\n",
      "iter: 335, grad: 0.39938, loss_in_train: 2.16170, loss_in_test: 36.04678\n",
      "iter: 336, grad: 0.39842, loss_in_train: 2.16152, loss_in_test: 36.04418\n",
      "iter: 337, grad: 0.39747, loss_in_train: 2.16135, loss_in_test: 36.04159\n",
      "iter: 338, grad: 0.39651, loss_in_train: 2.16118, loss_in_test: 36.03902\n",
      "iter: 339, grad: 0.39556, loss_in_train: 2.16101, loss_in_test: 36.03646\n",
      "iter: 340, grad: 0.39461, loss_in_train: 2.16085, loss_in_test: 36.03392\n",
      "iter: 341, grad: 0.39366, loss_in_train: 2.16069, loss_in_test: 36.03139\n",
      "iter: 342, grad: 0.39271, loss_in_train: 2.16053, loss_in_test: 36.02887\n",
      "iter: 343, grad: 0.39177, loss_in_train: 2.16038, loss_in_test: 36.02636\n",
      "iter: 344, grad: 0.39083, loss_in_train: 2.16023, loss_in_test: 36.02387\n",
      "iter: 345, grad: 0.38989, loss_in_train: 2.16008, loss_in_test: 36.02139\n",
      "iter: 346, grad: 0.38896, loss_in_train: 2.15994, loss_in_test: 36.01893\n",
      "iter: 347, grad: 0.38802, loss_in_train: 2.15980, loss_in_test: 36.01648\n",
      "iter: 348, grad: 0.38709, loss_in_train: 2.15966, loss_in_test: 36.01404\n",
      "iter: 349, grad: 0.38616, loss_in_train: 2.15953, loss_in_test: 36.01162\n",
      "iter: 350, grad: 0.38523, loss_in_train: 2.15939, loss_in_test: 36.00920\n",
      "iter: 351, grad: 0.38431, loss_in_train: 2.15926, loss_in_test: 36.00680\n",
      "iter: 352, grad: 0.38339, loss_in_train: 2.15914, loss_in_test: 36.00442\n",
      "iter: 353, grad: 0.38247, loss_in_train: 2.15901, loss_in_test: 36.00205\n",
      "iter: 354, grad: 0.38155, loss_in_train: 2.15889, loss_in_test: 35.99969\n",
      "iter: 355, grad: 0.38064, loss_in_train: 2.15877, loss_in_test: 35.99734\n",
      "iter: 356, grad: 0.37972, loss_in_train: 2.15865, loss_in_test: 35.99500\n",
      "iter: 357, grad: 0.37881, loss_in_train: 2.15854, loss_in_test: 35.99268\n",
      "iter: 358, grad: 0.37790, loss_in_train: 2.15842, loss_in_test: 35.99037\n",
      "iter: 359, grad: 0.37700, loss_in_train: 2.15831, loss_in_test: 35.98807\n",
      "iter: 360, grad: 0.37609, loss_in_train: 2.15821, loss_in_test: 35.98579\n",
      "iter: 361, grad: 0.37519, loss_in_train: 2.15810, loss_in_test: 35.98352\n",
      "iter: 362, grad: 0.37429, loss_in_train: 2.15800, loss_in_test: 35.98126\n",
      "iter: 363, grad: 0.37340, loss_in_train: 2.15789, loss_in_test: 35.97901\n",
      "iter: 364, grad: 0.37250, loss_in_train: 2.15779, loss_in_test: 35.97678\n",
      "iter: 365, grad: 0.37161, loss_in_train: 2.15770, loss_in_test: 35.97455\n",
      "iter: 366, grad: 0.37072, loss_in_train: 2.15760, loss_in_test: 35.97234\n",
      "iter: 367, grad: 0.36983, loss_in_train: 2.15751, loss_in_test: 35.97015\n",
      "iter: 368, grad: 0.36895, loss_in_train: 2.15741, loss_in_test: 35.96796\n",
      "iter: 369, grad: 0.36806, loss_in_train: 2.15732, loss_in_test: 35.96579\n",
      "iter: 370, grad: 0.36718, loss_in_train: 2.15723, loss_in_test: 35.96362\n",
      "iter: 371, grad: 0.36630, loss_in_train: 2.15715, loss_in_test: 35.96147\n",
      "iter: 372, grad: 0.36543, loss_in_train: 2.15706, loss_in_test: 35.95933\n",
      "iter: 373, grad: 0.36455, loss_in_train: 2.15698, loss_in_test: 35.95721\n",
      "iter: 374, grad: 0.36368, loss_in_train: 2.15690, loss_in_test: 35.95509\n",
      "iter: 375, grad: 0.36281, loss_in_train: 2.15682, loss_in_test: 35.95299\n",
      "iter: 376, grad: 0.36194, loss_in_train: 2.15674, loss_in_test: 35.95090\n",
      "iter: 377, grad: 0.36107, loss_in_train: 2.15666, loss_in_test: 35.94882\n",
      "iter: 378, grad: 0.36021, loss_in_train: 2.15659, loss_in_test: 35.94675\n",
      "iter: 379, grad: 0.35935, loss_in_train: 2.15651, loss_in_test: 35.94469\n",
      "iter: 380, grad: 0.35849, loss_in_train: 2.15644, loss_in_test: 35.94265\n",
      "iter: 381, grad: 0.35763, loss_in_train: 2.15637, loss_in_test: 35.94061\n",
      "iter: 382, grad: 0.35678, loss_in_train: 2.15630, loss_in_test: 35.93859\n",
      "iter: 383, grad: 0.35592, loss_in_train: 2.15623, loss_in_test: 35.93658\n",
      "iter: 384, grad: 0.35507, loss_in_train: 2.15617, loss_in_test: 35.93458\n",
      "iter: 385, grad: 0.35422, loss_in_train: 2.15610, loss_in_test: 35.93259\n",
      "iter: 386, grad: 0.35338, loss_in_train: 2.15604, loss_in_test: 35.93061\n",
      "iter: 387, grad: 0.35253, loss_in_train: 2.15597, loss_in_test: 35.92865\n",
      "iter: 388, grad: 0.35169, loss_in_train: 2.15591, loss_in_test: 35.92669\n",
      "iter: 389, grad: 0.35085, loss_in_train: 2.15585, loss_in_test: 35.92474\n",
      "iter: 390, grad: 0.35001, loss_in_train: 2.15579, loss_in_test: 35.92281\n",
      "iter: 391, grad: 0.34918, loss_in_train: 2.15573, loss_in_test: 35.92089\n",
      "iter: 392, grad: 0.34834, loss_in_train: 2.15568, loss_in_test: 35.91898\n",
      "iter: 393, grad: 0.34751, loss_in_train: 2.15562, loss_in_test: 35.91707\n",
      "iter: 394, grad: 0.34668, loss_in_train: 2.15557, loss_in_test: 35.91518\n",
      "iter: 395, grad: 0.34585, loss_in_train: 2.15551, loss_in_test: 35.91330\n",
      "iter: 396, grad: 0.34503, loss_in_train: 2.15546, loss_in_test: 35.91143\n",
      "iter: 397, grad: 0.34420, loss_in_train: 2.15541, loss_in_test: 35.90957\n",
      "iter: 398, grad: 0.34338, loss_in_train: 2.15536, loss_in_test: 35.90773\n",
      "iter: 399, grad: 0.34256, loss_in_train: 2.15531, loss_in_test: 35.90589\n",
      "iter: 400, grad: 0.34175, loss_in_train: 2.15526, loss_in_test: 35.90406\n",
      "iter: 401, grad: 0.34093, loss_in_train: 2.15521, loss_in_test: 35.90224\n",
      "iter: 402, grad: 0.34012, loss_in_train: 2.15516, loss_in_test: 35.90044\n",
      "iter: 403, grad: 0.33931, loss_in_train: 2.15512, loss_in_test: 35.89864\n",
      "iter: 404, grad: 0.33850, loss_in_train: 2.15507, loss_in_test: 35.89685\n",
      "iter: 405, grad: 0.33769, loss_in_train: 2.15503, loss_in_test: 35.89508\n",
      "iter: 406, grad: 0.33688, loss_in_train: 2.15499, loss_in_test: 35.89331\n",
      "iter: 407, grad: 0.33608, loss_in_train: 2.15494, loss_in_test: 35.89155\n",
      "iter: 408, grad: 0.33528, loss_in_train: 2.15490, loss_in_test: 35.88981\n",
      "iter: 409, grad: 0.33448, loss_in_train: 2.15486, loss_in_test: 35.88807\n",
      "iter: 410, grad: 0.33368, loss_in_train: 2.15482, loss_in_test: 35.88634\n",
      "iter: 411, grad: 0.33289, loss_in_train: 2.15478, loss_in_test: 35.88463\n",
      "iter: 412, grad: 0.33210, loss_in_train: 2.15474, loss_in_test: 35.88292\n",
      "iter: 413, grad: 0.33130, loss_in_train: 2.15471, loss_in_test: 35.88122\n",
      "iter: 414, grad: 0.33052, loss_in_train: 2.15467, loss_in_test: 35.87954\n",
      "iter: 415, grad: 0.32973, loss_in_train: 2.15463, loss_in_test: 35.87786\n",
      "iter: 416, grad: 0.32894, loss_in_train: 2.15460, loss_in_test: 35.87619\n",
      "iter: 417, grad: 0.32816, loss_in_train: 2.15456, loss_in_test: 35.87453\n",
      "iter: 418, grad: 0.32738, loss_in_train: 2.15453, loss_in_test: 35.87288\n",
      "iter: 419, grad: 0.32660, loss_in_train: 2.15450, loss_in_test: 35.87124\n",
      "iter: 420, grad: 0.32582, loss_in_train: 2.15446, loss_in_test: 35.86961\n",
      "iter: 421, grad: 0.32505, loss_in_train: 2.15443, loss_in_test: 35.86799\n",
      "iter: 422, grad: 0.32427, loss_in_train: 2.15440, loss_in_test: 35.86638\n",
      "iter: 423, grad: 0.32350, loss_in_train: 2.15437, loss_in_test: 35.86478\n",
      "iter: 424, grad: 0.32273, loss_in_train: 2.15434, loss_in_test: 35.86319\n",
      "iter: 425, grad: 0.32196, loss_in_train: 2.15431, loss_in_test: 35.86160\n",
      "iter: 426, grad: 0.32120, loss_in_train: 2.15428, loss_in_test: 35.86003\n",
      "iter: 427, grad: 0.32043, loss_in_train: 2.15425, loss_in_test: 35.85846\n",
      "iter: 428, grad: 0.31967, loss_in_train: 2.15422, loss_in_test: 35.85691\n",
      "iter: 429, grad: 0.31891, loss_in_train: 2.15420, loss_in_test: 35.85536\n",
      "iter: 430, grad: 0.31815, loss_in_train: 2.15417, loss_in_test: 35.85382\n",
      "iter: 431, grad: 0.31740, loss_in_train: 2.15414, loss_in_test: 35.85229\n",
      "iter: 432, grad: 0.31664, loss_in_train: 2.15412, loss_in_test: 35.85077\n",
      "iter: 433, grad: 0.31589, loss_in_train: 2.15409, loss_in_test: 35.84926\n",
      "iter: 434, grad: 0.31514, loss_in_train: 2.15407, loss_in_test: 35.84776\n",
      "iter: 435, grad: 0.31439, loss_in_train: 2.15404, loss_in_test: 35.84626\n",
      "iter: 436, grad: 0.31365, loss_in_train: 2.15402, loss_in_test: 35.84478\n",
      "iter: 437, grad: 0.31290, loss_in_train: 2.15400, loss_in_test: 35.84330\n",
      "iter: 438, grad: 0.31216, loss_in_train: 2.15397, loss_in_test: 35.84183\n",
      "iter: 439, grad: 0.31142, loss_in_train: 2.15395, loss_in_test: 35.84037\n",
      "iter: 440, grad: 0.31068, loss_in_train: 2.15393, loss_in_test: 35.83892\n",
      "iter: 441, grad: 0.30994, loss_in_train: 2.15391, loss_in_test: 35.83748\n",
      "iter: 442, grad: 0.30920, loss_in_train: 2.15389, loss_in_test: 35.83605\n",
      "iter: 443, grad: 0.30847, loss_in_train: 2.15386, loss_in_test: 35.83462\n",
      "iter: 444, grad: 0.30774, loss_in_train: 2.15384, loss_in_test: 35.83320\n",
      "iter: 445, grad: 0.30701, loss_in_train: 2.15382, loss_in_test: 35.83179\n",
      "iter: 446, grad: 0.30628, loss_in_train: 2.15380, loss_in_test: 35.83039\n",
      "iter: 447, grad: 0.30555, loss_in_train: 2.15379, loss_in_test: 35.82900\n",
      "iter: 448, grad: 0.30483, loss_in_train: 2.15377, loss_in_test: 35.82761\n",
      "iter: 449, grad: 0.30410, loss_in_train: 2.15375, loss_in_test: 35.82624\n",
      "iter: 450, grad: 0.30338, loss_in_train: 2.15373, loss_in_test: 35.82487\n",
      "iter: 451, grad: 0.30266, loss_in_train: 2.15371, loss_in_test: 35.82351\n",
      "iter: 452, grad: 0.30195, loss_in_train: 2.15369, loss_in_test: 35.82216\n",
      "iter: 453, grad: 0.30123, loss_in_train: 2.15368, loss_in_test: 35.82081\n",
      "iter: 454, grad: 0.30052, loss_in_train: 2.15366, loss_in_test: 35.81948\n",
      "iter: 455, grad: 0.29981, loss_in_train: 2.15364, loss_in_test: 35.81815\n",
      "iter: 456, grad: 0.29909, loss_in_train: 2.15363, loss_in_test: 35.81683\n",
      "iter: 457, grad: 0.29839, loss_in_train: 2.15361, loss_in_test: 35.81551\n",
      "iter: 458, grad: 0.29768, loss_in_train: 2.15360, loss_in_test: 35.81421\n",
      "iter: 459, grad: 0.29697, loss_in_train: 2.15358, loss_in_test: 35.81291\n",
      "iter: 460, grad: 0.29627, loss_in_train: 2.15357, loss_in_test: 35.81162\n",
      "iter: 461, grad: 0.29557, loss_in_train: 2.15355, loss_in_test: 35.81034\n",
      "iter: 462, grad: 0.29487, loss_in_train: 2.15354, loss_in_test: 35.80906\n",
      "iter: 463, grad: 0.29417, loss_in_train: 2.15352, loss_in_test: 35.80780\n",
      "iter: 464, grad: 0.29348, loss_in_train: 2.15351, loss_in_test: 35.80654\n",
      "iter: 465, grad: 0.29278, loss_in_train: 2.15350, loss_in_test: 35.80529\n",
      "iter: 466, grad: 0.29209, loss_in_train: 2.15348, loss_in_test: 35.80404\n",
      "iter: 467, grad: 0.29140, loss_in_train: 2.15347, loss_in_test: 35.80281\n",
      "iter: 468, grad: 0.29071, loss_in_train: 2.15346, loss_in_test: 35.80158\n",
      "iter: 469, grad: 0.29002, loss_in_train: 2.15344, loss_in_test: 35.80035\n",
      "iter: 470, grad: 0.28933, loss_in_train: 2.15343, loss_in_test: 35.79914\n",
      "iter: 471, grad: 0.28865, loss_in_train: 2.15342, loss_in_test: 35.79793\n",
      "iter: 472, grad: 0.28797, loss_in_train: 2.15341, loss_in_test: 35.79673\n",
      "iter: 473, grad: 0.28729, loss_in_train: 2.15340, loss_in_test: 35.79554\n",
      "iter: 474, grad: 0.28661, loss_in_train: 2.15339, loss_in_test: 35.79435\n",
      "iter: 475, grad: 0.28593, loss_in_train: 2.15337, loss_in_test: 35.79317\n",
      "iter: 476, grad: 0.28526, loss_in_train: 2.15336, loss_in_test: 35.79200\n",
      "iter: 477, grad: 0.28458, loss_in_train: 2.15335, loss_in_test: 35.79083\n",
      "iter: 478, grad: 0.28391, loss_in_train: 2.15334, loss_in_test: 35.78968\n",
      "iter: 479, grad: 0.28324, loss_in_train: 2.15333, loss_in_test: 35.78852\n",
      "iter: 480, grad: 0.28257, loss_in_train: 2.15332, loss_in_test: 35.78738\n",
      "iter: 481, grad: 0.28190, loss_in_train: 2.15331, loss_in_test: 35.78624\n",
      "iter: 482, grad: 0.28124, loss_in_train: 2.15330, loss_in_test: 35.78511\n",
      "iter: 483, grad: 0.28057, loss_in_train: 2.15329, loss_in_test: 35.78399\n",
      "iter: 484, grad: 0.27991, loss_in_train: 2.15328, loss_in_test: 35.78287\n",
      "iter: 485, grad: 0.27925, loss_in_train: 2.15327, loss_in_test: 35.78176\n",
      "iter: 486, grad: 0.27859, loss_in_train: 2.15326, loss_in_test: 35.78066\n",
      "iter: 487, grad: 0.27793, loss_in_train: 2.15326, loss_in_test: 35.77956\n",
      "iter: 488, grad: 0.27728, loss_in_train: 2.15325, loss_in_test: 35.77847\n",
      "iter: 489, grad: 0.27662, loss_in_train: 2.15324, loss_in_test: 35.77739\n",
      "iter: 490, grad: 0.27597, loss_in_train: 2.15323, loss_in_test: 35.77631\n",
      "iter: 491, grad: 0.27532, loss_in_train: 2.15322, loss_in_test: 35.77524\n",
      "iter: 492, grad: 0.27467, loss_in_train: 2.15321, loss_in_test: 35.77417\n",
      "iter: 493, grad: 0.27403, loss_in_train: 2.15321, loss_in_test: 35.77312\n",
      "iter: 494, grad: 0.27338, loss_in_train: 2.15320, loss_in_test: 35.77207\n",
      "iter: 495, grad: 0.27274, loss_in_train: 2.15319, loss_in_test: 35.77102\n",
      "iter: 496, grad: 0.27209, loss_in_train: 2.15318, loss_in_test: 35.76998\n",
      "iter: 497, grad: 0.27145, loss_in_train: 2.15318, loss_in_test: 35.76895\n",
      "iter: 498, grad: 0.27081, loss_in_train: 2.15317, loss_in_test: 35.76792\n",
      "iter: 499, grad: 0.27017, loss_in_train: 2.15316, loss_in_test: 35.76690\n",
      "iter: 500, grad: 0.26954, loss_in_train: 2.15316, loss_in_test: 35.76589\n",
      "iter: 501, grad: 0.26890, loss_in_train: 2.15315, loss_in_test: 35.76488\n",
      "iter: 502, grad: 0.26827, loss_in_train: 2.15314, loss_in_test: 35.76388\n",
      "iter: 503, grad: 0.26764, loss_in_train: 2.15314, loss_in_test: 35.76289\n",
      "iter: 504, grad: 0.26701, loss_in_train: 2.15313, loss_in_test: 35.76190\n",
      "iter: 505, grad: 0.26638, loss_in_train: 2.15312, loss_in_test: 35.76092\n",
      "iter: 506, grad: 0.26575, loss_in_train: 2.15312, loss_in_test: 35.75994\n",
      "iter: 507, grad: 0.26513, loss_in_train: 2.15311, loss_in_test: 35.75897\n",
      "iter: 508, grad: 0.26451, loss_in_train: 2.15310, loss_in_test: 35.75800\n",
      "iter: 509, grad: 0.26388, loss_in_train: 2.15310, loss_in_test: 35.75705\n",
      "iter: 510, grad: 0.26326, loss_in_train: 2.15309, loss_in_test: 35.75609\n",
      "iter: 511, grad: 0.26264, loss_in_train: 2.15309, loss_in_test: 35.75515\n",
      "iter: 512, grad: 0.26203, loss_in_train: 2.15308, loss_in_test: 35.75420\n",
      "iter: 513, grad: 0.26141, loss_in_train: 2.15308, loss_in_test: 35.75327\n",
      "iter: 514, grad: 0.26080, loss_in_train: 2.15307, loss_in_test: 35.75234\n",
      "iter: 515, grad: 0.26018, loss_in_train: 2.15307, loss_in_test: 35.75141\n",
      "iter: 516, grad: 0.25957, loss_in_train: 2.15306, loss_in_test: 35.75050\n",
      "iter: 517, grad: 0.25896, loss_in_train: 2.15306, loss_in_test: 35.74958\n",
      "iter: 518, grad: 0.25835, loss_in_train: 2.15305, loss_in_test: 35.74868\n",
      "iter: 519, grad: 0.25775, loss_in_train: 2.15305, loss_in_test: 35.74778\n",
      "iter: 520, grad: 0.25714, loss_in_train: 2.15304, loss_in_test: 35.74688\n",
      "iter: 521, grad: 0.25654, loss_in_train: 2.15304, loss_in_test: 35.74599\n",
      "iter: 522, grad: 0.25594, loss_in_train: 2.15303, loss_in_test: 35.74510\n",
      "iter: 523, grad: 0.25534, loss_in_train: 2.15303, loss_in_test: 35.74423\n",
      "iter: 524, grad: 0.25474, loss_in_train: 2.15302, loss_in_test: 35.74335\n",
      "iter: 525, grad: 0.25414, loss_in_train: 2.15302, loss_in_test: 35.74248\n",
      "iter: 526, grad: 0.25354, loss_in_train: 2.15302, loss_in_test: 35.74162\n",
      "iter: 527, grad: 0.25295, loss_in_train: 2.15301, loss_in_test: 35.74076\n",
      "iter: 528, grad: 0.25236, loss_in_train: 2.15301, loss_in_test: 35.73991\n",
      "iter: 529, grad: 0.25176, loss_in_train: 2.15300, loss_in_test: 35.73906\n",
      "iter: 530, grad: 0.25117, loss_in_train: 2.15300, loss_in_test: 35.73822\n",
      "iter: 531, grad: 0.25059, loss_in_train: 2.15300, loss_in_test: 35.73739\n",
      "iter: 532, grad: 0.25000, loss_in_train: 2.15299, loss_in_test: 35.73655\n",
      "iter: 533, grad: 0.24941, loss_in_train: 2.15299, loss_in_test: 35.73573\n",
      "iter: 534, grad: 0.24883, loss_in_train: 2.15299, loss_in_test: 35.73491\n",
      "iter: 535, grad: 0.24825, loss_in_train: 2.15298, loss_in_test: 35.73409\n",
      "iter: 536, grad: 0.24766, loss_in_train: 2.15298, loss_in_test: 35.73328\n",
      "iter: 537, grad: 0.24708, loss_in_train: 2.15297, loss_in_test: 35.73248\n",
      "iter: 538, grad: 0.24651, loss_in_train: 2.15297, loss_in_test: 35.73168\n",
      "iter: 539, grad: 0.24593, loss_in_train: 2.15297, loss_in_test: 35.73088\n",
      "iter: 540, grad: 0.24535, loss_in_train: 2.15297, loss_in_test: 35.73009\n",
      "iter: 541, grad: 0.24478, loss_in_train: 2.15296, loss_in_test: 35.72931\n",
      "iter: 542, grad: 0.24421, loss_in_train: 2.15296, loss_in_test: 35.72853\n",
      "iter: 543, grad: 0.24363, loss_in_train: 2.15296, loss_in_test: 35.72775\n",
      "iter: 544, grad: 0.24306, loss_in_train: 2.15295, loss_in_test: 35.72698\n",
      "iter: 545, grad: 0.24250, loss_in_train: 2.15295, loss_in_test: 35.72621\n",
      "iter: 546, grad: 0.24193, loss_in_train: 2.15295, loss_in_test: 35.72545\n",
      "iter: 547, grad: 0.24136, loss_in_train: 2.15294, loss_in_test: 35.72470\n",
      "iter: 548, grad: 0.24080, loss_in_train: 2.15294, loss_in_test: 35.72395\n",
      "iter: 549, grad: 0.24024, loss_in_train: 2.15294, loss_in_test: 35.72320\n",
      "iter: 550, grad: 0.23968, loss_in_train: 2.15294, loss_in_test: 35.72246\n",
      "iter: 551, grad: 0.23912, loss_in_train: 2.15293, loss_in_test: 35.72172\n",
      "iter: 552, grad: 0.23856, loss_in_train: 2.15293, loss_in_test: 35.72099\n",
      "iter: 553, grad: 0.23800, loss_in_train: 2.15293, loss_in_test: 35.72026\n",
      "iter: 554, grad: 0.23744, loss_in_train: 2.15293, loss_in_test: 35.71954\n",
      "iter: 555, grad: 0.23689, loss_in_train: 2.15292, loss_in_test: 35.71882\n",
      "iter: 556, grad: 0.23634, loss_in_train: 2.15292, loss_in_test: 35.71811\n",
      "iter: 557, grad: 0.23579, loss_in_train: 2.15292, loss_in_test: 35.71740\n",
      "iter: 558, grad: 0.23523, loss_in_train: 2.15292, loss_in_test: 35.71669\n",
      "iter: 559, grad: 0.23469, loss_in_train: 2.15291, loss_in_test: 35.71599\n",
      "iter: 560, grad: 0.23414, loss_in_train: 2.15291, loss_in_test: 35.71530\n",
      "iter: 561, grad: 0.23359, loss_in_train: 2.15291, loss_in_test: 35.71461\n",
      "iter: 562, grad: 0.23305, loss_in_train: 2.15291, loss_in_test: 35.71392\n",
      "iter: 563, grad: 0.23250, loss_in_train: 2.15291, loss_in_test: 35.71324\n",
      "iter: 564, grad: 0.23196, loss_in_train: 2.15290, loss_in_test: 35.71256\n",
      "iter: 565, grad: 0.23142, loss_in_train: 2.15290, loss_in_test: 35.71189\n",
      "iter: 566, grad: 0.23088, loss_in_train: 2.15290, loss_in_test: 35.71122\n",
      "iter: 567, grad: 0.23034, loss_in_train: 2.15290, loss_in_test: 35.71055\n",
      "iter: 568, grad: 0.22981, loss_in_train: 2.15290, loss_in_test: 35.70989\n",
      "iter: 569, grad: 0.22927, loss_in_train: 2.15290, loss_in_test: 35.70923\n",
      "iter: 570, grad: 0.22874, loss_in_train: 2.15289, loss_in_test: 35.70858\n",
      "iter: 571, grad: 0.22821, loss_in_train: 2.15289, loss_in_test: 35.70793\n",
      "iter: 572, grad: 0.22767, loss_in_train: 2.15289, loss_in_test: 35.70729\n",
      "iter: 573, grad: 0.22714, loss_in_train: 2.15289, loss_in_test: 35.70665\n",
      "iter: 574, grad: 0.22662, loss_in_train: 2.15289, loss_in_test: 35.70602\n",
      "iter: 575, grad: 0.22609, loss_in_train: 2.15288, loss_in_test: 35.70539\n",
      "iter: 576, grad: 0.22556, loss_in_train: 2.15288, loss_in_test: 35.70476\n",
      "iter: 577, grad: 0.22504, loss_in_train: 2.15288, loss_in_test: 35.70414\n",
      "iter: 578, grad: 0.22451, loss_in_train: 2.15288, loss_in_test: 35.70352\n",
      "iter: 579, grad: 0.22399, loss_in_train: 2.15288, loss_in_test: 35.70290\n",
      "iter: 580, grad: 0.22347, loss_in_train: 2.15288, loss_in_test: 35.70229\n",
      "iter: 581, grad: 0.22295, loss_in_train: 2.15288, loss_in_test: 35.70168\n",
      "iter: 582, grad: 0.22243, loss_in_train: 2.15287, loss_in_test: 35.70108\n",
      "iter: 583, grad: 0.22192, loss_in_train: 2.15287, loss_in_test: 35.70048\n",
      "iter: 584, grad: 0.22140, loss_in_train: 2.15287, loss_in_test: 35.69989\n",
      "iter: 585, grad: 0.22089, loss_in_train: 2.15287, loss_in_test: 35.69930\n",
      "iter: 586, grad: 0.22037, loss_in_train: 2.15287, loss_in_test: 35.69871\n",
      "iter: 587, grad: 0.21986, loss_in_train: 2.15287, loss_in_test: 35.69813\n",
      "iter: 588, grad: 0.21935, loss_in_train: 2.15287, loss_in_test: 35.69755\n",
      "iter: 589, grad: 0.21884, loss_in_train: 2.15286, loss_in_test: 35.69697\n",
      "iter: 590, grad: 0.21834, loss_in_train: 2.15286, loss_in_test: 35.69640\n",
      "iter: 591, grad: 0.21783, loss_in_train: 2.15286, loss_in_test: 35.69583\n",
      "iter: 592, grad: 0.21732, loss_in_train: 2.15286, loss_in_test: 35.69527\n",
      "iter: 593, grad: 0.21682, loss_in_train: 2.15286, loss_in_test: 35.69471\n",
      "iter: 594, grad: 0.21632, loss_in_train: 2.15286, loss_in_test: 35.69415\n",
      "iter: 595, grad: 0.21582, loss_in_train: 2.15286, loss_in_test: 35.69360\n",
      "iter: 596, grad: 0.21532, loss_in_train: 2.15286, loss_in_test: 35.69305\n",
      "iter: 597, grad: 0.21482, loss_in_train: 2.15286, loss_in_test: 35.69250\n",
      "iter: 598, grad: 0.21432, loss_in_train: 2.15285, loss_in_test: 35.69196\n",
      "iter: 599, grad: 0.21382, loss_in_train: 2.15285, loss_in_test: 35.69142\n",
      "iter: 600, grad: 0.21333, loss_in_train: 2.15285, loss_in_test: 35.69089\n",
      "iter: 601, grad: 0.21283, loss_in_train: 2.15285, loss_in_test: 35.69035\n",
      "iter: 602, grad: 0.21234, loss_in_train: 2.15285, loss_in_test: 35.68983\n",
      "iter: 603, grad: 0.21185, loss_in_train: 2.15285, loss_in_test: 35.68930\n",
      "iter: 604, grad: 0.21136, loss_in_train: 2.15285, loss_in_test: 35.68878\n",
      "iter: 605, grad: 0.21087, loss_in_train: 2.15285, loss_in_test: 35.68826\n",
      "iter: 606, grad: 0.21038, loss_in_train: 2.15285, loss_in_test: 35.68775\n",
      "iter: 607, grad: 0.20989, loss_in_train: 2.15285, loss_in_test: 35.68724\n",
      "iter: 608, grad: 0.20941, loss_in_train: 2.15285, loss_in_test: 35.68673\n",
      "iter: 609, grad: 0.20892, loss_in_train: 2.15284, loss_in_test: 35.68623\n",
      "iter: 610, grad: 0.20844, loss_in_train: 2.15284, loss_in_test: 35.68573\n",
      "iter: 611, grad: 0.20796, loss_in_train: 2.15284, loss_in_test: 35.68523\n",
      "iter: 612, grad: 0.20748, loss_in_train: 2.15284, loss_in_test: 35.68474\n",
      "iter: 613, grad: 0.20700, loss_in_train: 2.15284, loss_in_test: 35.68425\n",
      "iter: 614, grad: 0.20652, loss_in_train: 2.15284, loss_in_test: 35.68376\n",
      "iter: 615, grad: 0.20604, loss_in_train: 2.15284, loss_in_test: 35.68328\n",
      "iter: 616, grad: 0.20557, loss_in_train: 2.15284, loss_in_test: 35.68280\n",
      "iter: 617, grad: 0.20509, loss_in_train: 2.15284, loss_in_test: 35.68232\n",
      "iter: 618, grad: 0.20462, loss_in_train: 2.15284, loss_in_test: 35.68185\n",
      "iter: 619, grad: 0.20415, loss_in_train: 2.15284, loss_in_test: 35.68138\n",
      "iter: 620, grad: 0.20368, loss_in_train: 2.15284, loss_in_test: 35.68091\n",
      "iter: 621, grad: 0.20321, loss_in_train: 2.15284, loss_in_test: 35.68045\n",
      "iter: 622, grad: 0.20274, loss_in_train: 2.15283, loss_in_test: 35.67999\n",
      "iter: 623, grad: 0.20227, loss_in_train: 2.15283, loss_in_test: 35.67953\n",
      "iter: 624, grad: 0.20180, loss_in_train: 2.15283, loss_in_test: 35.67907\n",
      "iter: 625, grad: 0.20134, loss_in_train: 2.15283, loss_in_test: 35.67862\n",
      "iter: 626, grad: 0.20087, loss_in_train: 2.15283, loss_in_test: 35.67817\n",
      "iter: 627, grad: 0.20041, loss_in_train: 2.15283, loss_in_test: 35.67773\n",
      "iter: 628, grad: 0.19995, loss_in_train: 2.15283, loss_in_test: 35.67729\n",
      "iter: 629, grad: 0.19949, loss_in_train: 2.15283, loss_in_test: 35.67685\n",
      "iter: 630, grad: 0.19903, loss_in_train: 2.15283, loss_in_test: 35.67641\n",
      "iter: 631, grad: 0.19857, loss_in_train: 2.15283, loss_in_test: 35.67598\n",
      "iter: 632, grad: 0.19811, loss_in_train: 2.15283, loss_in_test: 35.67555\n",
      "iter: 633, grad: 0.19766, loss_in_train: 2.15283, loss_in_test: 35.67512\n",
      "iter: 634, grad: 0.19720, loss_in_train: 2.15283, loss_in_test: 35.67470\n",
      "iter: 635, grad: 0.19675, loss_in_train: 2.15283, loss_in_test: 35.67428\n",
      "iter: 636, grad: 0.19630, loss_in_train: 2.15283, loss_in_test: 35.67386\n",
      "iter: 637, grad: 0.19585, loss_in_train: 2.15283, loss_in_test: 35.67344\n",
      "iter: 638, grad: 0.19539, loss_in_train: 2.15283, loss_in_test: 35.67303\n",
      "iter: 639, grad: 0.19495, loss_in_train: 2.15283, loss_in_test: 35.67262\n",
      "iter: 640, grad: 0.19450, loss_in_train: 2.15282, loss_in_test: 35.67222\n",
      "iter: 641, grad: 0.19405, loss_in_train: 2.15282, loss_in_test: 35.67181\n",
      "iter: 642, grad: 0.19360, loss_in_train: 2.15282, loss_in_test: 35.67141\n",
      "iter: 643, grad: 0.19316, loss_in_train: 2.15282, loss_in_test: 35.67101\n",
      "iter: 644, grad: 0.19272, loss_in_train: 2.15282, loss_in_test: 35.67062\n",
      "iter: 645, grad: 0.19227, loss_in_train: 2.15282, loss_in_test: 35.67023\n",
      "iter: 646, grad: 0.19183, loss_in_train: 2.15282, loss_in_test: 35.66984\n",
      "iter: 647, grad: 0.19139, loss_in_train: 2.15282, loss_in_test: 35.66945\n",
      "iter: 648, grad: 0.19095, loss_in_train: 2.15282, loss_in_test: 35.66906\n",
      "iter: 649, grad: 0.19051, loss_in_train: 2.15282, loss_in_test: 35.66868\n",
      "iter: 650, grad: 0.19008, loss_in_train: 2.15282, loss_in_test: 35.66830\n",
      "iter: 651, grad: 0.18964, loss_in_train: 2.15282, loss_in_test: 35.66793\n",
      "iter: 652, grad: 0.18921, loss_in_train: 2.15282, loss_in_test: 35.66755\n",
      "iter: 653, grad: 0.18877, loss_in_train: 2.15282, loss_in_test: 35.66718\n",
      "iter: 654, grad: 0.18834, loss_in_train: 2.15282, loss_in_test: 35.66682\n",
      "iter: 655, grad: 0.18791, loss_in_train: 2.15282, loss_in_test: 35.66645\n",
      "iter: 656, grad: 0.18748, loss_in_train: 2.15282, loss_in_test: 35.66609\n",
      "iter: 657, grad: 0.18705, loss_in_train: 2.15282, loss_in_test: 35.66573\n",
      "iter: 658, grad: 0.18662, loss_in_train: 2.15282, loss_in_test: 35.66537\n",
      "iter: 659, grad: 0.18619, loss_in_train: 2.15282, loss_in_test: 35.66501\n",
      "iter: 660, grad: 0.18577, loss_in_train: 2.15282, loss_in_test: 35.66466\n",
      "iter: 661, grad: 0.18534, loss_in_train: 2.15282, loss_in_test: 35.66431\n",
      "iter: 662, grad: 0.18492, loss_in_train: 2.15282, loss_in_test: 35.66397\n",
      "iter: 663, grad: 0.18449, loss_in_train: 2.15282, loss_in_test: 35.66362\n",
      "iter: 664, grad: 0.18407, loss_in_train: 2.15282, loss_in_test: 35.66328\n",
      "iter: 665, grad: 0.18365, loss_in_train: 2.15282, loss_in_test: 35.66294\n",
      "iter: 666, grad: 0.18323, loss_in_train: 2.15281, loss_in_test: 35.66260\n",
      "iter: 667, grad: 0.18281, loss_in_train: 2.15281, loss_in_test: 35.66227\n",
      "iter: 668, grad: 0.18239, loss_in_train: 2.15281, loss_in_test: 35.66193\n",
      "iter: 669, grad: 0.18198, loss_in_train: 2.15281, loss_in_test: 35.66160\n",
      "iter: 670, grad: 0.18156, loss_in_train: 2.15281, loss_in_test: 35.66127\n",
      "iter: 671, grad: 0.18115, loss_in_train: 2.15281, loss_in_test: 35.66095\n",
      "iter: 672, grad: 0.18073, loss_in_train: 2.15281, loss_in_test: 35.66063\n",
      "iter: 673, grad: 0.18032, loss_in_train: 2.15281, loss_in_test: 35.66031\n",
      "iter: 674, grad: 0.17991, loss_in_train: 2.15281, loss_in_test: 35.65999\n",
      "iter: 675, grad: 0.17950, loss_in_train: 2.15281, loss_in_test: 35.65967\n",
      "iter: 676, grad: 0.17909, loss_in_train: 2.15281, loss_in_test: 35.65936\n",
      "iter: 677, grad: 0.17868, loss_in_train: 2.15281, loss_in_test: 35.65905\n",
      "iter: 678, grad: 0.17827, loss_in_train: 2.15281, loss_in_test: 35.65874\n",
      "iter: 679, grad: 0.17787, loss_in_train: 2.15281, loss_in_test: 35.65843\n",
      "iter: 680, grad: 0.17746, loss_in_train: 2.15281, loss_in_test: 35.65813\n",
      "iter: 681, grad: 0.17706, loss_in_train: 2.15281, loss_in_test: 35.65783\n",
      "iter: 682, grad: 0.17665, loss_in_train: 2.15281, loss_in_test: 35.65753\n",
      "iter: 683, grad: 0.17625, loss_in_train: 2.15281, loss_in_test: 35.65723\n",
      "iter: 684, grad: 0.17585, loss_in_train: 2.15281, loss_in_test: 35.65693\n",
      "iter: 685, grad: 0.17545, loss_in_train: 2.15281, loss_in_test: 35.65664\n",
      "iter: 686, grad: 0.17505, loss_in_train: 2.15281, loss_in_test: 35.65635\n",
      "iter: 687, grad: 0.17465, loss_in_train: 2.15281, loss_in_test: 35.65606\n",
      "iter: 688, grad: 0.17426, loss_in_train: 2.15281, loss_in_test: 35.65577\n",
      "iter: 689, grad: 0.17386, loss_in_train: 2.15281, loss_in_test: 35.65549\n",
      "iter: 690, grad: 0.17346, loss_in_train: 2.15281, loss_in_test: 35.65521\n",
      "iter: 691, grad: 0.17307, loss_in_train: 2.15281, loss_in_test: 35.65493\n",
      "iter: 692, grad: 0.17268, loss_in_train: 2.15281, loss_in_test: 35.65465\n",
      "iter: 693, grad: 0.17228, loss_in_train: 2.15281, loss_in_test: 35.65437\n",
      "iter: 694, grad: 0.17189, loss_in_train: 2.15281, loss_in_test: 35.65410\n",
      "iter: 695, grad: 0.17150, loss_in_train: 2.15281, loss_in_test: 35.65383\n",
      "iter: 696, grad: 0.17111, loss_in_train: 2.15281, loss_in_test: 35.65356\n",
      "iter: 697, grad: 0.17072, loss_in_train: 2.15281, loss_in_test: 35.65329\n",
      "iter: 698, grad: 0.17034, loss_in_train: 2.15281, loss_in_test: 35.65302\n",
      "iter: 699, grad: 0.16995, loss_in_train: 2.15281, loss_in_test: 35.65276\n",
      "iter: 700, grad: 0.16956, loss_in_train: 2.15281, loss_in_test: 35.65250\n",
      "iter: 701, grad: 0.16918, loss_in_train: 2.15281, loss_in_test: 35.65224\n",
      "iter: 702, grad: 0.16880, loss_in_train: 2.15281, loss_in_test: 35.65198\n",
      "iter: 703, grad: 0.16841, loss_in_train: 2.15281, loss_in_test: 35.65173\n",
      "iter: 704, grad: 0.16803, loss_in_train: 2.15281, loss_in_test: 35.65147\n",
      "iter: 705, grad: 0.16765, loss_in_train: 2.15281, loss_in_test: 35.65122\n",
      "iter: 706, grad: 0.16727, loss_in_train: 2.15281, loss_in_test: 35.65097\n",
      "iter: 707, grad: 0.16689, loss_in_train: 2.15281, loss_in_test: 35.65073\n",
      "iter: 708, grad: 0.16651, loss_in_train: 2.15281, loss_in_test: 35.65048\n",
      "iter: 709, grad: 0.16614, loss_in_train: 2.15281, loss_in_test: 35.65024\n",
      "iter: 710, grad: 0.16576, loss_in_train: 2.15281, loss_in_test: 35.64999\n",
      "iter: 711, grad: 0.16538, loss_in_train: 2.15281, loss_in_test: 35.64975\n",
      "iter: 712, grad: 0.16501, loss_in_train: 2.15281, loss_in_test: 35.64952\n",
      "iter: 713, grad: 0.16464, loss_in_train: 2.15281, loss_in_test: 35.64928\n",
      "iter: 714, grad: 0.16426, loss_in_train: 2.15281, loss_in_test: 35.64905\n",
      "iter: 715, grad: 0.16389, loss_in_train: 2.15281, loss_in_test: 35.64881\n",
      "iter: 716, grad: 0.16352, loss_in_train: 2.15281, loss_in_test: 35.64858\n",
      "iter: 717, grad: 0.16315, loss_in_train: 2.15281, loss_in_test: 35.64835\n",
      "iter: 718, grad: 0.16278, loss_in_train: 2.15281, loss_in_test: 35.64813\n",
      "iter: 719, grad: 0.16242, loss_in_train: 2.15281, loss_in_test: 35.64790\n",
      "iter: 720, grad: 0.16205, loss_in_train: 2.15281, loss_in_test: 35.64768\n",
      "iter: 721, grad: 0.16168, loss_in_train: 2.15281, loss_in_test: 35.64746\n",
      "iter: 722, grad: 0.16132, loss_in_train: 2.15281, loss_in_test: 35.64724\n",
      "iter: 723, grad: 0.16096, loss_in_train: 2.15281, loss_in_test: 35.64702\n",
      "iter: 724, grad: 0.16059, loss_in_train: 2.15280, loss_in_test: 35.64680\n",
      "iter: 725, grad: 0.16023, loss_in_train: 2.15280, loss_in_test: 35.64659\n",
      "iter: 726, grad: 0.15987, loss_in_train: 2.15280, loss_in_test: 35.64638\n",
      "iter: 727, grad: 0.15951, loss_in_train: 2.15280, loss_in_test: 35.64617\n",
      "iter: 728, grad: 0.15915, loss_in_train: 2.15280, loss_in_test: 35.64596\n",
      "iter: 729, grad: 0.15879, loss_in_train: 2.15280, loss_in_test: 35.64575\n",
      "iter: 730, grad: 0.15843, loss_in_train: 2.15280, loss_in_test: 35.64554\n",
      "iter: 731, grad: 0.15808, loss_in_train: 2.15280, loss_in_test: 35.64534\n",
      "iter: 732, grad: 0.15772, loss_in_train: 2.15280, loss_in_test: 35.64514\n",
      "iter: 733, grad: 0.15736, loss_in_train: 2.15280, loss_in_test: 35.64494\n",
      "iter: 734, grad: 0.15701, loss_in_train: 2.15280, loss_in_test: 35.64474\n",
      "iter: 735, grad: 0.15666, loss_in_train: 2.15280, loss_in_test: 35.64454\n",
      "iter: 736, grad: 0.15630, loss_in_train: 2.15280, loss_in_test: 35.64434\n",
      "iter: 737, grad: 0.15595, loss_in_train: 2.15280, loss_in_test: 35.64415\n",
      "iter: 738, grad: 0.15560, loss_in_train: 2.15280, loss_in_test: 35.64396\n",
      "iter: 739, grad: 0.15525, loss_in_train: 2.15280, loss_in_test: 35.64376\n",
      "iter: 740, grad: 0.15490, loss_in_train: 2.15280, loss_in_test: 35.64358\n",
      "iter: 741, grad: 0.15456, loss_in_train: 2.15280, loss_in_test: 35.64339\n",
      "iter: 742, grad: 0.15421, loss_in_train: 2.15280, loss_in_test: 35.64320\n",
      "iter: 743, grad: 0.15386, loss_in_train: 2.15280, loss_in_test: 35.64302\n",
      "iter: 744, grad: 0.15352, loss_in_train: 2.15280, loss_in_test: 35.64283\n",
      "iter: 745, grad: 0.15317, loss_in_train: 2.15280, loss_in_test: 35.64265\n",
      "iter: 746, grad: 0.15283, loss_in_train: 2.15280, loss_in_test: 35.64247\n",
      "iter: 747, grad: 0.15249, loss_in_train: 2.15280, loss_in_test: 35.64229\n",
      "iter: 748, grad: 0.15214, loss_in_train: 2.15280, loss_in_test: 35.64212\n",
      "iter: 749, grad: 0.15180, loss_in_train: 2.15280, loss_in_test: 35.64194\n",
      "iter: 750, grad: 0.15146, loss_in_train: 2.15280, loss_in_test: 35.64177\n",
      "iter: 751, grad: 0.15112, loss_in_train: 2.15280, loss_in_test: 35.64159\n",
      "iter: 752, grad: 0.15078, loss_in_train: 2.15280, loss_in_test: 35.64142\n",
      "iter: 753, grad: 0.15045, loss_in_train: 2.15280, loss_in_test: 35.64125\n",
      "iter: 754, grad: 0.15011, loss_in_train: 2.15280, loss_in_test: 35.64109\n",
      "iter: 755, grad: 0.14977, loss_in_train: 2.15280, loss_in_test: 35.64092\n",
      "iter: 756, grad: 0.14944, loss_in_train: 2.15280, loss_in_test: 35.64075\n",
      "iter: 757, grad: 0.14910, loss_in_train: 2.15280, loss_in_test: 35.64059\n",
      "iter: 758, grad: 0.14877, loss_in_train: 2.15280, loss_in_test: 35.64043\n",
      "iter: 759, grad: 0.14844, loss_in_train: 2.15280, loss_in_test: 35.64027\n",
      "iter: 760, grad: 0.14811, loss_in_train: 2.15280, loss_in_test: 35.64011\n",
      "iter: 761, grad: 0.14778, loss_in_train: 2.15280, loss_in_test: 35.63995\n",
      "iter: 762, grad: 0.14745, loss_in_train: 2.15280, loss_in_test: 35.63979\n",
      "iter: 763, grad: 0.14712, loss_in_train: 2.15280, loss_in_test: 35.63964\n",
      "iter: 764, grad: 0.14679, loss_in_train: 2.15280, loss_in_test: 35.63948\n",
      "iter: 765, grad: 0.14646, loss_in_train: 2.15280, loss_in_test: 35.63933\n",
      "iter: 766, grad: 0.14613, loss_in_train: 2.15280, loss_in_test: 35.63918\n",
      "iter: 767, grad: 0.14581, loss_in_train: 2.15280, loss_in_test: 35.63903\n",
      "iter: 768, grad: 0.14548, loss_in_train: 2.15280, loss_in_test: 35.63888\n",
      "iter: 769, grad: 0.14516, loss_in_train: 2.15280, loss_in_test: 35.63874\n",
      "iter: 770, grad: 0.14483, loss_in_train: 2.15280, loss_in_test: 35.63859\n",
      "iter: 771, grad: 0.14451, loss_in_train: 2.15280, loss_in_test: 35.63845\n",
      "iter: 772, grad: 0.14419, loss_in_train: 2.15280, loss_in_test: 35.63830\n",
      "iter: 773, grad: 0.14387, loss_in_train: 2.15280, loss_in_test: 35.63816\n",
      "iter: 774, grad: 0.14355, loss_in_train: 2.15280, loss_in_test: 35.63802\n",
      "iter: 775, grad: 0.14323, loss_in_train: 2.15280, loss_in_test: 35.63788\n",
      "iter: 776, grad: 0.14291, loss_in_train: 2.15280, loss_in_test: 35.63774\n",
      "iter: 777, grad: 0.14259, loss_in_train: 2.15280, loss_in_test: 35.63761\n",
      "iter: 778, grad: 0.14227, loss_in_train: 2.15280, loss_in_test: 35.63747\n",
      "iter: 779, grad: 0.14196, loss_in_train: 2.15280, loss_in_test: 35.63734\n",
      "iter: 780, grad: 0.14164, loss_in_train: 2.15280, loss_in_test: 35.63720\n",
      "iter: 781, grad: 0.14133, loss_in_train: 2.15280, loss_in_test: 35.63707\n",
      "iter: 782, grad: 0.14101, loss_in_train: 2.15280, loss_in_test: 35.63694\n",
      "iter: 783, grad: 0.14070, loss_in_train: 2.15280, loss_in_test: 35.63681\n",
      "iter: 784, grad: 0.14039, loss_in_train: 2.15280, loss_in_test: 35.63669\n",
      "iter: 785, grad: 0.14007, loss_in_train: 2.15280, loss_in_test: 35.63656\n",
      "iter: 786, grad: 0.13976, loss_in_train: 2.15280, loss_in_test: 35.63643\n",
      "iter: 787, grad: 0.13945, loss_in_train: 2.15280, loss_in_test: 35.63631\n",
      "iter: 788, grad: 0.13914, loss_in_train: 2.15280, loss_in_test: 35.63619\n",
      "iter: 789, grad: 0.13884, loss_in_train: 2.15280, loss_in_test: 35.63606\n",
      "iter: 790, grad: 0.13853, loss_in_train: 2.15280, loss_in_test: 35.63594\n",
      "iter: 791, grad: 0.13822, loss_in_train: 2.15280, loss_in_test: 35.63582\n",
      "iter: 792, grad: 0.13791, loss_in_train: 2.15280, loss_in_test: 35.63571\n",
      "iter: 793, grad: 0.13761, loss_in_train: 2.15280, loss_in_test: 35.63559\n",
      "iter: 794, grad: 0.13730, loss_in_train: 2.15280, loss_in_test: 35.63547\n",
      "iter: 795, grad: 0.13700, loss_in_train: 2.15280, loss_in_test: 35.63536\n",
      "iter: 796, grad: 0.13670, loss_in_train: 2.15280, loss_in_test: 35.63524\n",
      "iter: 797, grad: 0.13639, loss_in_train: 2.15280, loss_in_test: 35.63513\n",
      "iter: 798, grad: 0.13609, loss_in_train: 2.15280, loss_in_test: 35.63502\n",
      "iter: 799, grad: 0.13579, loss_in_train: 2.15280, loss_in_test: 35.63491\n",
      "iter: 800, grad: 0.13549, loss_in_train: 2.15280, loss_in_test: 35.63480\n",
      "iter: 801, grad: 0.13519, loss_in_train: 2.15280, loss_in_test: 35.63469\n",
      "iter: 802, grad: 0.13489, loss_in_train: 2.15280, loss_in_test: 35.63458\n",
      "iter: 803, grad: 0.13460, loss_in_train: 2.15280, loss_in_test: 35.63448\n",
      "iter: 804, grad: 0.13430, loss_in_train: 2.15280, loss_in_test: 35.63437\n",
      "iter: 805, grad: 0.13400, loss_in_train: 2.15280, loss_in_test: 35.63427\n",
      "iter: 806, grad: 0.13371, loss_in_train: 2.15280, loss_in_test: 35.63417\n",
      "iter: 807, grad: 0.13341, loss_in_train: 2.15280, loss_in_test: 35.63407\n",
      "iter: 808, grad: 0.13312, loss_in_train: 2.15280, loss_in_test: 35.63396\n",
      "iter: 809, grad: 0.13282, loss_in_train: 2.15280, loss_in_test: 35.63387\n",
      "iter: 810, grad: 0.13253, loss_in_train: 2.15280, loss_in_test: 35.63377\n",
      "iter: 811, grad: 0.13224, loss_in_train: 2.15280, loss_in_test: 35.63367\n",
      "iter: 812, grad: 0.13195, loss_in_train: 2.15280, loss_in_test: 35.63357\n",
      "iter: 813, grad: 0.13166, loss_in_train: 2.15280, loss_in_test: 35.63348\n",
      "iter: 814, grad: 0.13137, loss_in_train: 2.15280, loss_in_test: 35.63338\n",
      "iter: 815, grad: 0.13108, loss_in_train: 2.15280, loss_in_test: 35.63329\n",
      "iter: 816, grad: 0.13079, loss_in_train: 2.15280, loss_in_test: 35.63320\n",
      "iter: 817, grad: 0.13050, loss_in_train: 2.15280, loss_in_test: 35.63310\n",
      "iter: 818, grad: 0.13021, loss_in_train: 2.15280, loss_in_test: 35.63301\n",
      "iter: 819, grad: 0.12993, loss_in_train: 2.15280, loss_in_test: 35.63292\n",
      "iter: 820, grad: 0.12964, loss_in_train: 2.15280, loss_in_test: 35.63284\n",
      "iter: 821, grad: 0.12936, loss_in_train: 2.15280, loss_in_test: 35.63275\n",
      "iter: 822, grad: 0.12907, loss_in_train: 2.15280, loss_in_test: 35.63266\n",
      "iter: 823, grad: 0.12879, loss_in_train: 2.15280, loss_in_test: 35.63258\n",
      "iter: 824, grad: 0.12851, loss_in_train: 2.15280, loss_in_test: 35.63249\n",
      "iter: 825, grad: 0.12823, loss_in_train: 2.15280, loss_in_test: 35.63241\n",
      "iter: 826, grad: 0.12794, loss_in_train: 2.15280, loss_in_test: 35.63233\n",
      "iter: 827, grad: 0.12766, loss_in_train: 2.15280, loss_in_test: 35.63224\n",
      "iter: 828, grad: 0.12738, loss_in_train: 2.15280, loss_in_test: 35.63216\n",
      "iter: 829, grad: 0.12710, loss_in_train: 2.15280, loss_in_test: 35.63208\n",
      "iter: 830, grad: 0.12683, loss_in_train: 2.15280, loss_in_test: 35.63200\n",
      "iter: 831, grad: 0.12655, loss_in_train: 2.15280, loss_in_test: 35.63192\n",
      "iter: 832, grad: 0.12627, loss_in_train: 2.15280, loss_in_test: 35.63185\n",
      "iter: 833, grad: 0.12599, loss_in_train: 2.15280, loss_in_test: 35.63177\n",
      "iter: 834, grad: 0.12572, loss_in_train: 2.15280, loss_in_test: 35.63170\n",
      "iter: 835, grad: 0.12544, loss_in_train: 2.15280, loss_in_test: 35.63162\n",
      "iter: 836, grad: 0.12517, loss_in_train: 2.15280, loss_in_test: 35.63155\n",
      "iter: 837, grad: 0.12490, loss_in_train: 2.15280, loss_in_test: 35.63147\n",
      "iter: 838, grad: 0.12462, loss_in_train: 2.15280, loss_in_test: 35.63140\n",
      "iter: 839, grad: 0.12435, loss_in_train: 2.15280, loss_in_test: 35.63133\n",
      "iter: 840, grad: 0.12408, loss_in_train: 2.15280, loss_in_test: 35.63126\n",
      "iter: 841, grad: 0.12381, loss_in_train: 2.15280, loss_in_test: 35.63119\n",
      "iter: 842, grad: 0.12354, loss_in_train: 2.15280, loss_in_test: 35.63112\n",
      "iter: 843, grad: 0.12327, loss_in_train: 2.15280, loss_in_test: 35.63105\n",
      "iter: 844, grad: 0.12300, loss_in_train: 2.15280, loss_in_test: 35.63099\n",
      "iter: 845, grad: 0.12273, loss_in_train: 2.15280, loss_in_test: 35.63092\n",
      "iter: 846, grad: 0.12246, loss_in_train: 2.15280, loss_in_test: 35.63086\n",
      "iter: 847, grad: 0.12220, loss_in_train: 2.15280, loss_in_test: 35.63079\n",
      "iter: 848, grad: 0.12193, loss_in_train: 2.15280, loss_in_test: 35.63073\n",
      "iter: 849, grad: 0.12166, loss_in_train: 2.15280, loss_in_test: 35.63067\n",
      "iter: 850, grad: 0.12140, loss_in_train: 2.15280, loss_in_test: 35.63060\n",
      "iter: 851, grad: 0.12114, loss_in_train: 2.15280, loss_in_test: 35.63054\n",
      "iter: 852, grad: 0.12087, loss_in_train: 2.15280, loss_in_test: 35.63048\n",
      "iter: 853, grad: 0.12061, loss_in_train: 2.15280, loss_in_test: 35.63042\n",
      "iter: 854, grad: 0.12035, loss_in_train: 2.15280, loss_in_test: 35.63036\n",
      "iter: 855, grad: 0.12008, loss_in_train: 2.15280, loss_in_test: 35.63031\n",
      "iter: 856, grad: 0.11982, loss_in_train: 2.15280, loss_in_test: 35.63025\n",
      "iter: 857, grad: 0.11956, loss_in_train: 2.15280, loss_in_test: 35.63019\n",
      "iter: 858, grad: 0.11930, loss_in_train: 2.15280, loss_in_test: 35.63014\n",
      "iter: 859, grad: 0.11904, loss_in_train: 2.15280, loss_in_test: 35.63008\n",
      "iter: 860, grad: 0.11879, loss_in_train: 2.15280, loss_in_test: 35.63003\n",
      "iter: 861, grad: 0.11853, loss_in_train: 2.15280, loss_in_test: 35.62997\n",
      "iter: 862, grad: 0.11827, loss_in_train: 2.15280, loss_in_test: 35.62992\n",
      "iter: 863, grad: 0.11802, loss_in_train: 2.15280, loss_in_test: 35.62987\n",
      "iter: 864, grad: 0.11776, loss_in_train: 2.15280, loss_in_test: 35.62982\n",
      "iter: 865, grad: 0.11750, loss_in_train: 2.15280, loss_in_test: 35.62977\n",
      "iter: 866, grad: 0.11725, loss_in_train: 2.15280, loss_in_test: 35.62972\n",
      "iter: 867, grad: 0.11700, loss_in_train: 2.15280, loss_in_test: 35.62967\n",
      "iter: 868, grad: 0.11674, loss_in_train: 2.15280, loss_in_test: 35.62962\n",
      "iter: 869, grad: 0.11649, loss_in_train: 2.15280, loss_in_test: 35.62957\n",
      "iter: 870, grad: 0.11624, loss_in_train: 2.15280, loss_in_test: 35.62952\n",
      "iter: 871, grad: 0.11599, loss_in_train: 2.15280, loss_in_test: 35.62948\n",
      "iter: 872, grad: 0.11574, loss_in_train: 2.15280, loss_in_test: 35.62943\n",
      "iter: 873, grad: 0.11549, loss_in_train: 2.15280, loss_in_test: 35.62939\n",
      "iter: 874, grad: 0.11524, loss_in_train: 2.15280, loss_in_test: 35.62934\n",
      "iter: 875, grad: 0.11499, loss_in_train: 2.15280, loss_in_test: 35.62930\n",
      "iter: 876, grad: 0.11474, loss_in_train: 2.15280, loss_in_test: 35.62926\n",
      "iter: 877, grad: 0.11449, loss_in_train: 2.15280, loss_in_test: 35.62921\n",
      "iter: 878, grad: 0.11424, loss_in_train: 2.15280, loss_in_test: 35.62917\n",
      "iter: 879, grad: 0.11400, loss_in_train: 2.15280, loss_in_test: 35.62913\n",
      "iter: 880, grad: 0.11375, loss_in_train: 2.15280, loss_in_test: 35.62909\n",
      "iter: 881, grad: 0.11351, loss_in_train: 2.15280, loss_in_test: 35.62905\n",
      "iter: 882, grad: 0.11326, loss_in_train: 2.15280, loss_in_test: 35.62901\n",
      "iter: 883, grad: 0.11302, loss_in_train: 2.15280, loss_in_test: 35.62898\n",
      "iter: 884, grad: 0.11278, loss_in_train: 2.15280, loss_in_test: 35.62894\n",
      "iter: 885, grad: 0.11253, loss_in_train: 2.15280, loss_in_test: 35.62890\n",
      "iter: 886, grad: 0.11229, loss_in_train: 2.15280, loss_in_test: 35.62886\n",
      "iter: 887, grad: 0.11205, loss_in_train: 2.15280, loss_in_test: 35.62883\n",
      "iter: 888, grad: 0.11181, loss_in_train: 2.15280, loss_in_test: 35.62879\n",
      "iter: 889, grad: 0.11157, loss_in_train: 2.15280, loss_in_test: 35.62876\n",
      "iter: 890, grad: 0.11133, loss_in_train: 2.15280, loss_in_test: 35.62873\n",
      "iter: 891, grad: 0.11109, loss_in_train: 2.15280, loss_in_test: 35.62869\n",
      "iter: 892, grad: 0.11085, loss_in_train: 2.15280, loss_in_test: 35.62866\n",
      "iter: 893, grad: 0.11061, loss_in_train: 2.15280, loss_in_test: 35.62863\n",
      "iter: 894, grad: 0.11038, loss_in_train: 2.15280, loss_in_test: 35.62860\n",
      "iter: 895, grad: 0.11014, loss_in_train: 2.15280, loss_in_test: 35.62857\n",
      "iter: 896, grad: 0.10990, loss_in_train: 2.15280, loss_in_test: 35.62854\n",
      "iter: 897, grad: 0.10967, loss_in_train: 2.15280, loss_in_test: 35.62851\n",
      "iter: 898, grad: 0.10943, loss_in_train: 2.15280, loss_in_test: 35.62848\n",
      "iter: 899, grad: 0.10920, loss_in_train: 2.15280, loss_in_test: 35.62845\n",
      "iter: 900, grad: 0.10896, loss_in_train: 2.15280, loss_in_test: 35.62842\n",
      "iter: 901, grad: 0.10873, loss_in_train: 2.15280, loss_in_test: 35.62839\n",
      "iter: 902, grad: 0.10850, loss_in_train: 2.15280, loss_in_test: 35.62837\n",
      "iter: 903, grad: 0.10827, loss_in_train: 2.15280, loss_in_test: 35.62834\n",
      "iter: 904, grad: 0.10804, loss_in_train: 2.15280, loss_in_test: 35.62831\n",
      "iter: 905, grad: 0.10780, loss_in_train: 2.15280, loss_in_test: 35.62829\n",
      "iter: 906, grad: 0.10757, loss_in_train: 2.15280, loss_in_test: 35.62826\n",
      "iter: 907, grad: 0.10734, loss_in_train: 2.15280, loss_in_test: 35.62824\n",
      "iter: 908, grad: 0.10712, loss_in_train: 2.15280, loss_in_test: 35.62822\n",
      "iter: 909, grad: 0.10689, loss_in_train: 2.15280, loss_in_test: 35.62819\n",
      "iter: 910, grad: 0.10666, loss_in_train: 2.15280, loss_in_test: 35.62817\n",
      "iter: 911, grad: 0.10643, loss_in_train: 2.15280, loss_in_test: 35.62815\n",
      "iter: 912, grad: 0.10621, loss_in_train: 2.15280, loss_in_test: 35.62813\n",
      "iter: 913, grad: 0.10598, loss_in_train: 2.15280, loss_in_test: 35.62811\n",
      "iter: 914, grad: 0.10575, loss_in_train: 2.15280, loss_in_test: 35.62809\n",
      "iter: 915, grad: 0.10553, loss_in_train: 2.15280, loss_in_test: 35.62807\n",
      "iter: 916, grad: 0.10530, loss_in_train: 2.15280, loss_in_test: 35.62805\n",
      "iter: 917, grad: 0.10508, loss_in_train: 2.15280, loss_in_test: 35.62803\n",
      "iter: 918, grad: 0.10486, loss_in_train: 2.15280, loss_in_test: 35.62801\n",
      "iter: 919, grad: 0.10463, loss_in_train: 2.15280, loss_in_test: 35.62799\n",
      "iter: 920, grad: 0.10441, loss_in_train: 2.15280, loss_in_test: 35.62798\n",
      "iter: 921, grad: 0.10419, loss_in_train: 2.15280, loss_in_test: 35.62796\n",
      "iter: 922, grad: 0.10397, loss_in_train: 2.15280, loss_in_test: 35.62794\n",
      "iter: 923, grad: 0.10375, loss_in_train: 2.15280, loss_in_test: 35.62793\n",
      "iter: 924, grad: 0.10353, loss_in_train: 2.15280, loss_in_test: 35.62791\n",
      "iter: 925, grad: 0.10331, loss_in_train: 2.15280, loss_in_test: 35.62790\n",
      "iter: 926, grad: 0.10309, loss_in_train: 2.15280, loss_in_test: 35.62788\n",
      "iter: 927, grad: 0.10287, loss_in_train: 2.15280, loss_in_test: 35.62787\n",
      "iter: 928, grad: 0.10265, loss_in_train: 2.15280, loss_in_test: 35.62786\n",
      "iter: 929, grad: 0.10243, loss_in_train: 2.15280, loss_in_test: 35.62784\n",
      "iter: 930, grad: 0.10222, loss_in_train: 2.15280, loss_in_test: 35.62783\n",
      "iter: 931, grad: 0.10200, loss_in_train: 2.15280, loss_in_test: 35.62782\n",
      "iter: 932, grad: 0.10179, loss_in_train: 2.15280, loss_in_test: 35.62781\n",
      "iter: 933, grad: 0.10157, loss_in_train: 2.15280, loss_in_test: 35.62780\n",
      "iter: 934, grad: 0.10136, loss_in_train: 2.15280, loss_in_test: 35.62779\n",
      "iter: 935, grad: 0.10114, loss_in_train: 2.15280, loss_in_test: 35.62778\n",
      "iter: 936, grad: 0.10093, loss_in_train: 2.15280, loss_in_test: 35.62777\n",
      "iter: 937, grad: 0.10072, loss_in_train: 2.15280, loss_in_test: 35.62776\n",
      "iter: 938, grad: 0.10050, loss_in_train: 2.15280, loss_in_test: 35.62775\n",
      "iter: 939, grad: 0.10029, loss_in_train: 2.15280, loss_in_test: 35.62774\n",
      "iter: 940, grad: 0.10008, loss_in_train: 2.15280, loss_in_test: 35.62773\n",
      "iter: 941, grad: 0.09987, loss_in_train: 2.15280, loss_in_test: 35.62772\n"
     ]
    }
   ],
   "source": [
    "reg = GradientDescentLinerRegressionWithRegularization(learning_rate=1e-3, _lambda=100, min_grad=0.1, max_item=5000, seed=1024)\n",
    "reg.fit(train_x, train_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1, grad: 0.99524, loss_in_train: 30.00431, loss_in_test: 35.12043\n",
      "iter: 2, grad: 0.99052, loss_in_train: 29.21537, loss_in_test: 35.16077\n",
      "iter: 3, grad: 0.98587, loss_in_train: 28.44995, loss_in_test: 35.20093\n",
      "iter: 4, grad: 0.98129, loss_in_train: 27.70729, loss_in_test: 35.24087\n",
      "iter: 5, grad: 0.97679, loss_in_train: 26.98667, loss_in_test: 35.28055\n",
      "iter: 6, grad: 0.97235, loss_in_train: 26.28739, loss_in_test: 35.31994\n",
      "iter: 7, grad: 0.96799, loss_in_train: 25.60878, loss_in_test: 35.35899\n",
      "iter: 8, grad: 0.96369, loss_in_train: 24.95019, loss_in_test: 35.39768\n",
      "iter: 9, grad: 0.95945, loss_in_train: 24.31099, loss_in_test: 35.43598\n",
      "iter: 10, grad: 0.95527, loss_in_train: 23.69056, loss_in_test: 35.47386\n",
      "iter: 11, grad: 0.95116, loss_in_train: 23.08833, loss_in_test: 35.51129\n",
      "iter: 12, grad: 0.94710, loss_in_train: 22.50373, loss_in_test: 35.54827\n",
      "iter: 13, grad: 0.94310, loss_in_train: 21.93619, loss_in_test: 35.58476\n",
      "iter: 14, grad: 0.93916, loss_in_train: 21.38520, loss_in_test: 35.62075\n",
      "iter: 15, grad: 0.93527, loss_in_train: 20.85025, loss_in_test: 35.65623\n",
      "iter: 16, grad: 0.93144, loss_in_train: 20.33082, loss_in_test: 35.69118\n",
      "iter: 17, grad: 0.92765, loss_in_train: 19.82645, loss_in_test: 35.72559\n",
      "iter: 18, grad: 0.92392, loss_in_train: 19.33667, loss_in_test: 35.75945\n",
      "iter: 19, grad: 0.92024, loss_in_train: 18.86103, loss_in_test: 35.79275\n",
      "iter: 20, grad: 0.91660, loss_in_train: 18.39910, loss_in_test: 35.82547\n",
      "iter: 21, grad: 0.91301, loss_in_train: 17.95045, loss_in_test: 35.85763\n",
      "iter: 22, grad: 0.90946, loss_in_train: 17.51469, loss_in_test: 35.88920\n",
      "iter: 23, grad: 0.90596, loss_in_train: 17.09142, loss_in_test: 35.92019\n",
      "iter: 24, grad: 0.90251, loss_in_train: 16.68025, loss_in_test: 35.95059\n",
      "iter: 25, grad: 0.89909, loss_in_train: 16.28083, loss_in_test: 35.98040\n",
      "iter: 26, grad: 0.89572, loss_in_train: 15.89279, loss_in_test: 36.00962\n",
      "iter: 27, grad: 0.89238, loss_in_train: 15.51580, loss_in_test: 36.03825\n",
      "iter: 28, grad: 0.88909, loss_in_train: 15.14952, loss_in_test: 36.06628\n",
      "iter: 29, grad: 0.88583, loss_in_train: 14.79362, loss_in_test: 36.09372\n",
      "iter: 30, grad: 0.88261, loss_in_train: 14.44780, loss_in_test: 36.12058\n",
      "iter: 31, grad: 0.87943, loss_in_train: 14.11175, loss_in_test: 36.14684\n",
      "iter: 32, grad: 0.87628, loss_in_train: 13.78519, loss_in_test: 36.17251\n",
      "iter: 33, grad: 0.87317, loss_in_train: 13.46783, loss_in_test: 36.19761\n",
      "iter: 34, grad: 0.87009, loss_in_train: 13.15939, loss_in_test: 36.22212\n",
      "iter: 35, grad: 0.86704, loss_in_train: 12.85962, loss_in_test: 36.24606\n",
      "iter: 36, grad: 0.86402, loss_in_train: 12.56825, loss_in_test: 36.26943\n",
      "iter: 37, grad: 0.86104, loss_in_train: 12.28504, loss_in_test: 36.29223\n",
      "iter: 38, grad: 0.85809, loss_in_train: 12.00975, loss_in_test: 36.31447\n",
      "iter: 39, grad: 0.85516, loss_in_train: 11.74214, loss_in_test: 36.33615\n",
      "iter: 40, grad: 0.85227, loss_in_train: 11.48199, loss_in_test: 36.35728\n",
      "iter: 41, grad: 0.84940, loss_in_train: 11.22908, loss_in_test: 36.37787\n",
      "iter: 42, grad: 0.84656, loss_in_train: 10.98320, loss_in_test: 36.39792\n",
      "iter: 43, grad: 0.84375, loss_in_train: 10.74415, loss_in_test: 36.41743\n",
      "iter: 44, grad: 0.84097, loss_in_train: 10.51172, loss_in_test: 36.43642\n",
      "iter: 45, grad: 0.83821, loss_in_train: 10.28573, loss_in_test: 36.45489\n",
      "iter: 46, grad: 0.83548, loss_in_train: 10.06599, loss_in_test: 36.47285\n",
      "iter: 47, grad: 0.83277, loss_in_train: 9.85232, loss_in_test: 36.49031\n",
      "iter: 48, grad: 0.83008, loss_in_train: 9.64454, loss_in_test: 36.50726\n",
      "iter: 49, grad: 0.82742, loss_in_train: 9.44248, loss_in_test: 36.52372\n",
      "iter: 50, grad: 0.82478, loss_in_train: 9.24599, loss_in_test: 36.53970\n",
      "iter: 51, grad: 0.82217, loss_in_train: 9.05489, loss_in_test: 36.55520\n",
      "iter: 52, grad: 0.81958, loss_in_train: 8.86905, loss_in_test: 36.57024\n",
      "iter: 53, grad: 0.81700, loss_in_train: 8.68830, loss_in_test: 36.58481\n",
      "iter: 54, grad: 0.81445, loss_in_train: 8.51250, loss_in_test: 36.59892\n",
      "iter: 55, grad: 0.81192, loss_in_train: 8.34152, loss_in_test: 36.61259\n",
      "iter: 56, grad: 0.80942, loss_in_train: 8.17521, loss_in_test: 36.62582\n",
      "iter: 57, grad: 0.80693, loss_in_train: 8.01345, loss_in_test: 36.63861\n",
      "iter: 58, grad: 0.80446, loss_in_train: 7.85611, loss_in_test: 36.65098\n",
      "iter: 59, grad: 0.80201, loss_in_train: 7.70306, loss_in_test: 36.66293\n",
      "iter: 60, grad: 0.79957, loss_in_train: 7.55418, loss_in_test: 36.67447\n",
      "iter: 61, grad: 0.79716, loss_in_train: 7.40935, loss_in_test: 36.68561\n",
      "iter: 62, grad: 0.79476, loss_in_train: 7.26847, loss_in_test: 36.69635\n",
      "iter: 63, grad: 0.79238, loss_in_train: 7.13142, loss_in_test: 36.70671\n",
      "iter: 64, grad: 0.79002, loss_in_train: 6.99809, loss_in_test: 36.71668\n",
      "iter: 65, grad: 0.78768, loss_in_train: 6.86839, loss_in_test: 36.72628\n",
      "iter: 66, grad: 0.78535, loss_in_train: 6.74221, loss_in_test: 36.73551\n",
      "iter: 67, grad: 0.78304, loss_in_train: 6.61945, loss_in_test: 36.74438\n",
      "iter: 68, grad: 0.78074, loss_in_train: 6.50002, loss_in_test: 36.75290\n",
      "iter: 69, grad: 0.77846, loss_in_train: 6.38383, loss_in_test: 36.76107\n",
      "iter: 70, grad: 0.77620, loss_in_train: 6.27079, loss_in_test: 36.76891\n",
      "iter: 71, grad: 0.77395, loss_in_train: 6.16081, loss_in_test: 36.77641\n",
      "iter: 72, grad: 0.77171, loss_in_train: 6.05381, loss_in_test: 36.78358\n",
      "iter: 73, grad: 0.76949, loss_in_train: 5.94971, loss_in_test: 36.79044\n",
      "iter: 74, grad: 0.76728, loss_in_train: 5.84842, loss_in_test: 36.79699\n",
      "iter: 75, grad: 0.76509, loss_in_train: 5.74987, loss_in_test: 36.80323\n",
      "iter: 76, grad: 0.76291, loss_in_train: 5.65399, loss_in_test: 36.80917\n",
      "iter: 77, grad: 0.76074, loss_in_train: 5.56070, loss_in_test: 36.81482\n",
      "iter: 78, grad: 0.75859, loss_in_train: 5.46993, loss_in_test: 36.82018\n",
      "iter: 79, grad: 0.75644, loss_in_train: 5.38162, loss_in_test: 36.82526\n",
      "iter: 80, grad: 0.75432, loss_in_train: 5.29569, loss_in_test: 36.83006\n",
      "iter: 81, grad: 0.75220, loss_in_train: 5.21209, loss_in_test: 36.83460\n",
      "iter: 82, grad: 0.75010, loss_in_train: 5.13075, loss_in_test: 36.83888\n",
      "iter: 83, grad: 0.74800, loss_in_train: 5.05160, loss_in_test: 36.84290\n",
      "iter: 84, grad: 0.74592, loss_in_train: 4.97459, loss_in_test: 36.84667\n",
      "iter: 85, grad: 0.74386, loss_in_train: 4.89967, loss_in_test: 36.85019\n",
      "iter: 86, grad: 0.74180, loss_in_train: 4.82676, loss_in_test: 36.85347\n",
      "iter: 87, grad: 0.73975, loss_in_train: 4.75583, loss_in_test: 36.85652\n",
      "iter: 88, grad: 0.73772, loss_in_train: 4.68682, loss_in_test: 36.85934\n",
      "iter: 89, grad: 0.73569, loss_in_train: 4.61967, loss_in_test: 36.86194\n",
      "iter: 90, grad: 0.73368, loss_in_train: 4.55433, loss_in_test: 36.86432\n",
      "iter: 91, grad: 0.73168, loss_in_train: 4.49077, loss_in_test: 36.86649\n",
      "iter: 92, grad: 0.72968, loss_in_train: 4.42892, loss_in_test: 36.86845\n",
      "iter: 93, grad: 0.72770, loss_in_train: 4.36874, loss_in_test: 36.87020\n",
      "iter: 94, grad: 0.72573, loss_in_train: 4.31019, loss_in_test: 36.87176\n",
      "iter: 95, grad: 0.72376, loss_in_train: 4.25322, loss_in_test: 36.87312\n",
      "iter: 96, grad: 0.72181, loss_in_train: 4.19780, loss_in_test: 36.87430\n",
      "iter: 97, grad: 0.71987, loss_in_train: 4.14387, loss_in_test: 36.87529\n",
      "iter: 98, grad: 0.71793, loss_in_train: 4.09140, loss_in_test: 36.87610\n",
      "iter: 99, grad: 0.71601, loss_in_train: 4.04036, loss_in_test: 36.87674\n",
      "iter: 100, grad: 0.71409, loss_in_train: 3.99069, loss_in_test: 36.87720\n",
      "iter: 101, grad: 0.71218, loss_in_train: 3.94237, loss_in_test: 36.87750\n",
      "iter: 102, grad: 0.71028, loss_in_train: 3.89536, loss_in_test: 36.87764\n",
      "iter: 103, grad: 0.70839, loss_in_train: 3.84961, loss_in_test: 36.87762\n",
      "iter: 104, grad: 0.70651, loss_in_train: 3.80511, loss_in_test: 36.87745\n",
      "iter: 105, grad: 0.70464, loss_in_train: 3.76181, loss_in_test: 36.87712\n",
      "iter: 106, grad: 0.70277, loss_in_train: 3.71969, loss_in_test: 36.87665\n",
      "iter: 107, grad: 0.70092, loss_in_train: 3.67870, loss_in_test: 36.87604\n",
      "iter: 108, grad: 0.69907, loss_in_train: 3.63883, loss_in_test: 36.87529\n",
      "iter: 109, grad: 0.69723, loss_in_train: 3.60003, loss_in_test: 36.87440\n",
      "iter: 110, grad: 0.69540, loss_in_train: 3.56229, loss_in_test: 36.87338\n",
      "iter: 111, grad: 0.69357, loss_in_train: 3.52557, loss_in_test: 36.87223\n",
      "iter: 112, grad: 0.69175, loss_in_train: 3.48984, loss_in_test: 36.87096\n",
      "iter: 113, grad: 0.68994, loss_in_train: 3.45508, loss_in_test: 36.86957\n",
      "iter: 114, grad: 0.68814, loss_in_train: 3.42126, loss_in_test: 36.86807\n",
      "iter: 115, grad: 0.68635, loss_in_train: 3.38835, loss_in_test: 36.86644\n",
      "iter: 116, grad: 0.68456, loss_in_train: 3.35634, loss_in_test: 36.86471\n",
      "iter: 117, grad: 0.68278, loss_in_train: 3.32519, loss_in_test: 36.86287\n",
      "iter: 118, grad: 0.68101, loss_in_train: 3.29489, loss_in_test: 36.86092\n",
      "iter: 119, grad: 0.67924, loss_in_train: 3.26540, loss_in_test: 36.85887\n",
      "iter: 120, grad: 0.67748, loss_in_train: 3.23672, loss_in_test: 36.85672\n",
      "iter: 121, grad: 0.67573, loss_in_train: 3.20880, loss_in_test: 36.85448\n",
      "iter: 122, grad: 0.67398, loss_in_train: 3.18165, loss_in_test: 36.85214\n",
      "iter: 123, grad: 0.67225, loss_in_train: 3.15522, loss_in_test: 36.84971\n",
      "iter: 124, grad: 0.67051, loss_in_train: 3.12951, loss_in_test: 36.84719\n",
      "iter: 125, grad: 0.66879, loss_in_train: 3.10450, loss_in_test: 36.84459\n",
      "iter: 126, grad: 0.66707, loss_in_train: 3.08015, loss_in_test: 36.84190\n",
      "iter: 127, grad: 0.66536, loss_in_train: 3.05647, loss_in_test: 36.83914\n",
      "iter: 128, grad: 0.66365, loss_in_train: 3.03342, loss_in_test: 36.83630\n",
      "iter: 129, grad: 0.66195, loss_in_train: 3.01100, loss_in_test: 36.83338\n",
      "iter: 130, grad: 0.66026, loss_in_train: 2.98917, loss_in_test: 36.83039\n",
      "iter: 131, grad: 0.65857, loss_in_train: 2.96794, loss_in_test: 36.82732\n",
      "iter: 132, grad: 0.65689, loss_in_train: 2.94727, loss_in_test: 36.82419\n",
      "iter: 133, grad: 0.65521, loss_in_train: 2.92717, loss_in_test: 36.82100\n",
      "iter: 134, grad: 0.65354, loss_in_train: 2.90760, loss_in_test: 36.81773\n",
      "iter: 135, grad: 0.65188, loss_in_train: 2.88855, loss_in_test: 36.81441\n",
      "iter: 136, grad: 0.65022, loss_in_train: 2.87002, loss_in_test: 36.81103\n",
      "iter: 137, grad: 0.64857, loss_in_train: 2.85198, loss_in_test: 36.80759\n",
      "iter: 138, grad: 0.64692, loss_in_train: 2.83442, loss_in_test: 36.80409\n",
      "iter: 139, grad: 0.64528, loss_in_train: 2.81734, loss_in_test: 36.80054\n",
      "iter: 140, grad: 0.64364, loss_in_train: 2.80070, loss_in_test: 36.79693\n",
      "iter: 141, grad: 0.64201, loss_in_train: 2.78452, loss_in_test: 36.79328\n",
      "iter: 142, grad: 0.64039, loss_in_train: 2.76876, loss_in_test: 36.78958\n",
      "iter: 143, grad: 0.63877, loss_in_train: 2.75343, loss_in_test: 36.78583\n",
      "iter: 144, grad: 0.63716, loss_in_train: 2.73850, loss_in_test: 36.78203\n",
      "iter: 145, grad: 0.63555, loss_in_train: 2.72396, loss_in_test: 36.77819\n",
      "iter: 146, grad: 0.63395, loss_in_train: 2.70982, loss_in_test: 36.77431\n",
      "iter: 147, grad: 0.63235, loss_in_train: 2.69604, loss_in_test: 36.77039\n",
      "iter: 148, grad: 0.63076, loss_in_train: 2.68264, loss_in_test: 36.76643\n",
      "iter: 149, grad: 0.62917, loss_in_train: 2.66958, loss_in_test: 36.76244\n",
      "iter: 150, grad: 0.62759, loss_in_train: 2.65687, loss_in_test: 36.75840\n",
      "iter: 151, grad: 0.62601, loss_in_train: 2.64450, loss_in_test: 36.75434\n",
      "iter: 152, grad: 0.62444, loss_in_train: 2.63245, loss_in_test: 36.75024\n",
      "iter: 153, grad: 0.62288, loss_in_train: 2.62071, loss_in_test: 36.74611\n",
      "iter: 154, grad: 0.62131, loss_in_train: 2.60929, loss_in_test: 36.74195\n",
      "iter: 155, grad: 0.61976, loss_in_train: 2.59816, loss_in_test: 36.73776\n",
      "iter: 156, grad: 0.61821, loss_in_train: 2.58733, loss_in_test: 36.73354\n",
      "iter: 157, grad: 0.61666, loss_in_train: 2.57677, loss_in_test: 36.72930\n",
      "iter: 158, grad: 0.61512, loss_in_train: 2.56650, loss_in_test: 36.72503\n",
      "iter: 159, grad: 0.61358, loss_in_train: 2.55649, loss_in_test: 36.72074\n",
      "iter: 160, grad: 0.61205, loss_in_train: 2.54674, loss_in_test: 36.71642\n",
      "iter: 161, grad: 0.61052, loss_in_train: 2.53724, loss_in_test: 36.71209\n",
      "iter: 162, grad: 0.60900, loss_in_train: 2.52799, loss_in_test: 36.70773\n",
      "iter: 163, grad: 0.60748, loss_in_train: 2.51898, loss_in_test: 36.70336\n",
      "iter: 164, grad: 0.60596, loss_in_train: 2.51020, loss_in_test: 36.69896\n",
      "iter: 165, grad: 0.60445, loss_in_train: 2.50165, loss_in_test: 36.69455\n",
      "iter: 166, grad: 0.60295, loss_in_train: 2.49332, loss_in_test: 36.69012\n",
      "iter: 167, grad: 0.60145, loss_in_train: 2.48520, loss_in_test: 36.68568\n",
      "iter: 168, grad: 0.59995, loss_in_train: 2.47729, loss_in_test: 36.68122\n",
      "iter: 169, grad: 0.59846, loss_in_train: 2.46958, loss_in_test: 36.67675\n",
      "iter: 170, grad: 0.59698, loss_in_train: 2.46207, loss_in_test: 36.67227\n",
      "iter: 171, grad: 0.59549, loss_in_train: 2.45475, loss_in_test: 36.66777\n",
      "iter: 172, grad: 0.59402, loss_in_train: 2.44761, loss_in_test: 36.66327\n",
      "iter: 173, grad: 0.59254, loss_in_train: 2.44066, loss_in_test: 36.65875\n",
      "iter: 174, grad: 0.59107, loss_in_train: 2.43388, loss_in_test: 36.65423\n",
      "iter: 175, grad: 0.58961, loss_in_train: 2.42728, loss_in_test: 36.64970\n",
      "iter: 176, grad: 0.58815, loss_in_train: 2.42084, loss_in_test: 36.64516\n",
      "iter: 177, grad: 0.58669, loss_in_train: 2.41457, loss_in_test: 36.64061\n",
      "iter: 178, grad: 0.58524, loss_in_train: 2.40845, loss_in_test: 36.63606\n",
      "iter: 179, grad: 0.58379, loss_in_train: 2.40248, loss_in_test: 36.63150\n",
      "iter: 180, grad: 0.58235, loss_in_train: 2.39667, loss_in_test: 36.62693\n",
      "iter: 181, grad: 0.58091, loss_in_train: 2.39100, loss_in_test: 36.62237\n",
      "iter: 182, grad: 0.57947, loss_in_train: 2.38547, loss_in_test: 36.61780\n",
      "iter: 183, grad: 0.57804, loss_in_train: 2.38008, loss_in_test: 36.61323\n",
      "iter: 184, grad: 0.57661, loss_in_train: 2.37483, loss_in_test: 36.60865\n",
      "iter: 185, grad: 0.57519, loss_in_train: 2.36970, loss_in_test: 36.60407\n",
      "iter: 186, grad: 0.57377, loss_in_train: 2.36470, loss_in_test: 36.59950\n",
      "iter: 187, grad: 0.57236, loss_in_train: 2.35983, loss_in_test: 36.59492\n",
      "iter: 188, grad: 0.57095, loss_in_train: 2.35508, loss_in_test: 36.59034\n",
      "iter: 189, grad: 0.56954, loss_in_train: 2.35044, loss_in_test: 36.58577\n",
      "iter: 190, grad: 0.56814, loss_in_train: 2.34591, loss_in_test: 36.58119\n",
      "iter: 191, grad: 0.56674, loss_in_train: 2.34150, loss_in_test: 36.57662\n",
      "iter: 192, grad: 0.56534, loss_in_train: 2.33720, loss_in_test: 36.57205\n",
      "iter: 193, grad: 0.56395, loss_in_train: 2.33300, loss_in_test: 36.56748\n",
      "iter: 194, grad: 0.56256, loss_in_train: 2.32890, loss_in_test: 36.56292\n",
      "iter: 195, grad: 0.56118, loss_in_train: 2.32491, loss_in_test: 36.55836\n",
      "iter: 196, grad: 0.55980, loss_in_train: 2.32101, loss_in_test: 36.55380\n",
      "iter: 197, grad: 0.55842, loss_in_train: 2.31720, loss_in_test: 36.54925\n",
      "iter: 198, grad: 0.55705, loss_in_train: 2.31349, loss_in_test: 36.54471\n",
      "iter: 199, grad: 0.55568, loss_in_train: 2.30986, loss_in_test: 36.54017\n",
      "iter: 200, grad: 0.55432, loss_in_train: 2.30633, loss_in_test: 36.53563\n",
      "iter: 201, grad: 0.55296, loss_in_train: 2.30288, loss_in_test: 36.53111\n",
      "iter: 202, grad: 0.55160, loss_in_train: 2.29951, loss_in_test: 36.52659\n",
      "iter: 203, grad: 0.55025, loss_in_train: 2.29622, loss_in_test: 36.52207\n",
      "iter: 204, grad: 0.54890, loss_in_train: 2.29301, loss_in_test: 36.51757\n",
      "iter: 205, grad: 0.54755, loss_in_train: 2.28988, loss_in_test: 36.51307\n",
      "iter: 206, grad: 0.54621, loss_in_train: 2.28682, loss_in_test: 36.50858\n",
      "iter: 207, grad: 0.54487, loss_in_train: 2.28384, loss_in_test: 36.50409\n",
      "iter: 208, grad: 0.54353, loss_in_train: 2.28092, loss_in_test: 36.49962\n",
      "iter: 209, grad: 0.54220, loss_in_train: 2.27808, loss_in_test: 36.49516\n",
      "iter: 210, grad: 0.54087, loss_in_train: 2.27530, loss_in_test: 36.49070\n",
      "iter: 211, grad: 0.53955, loss_in_train: 2.27259, loss_in_test: 36.48626\n",
      "iter: 212, grad: 0.53823, loss_in_train: 2.26994, loss_in_test: 36.48182\n",
      "iter: 213, grad: 0.53691, loss_in_train: 2.26736, loss_in_test: 36.47740\n",
      "iter: 214, grad: 0.53560, loss_in_train: 2.26483, loss_in_test: 36.47298\n",
      "iter: 215, grad: 0.53429, loss_in_train: 2.26236, loss_in_test: 36.46858\n",
      "iter: 216, grad: 0.53298, loss_in_train: 2.25996, loss_in_test: 36.46419\n",
      "iter: 217, grad: 0.53168, loss_in_train: 2.25760, loss_in_test: 36.45981\n",
      "iter: 218, grad: 0.53038, loss_in_train: 2.25531, loss_in_test: 36.45544\n",
      "iter: 219, grad: 0.52908, loss_in_train: 2.25306, loss_in_test: 36.45108\n",
      "iter: 220, grad: 0.52779, loss_in_train: 2.25087, loss_in_test: 36.44673\n",
      "iter: 221, grad: 0.52650, loss_in_train: 2.24873, loss_in_test: 36.44240\n",
      "iter: 222, grad: 0.52521, loss_in_train: 2.24664, loss_in_test: 36.43807\n",
      "iter: 223, grad: 0.52393, loss_in_train: 2.24460, loss_in_test: 36.43376\n",
      "iter: 224, grad: 0.52265, loss_in_train: 2.24260, loss_in_test: 36.42947\n",
      "iter: 225, grad: 0.52138, loss_in_train: 2.24065, loss_in_test: 36.42518\n",
      "iter: 226, grad: 0.52010, loss_in_train: 2.23875, loss_in_test: 36.42091\n",
      "iter: 227, grad: 0.51884, loss_in_train: 2.23688, loss_in_test: 36.41665\n",
      "iter: 228, grad: 0.51757, loss_in_train: 2.23507, loss_in_test: 36.41241\n",
      "iter: 229, grad: 0.51631, loss_in_train: 2.23329, loss_in_test: 36.40818\n",
      "iter: 230, grad: 0.51505, loss_in_train: 2.23155, loss_in_test: 36.40396\n",
      "iter: 231, grad: 0.51379, loss_in_train: 2.22985, loss_in_test: 36.39975\n",
      "iter: 232, grad: 0.51254, loss_in_train: 2.22820, loss_in_test: 36.39556\n",
      "iter: 233, grad: 0.51129, loss_in_train: 2.22658, loss_in_test: 36.39139\n",
      "iter: 234, grad: 0.51005, loss_in_train: 2.22499, loss_in_test: 36.38723\n",
      "iter: 235, grad: 0.50880, loss_in_train: 2.22344, loss_in_test: 36.38308\n",
      "iter: 236, grad: 0.50756, loss_in_train: 2.22193, loss_in_test: 36.37895\n",
      "iter: 237, grad: 0.50633, loss_in_train: 2.22045, loss_in_test: 36.37483\n",
      "iter: 238, grad: 0.50510, loss_in_train: 2.21901, loss_in_test: 36.37072\n",
      "iter: 239, grad: 0.50387, loss_in_train: 2.21759, loss_in_test: 36.36663\n",
      "iter: 240, grad: 0.50264, loss_in_train: 2.21621, loss_in_test: 36.36256\n",
      "iter: 241, grad: 0.50142, loss_in_train: 2.21486, loss_in_test: 36.35850\n",
      "iter: 242, grad: 0.50020, loss_in_train: 2.21354, loss_in_test: 36.35445\n",
      "iter: 243, grad: 0.49898, loss_in_train: 2.21225, loss_in_test: 36.35042\n",
      "iter: 244, grad: 0.49777, loss_in_train: 2.21099, loss_in_test: 36.34641\n",
      "iter: 245, grad: 0.49656, loss_in_train: 2.20975, loss_in_test: 36.34241\n",
      "iter: 246, grad: 0.49535, loss_in_train: 2.20855, loss_in_test: 36.33843\n",
      "iter: 247, grad: 0.49414, loss_in_train: 2.20737, loss_in_test: 36.33446\n",
      "iter: 248, grad: 0.49294, loss_in_train: 2.20622, loss_in_test: 36.33050\n",
      "iter: 249, grad: 0.49174, loss_in_train: 2.20509, loss_in_test: 36.32656\n",
      "iter: 250, grad: 0.49055, loss_in_train: 2.20399, loss_in_test: 36.32264\n",
      "iter: 251, grad: 0.48936, loss_in_train: 2.20291, loss_in_test: 36.31874\n",
      "iter: 252, grad: 0.48817, loss_in_train: 2.20185, loss_in_test: 36.31484\n",
      "iter: 253, grad: 0.48698, loss_in_train: 2.20082, loss_in_test: 36.31097\n",
      "iter: 254, grad: 0.48580, loss_in_train: 2.19981, loss_in_test: 36.30711\n",
      "iter: 255, grad: 0.48462, loss_in_train: 2.19883, loss_in_test: 36.30327\n",
      "iter: 256, grad: 0.48345, loss_in_train: 2.19786, loss_in_test: 36.29944\n",
      "iter: 257, grad: 0.48227, loss_in_train: 2.19692, loss_in_test: 36.29563\n",
      "iter: 258, grad: 0.48110, loss_in_train: 2.19599, loss_in_test: 36.29183\n",
      "iter: 259, grad: 0.47993, loss_in_train: 2.19509, loss_in_test: 36.28805\n",
      "iter: 260, grad: 0.47877, loss_in_train: 2.19421, loss_in_test: 36.28428\n",
      "iter: 261, grad: 0.47761, loss_in_train: 2.19334, loss_in_test: 36.28054\n",
      "iter: 262, grad: 0.47645, loss_in_train: 2.19250, loss_in_test: 36.27680\n",
      "iter: 263, grad: 0.47529, loss_in_train: 2.19167, loss_in_test: 36.27309\n",
      "iter: 264, grad: 0.47414, loss_in_train: 2.19086, loss_in_test: 36.26939\n",
      "iter: 265, grad: 0.47299, loss_in_train: 2.19007, loss_in_test: 36.26570\n",
      "iter: 266, grad: 0.47185, loss_in_train: 2.18930, loss_in_test: 36.26203\n",
      "iter: 267, grad: 0.47070, loss_in_train: 2.18854, loss_in_test: 36.25838\n",
      "iter: 268, grad: 0.46956, loss_in_train: 2.18780, loss_in_test: 36.25474\n",
      "iter: 269, grad: 0.46842, loss_in_train: 2.18708, loss_in_test: 36.25112\n",
      "iter: 270, grad: 0.46729, loss_in_train: 2.18637, loss_in_test: 36.24752\n",
      "iter: 271, grad: 0.46616, loss_in_train: 2.18567, loss_in_test: 36.24393\n",
      "iter: 272, grad: 0.46503, loss_in_train: 2.18499, loss_in_test: 36.24036\n",
      "iter: 273, grad: 0.46390, loss_in_train: 2.18433, loss_in_test: 36.23680\n",
      "iter: 274, grad: 0.46278, loss_in_train: 2.18368, loss_in_test: 36.23326\n",
      "iter: 275, grad: 0.46166, loss_in_train: 2.18304, loss_in_test: 36.22974\n",
      "iter: 276, grad: 0.46054, loss_in_train: 2.18242, loss_in_test: 36.22623\n",
      "iter: 277, grad: 0.45943, loss_in_train: 2.18181, loss_in_test: 36.22274\n",
      "iter: 278, grad: 0.45831, loss_in_train: 2.18121, loss_in_test: 36.21927\n",
      "iter: 279, grad: 0.45720, loss_in_train: 2.18063, loss_in_test: 36.21581\n",
      "iter: 280, grad: 0.45610, loss_in_train: 2.18006, loss_in_test: 36.21236\n",
      "iter: 281, grad: 0.45500, loss_in_train: 2.17950, loss_in_test: 36.20893\n",
      "iter: 282, grad: 0.45389, loss_in_train: 2.17895, loss_in_test: 36.20552\n",
      "iter: 283, grad: 0.45280, loss_in_train: 2.17842, loss_in_test: 36.20213\n",
      "iter: 284, grad: 0.45170, loss_in_train: 2.17789, loss_in_test: 36.19875\n",
      "iter: 285, grad: 0.45061, loss_in_train: 2.17738, loss_in_test: 36.19538\n",
      "iter: 286, grad: 0.44952, loss_in_train: 2.17688, loss_in_test: 36.19204\n",
      "iter: 287, grad: 0.44843, loss_in_train: 2.17639, loss_in_test: 36.18871\n",
      "iter: 288, grad: 0.44735, loss_in_train: 2.17590, loss_in_test: 36.18539\n",
      "iter: 289, grad: 0.44627, loss_in_train: 2.17543, loss_in_test: 36.18209\n",
      "iter: 290, grad: 0.44519, loss_in_train: 2.17497, loss_in_test: 36.17880\n",
      "iter: 291, grad: 0.44411, loss_in_train: 2.17452, loss_in_test: 36.17554\n",
      "iter: 292, grad: 0.44304, loss_in_train: 2.17408, loss_in_test: 36.17228\n",
      "iter: 293, grad: 0.44197, loss_in_train: 2.17365, loss_in_test: 36.16905\n",
      "iter: 294, grad: 0.44090, loss_in_train: 2.17322, loss_in_test: 36.16583\n",
      "iter: 295, grad: 0.43984, loss_in_train: 2.17281, loss_in_test: 36.16262\n",
      "iter: 296, grad: 0.43878, loss_in_train: 2.17240, loss_in_test: 36.15943\n",
      "iter: 297, grad: 0.43772, loss_in_train: 2.17200, loss_in_test: 36.15626\n",
      "iter: 298, grad: 0.43666, loss_in_train: 2.17162, loss_in_test: 36.15310\n",
      "iter: 299, grad: 0.43561, loss_in_train: 2.17123, loss_in_test: 36.14995\n",
      "iter: 300, grad: 0.43456, loss_in_train: 2.17086, loss_in_test: 36.14683\n",
      "iter: 301, grad: 0.43351, loss_in_train: 2.17050, loss_in_test: 36.14371\n",
      "iter: 302, grad: 0.43246, loss_in_train: 2.17014, loss_in_test: 36.14062\n",
      "iter: 303, grad: 0.43142, loss_in_train: 2.16979, loss_in_test: 36.13753\n",
      "iter: 304, grad: 0.43038, loss_in_train: 2.16945, loss_in_test: 36.13447\n",
      "iter: 305, grad: 0.42934, loss_in_train: 2.16911, loss_in_test: 36.13142\n",
      "iter: 306, grad: 0.42830, loss_in_train: 2.16878, loss_in_test: 36.12838\n",
      "iter: 307, grad: 0.42727, loss_in_train: 2.16846, loss_in_test: 36.12536\n",
      "iter: 308, grad: 0.42624, loss_in_train: 2.16814, loss_in_test: 36.12236\n",
      "iter: 309, grad: 0.42521, loss_in_train: 2.16783, loss_in_test: 36.11937\n",
      "iter: 310, grad: 0.42419, loss_in_train: 2.16753, loss_in_test: 36.11639\n",
      "iter: 311, grad: 0.42316, loss_in_train: 2.16724, loss_in_test: 36.11343\n",
      "iter: 312, grad: 0.42214, loss_in_train: 2.16695, loss_in_test: 36.11048\n",
      "iter: 313, grad: 0.42113, loss_in_train: 2.16666, loss_in_test: 36.10755\n",
      "iter: 314, grad: 0.42011, loss_in_train: 2.16638, loss_in_test: 36.10464\n",
      "iter: 315, grad: 0.41910, loss_in_train: 2.16611, loss_in_test: 36.10174\n",
      "iter: 316, grad: 0.41809, loss_in_train: 2.16584, loss_in_test: 36.09885\n",
      "iter: 317, grad: 0.41708, loss_in_train: 2.16558, loss_in_test: 36.09598\n",
      "iter: 318, grad: 0.41608, loss_in_train: 2.16533, loss_in_test: 36.09313\n",
      "iter: 319, grad: 0.41508, loss_in_train: 2.16508, loss_in_test: 36.09029\n",
      "iter: 320, grad: 0.41408, loss_in_train: 2.16483, loss_in_test: 36.08746\n",
      "iter: 321, grad: 0.41308, loss_in_train: 2.16459, loss_in_test: 36.08465\n",
      "iter: 322, grad: 0.41209, loss_in_train: 2.16436, loss_in_test: 36.08185\n",
      "iter: 323, grad: 0.41109, loss_in_train: 2.16412, loss_in_test: 36.07907\n",
      "iter: 324, grad: 0.41010, loss_in_train: 2.16390, loss_in_test: 36.07630\n",
      "iter: 325, grad: 0.40912, loss_in_train: 2.16368, loss_in_test: 36.07354\n",
      "iter: 326, grad: 0.40813, loss_in_train: 2.16346, loss_in_test: 36.07080\n",
      "iter: 327, grad: 0.40715, loss_in_train: 2.16325, loss_in_test: 36.06808\n",
      "iter: 328, grad: 0.40617, loss_in_train: 2.16304, loss_in_test: 36.06536\n",
      "iter: 329, grad: 0.40519, loss_in_train: 2.16284, loss_in_test: 36.06267\n",
      "iter: 330, grad: 0.40422, loss_in_train: 2.16264, loss_in_test: 36.05998\n",
      "iter: 331, grad: 0.40325, loss_in_train: 2.16244, loss_in_test: 36.05731\n",
      "iter: 332, grad: 0.40228, loss_in_train: 2.16225, loss_in_test: 36.05466\n",
      "iter: 333, grad: 0.40131, loss_in_train: 2.16206, loss_in_test: 36.05202\n",
      "iter: 334, grad: 0.40035, loss_in_train: 2.16188, loss_in_test: 36.04939\n",
      "iter: 335, grad: 0.39938, loss_in_train: 2.16170, loss_in_test: 36.04678\n",
      "iter: 336, grad: 0.39842, loss_in_train: 2.16152, loss_in_test: 36.04418\n",
      "iter: 337, grad: 0.39747, loss_in_train: 2.16135, loss_in_test: 36.04159\n",
      "iter: 338, grad: 0.39651, loss_in_train: 2.16118, loss_in_test: 36.03902\n",
      "iter: 339, grad: 0.39556, loss_in_train: 2.16101, loss_in_test: 36.03646\n",
      "iter: 340, grad: 0.39461, loss_in_train: 2.16085, loss_in_test: 36.03392\n",
      "iter: 341, grad: 0.39366, loss_in_train: 2.16069, loss_in_test: 36.03139\n",
      "iter: 342, grad: 0.39271, loss_in_train: 2.16053, loss_in_test: 36.02887\n",
      "iter: 343, grad: 0.39177, loss_in_train: 2.16038, loss_in_test: 36.02636\n",
      "iter: 344, grad: 0.39083, loss_in_train: 2.16023, loss_in_test: 36.02387\n",
      "iter: 345, grad: 0.38989, loss_in_train: 2.16008, loss_in_test: 36.02139\n",
      "iter: 346, grad: 0.38896, loss_in_train: 2.15994, loss_in_test: 36.01893\n",
      "iter: 347, grad: 0.38802, loss_in_train: 2.15980, loss_in_test: 36.01648\n",
      "iter: 348, grad: 0.38709, loss_in_train: 2.15966, loss_in_test: 36.01404\n",
      "iter: 349, grad: 0.38616, loss_in_train: 2.15953, loss_in_test: 36.01162\n",
      "iter: 350, grad: 0.38523, loss_in_train: 2.15939, loss_in_test: 36.00920\n",
      "iter: 351, grad: 0.38431, loss_in_train: 2.15926, loss_in_test: 36.00680\n",
      "iter: 352, grad: 0.38339, loss_in_train: 2.15914, loss_in_test: 36.00442\n",
      "iter: 353, grad: 0.38247, loss_in_train: 2.15901, loss_in_test: 36.00205\n",
      "iter: 354, grad: 0.38155, loss_in_train: 2.15889, loss_in_test: 35.99969\n",
      "iter: 355, grad: 0.38064, loss_in_train: 2.15877, loss_in_test: 35.99734\n",
      "iter: 356, grad: 0.37972, loss_in_train: 2.15865, loss_in_test: 35.99500\n",
      "iter: 357, grad: 0.37881, loss_in_train: 2.15854, loss_in_test: 35.99268\n",
      "iter: 358, grad: 0.37790, loss_in_train: 2.15842, loss_in_test: 35.99037\n",
      "iter: 359, grad: 0.37700, loss_in_train: 2.15831, loss_in_test: 35.98807\n",
      "iter: 360, grad: 0.37609, loss_in_train: 2.15821, loss_in_test: 35.98579\n",
      "iter: 361, grad: 0.37519, loss_in_train: 2.15810, loss_in_test: 35.98352\n",
      "iter: 362, grad: 0.37429, loss_in_train: 2.15800, loss_in_test: 35.98126\n",
      "iter: 363, grad: 0.37340, loss_in_train: 2.15789, loss_in_test: 35.97901\n",
      "iter: 364, grad: 0.37250, loss_in_train: 2.15779, loss_in_test: 35.97678\n",
      "iter: 365, grad: 0.37161, loss_in_train: 2.15770, loss_in_test: 35.97455\n",
      "iter: 366, grad: 0.37072, loss_in_train: 2.15760, loss_in_test: 35.97234\n",
      "iter: 367, grad: 0.36983, loss_in_train: 2.15751, loss_in_test: 35.97015\n",
      "iter: 368, grad: 0.36895, loss_in_train: 2.15741, loss_in_test: 35.96796\n",
      "iter: 369, grad: 0.36806, loss_in_train: 2.15732, loss_in_test: 35.96579\n",
      "iter: 370, grad: 0.36718, loss_in_train: 2.15723, loss_in_test: 35.96362\n",
      "iter: 371, grad: 0.36630, loss_in_train: 2.15715, loss_in_test: 35.96147\n",
      "iter: 372, grad: 0.36543, loss_in_train: 2.15706, loss_in_test: 35.95933\n",
      "iter: 373, grad: 0.36455, loss_in_train: 2.15698, loss_in_test: 35.95721\n",
      "iter: 374, grad: 0.36368, loss_in_train: 2.15690, loss_in_test: 35.95509\n",
      "iter: 375, grad: 0.36281, loss_in_train: 2.15682, loss_in_test: 35.95299\n",
      "iter: 376, grad: 0.36194, loss_in_train: 2.15674, loss_in_test: 35.95090\n",
      "iter: 377, grad: 0.36107, loss_in_train: 2.15666, loss_in_test: 35.94882\n",
      "iter: 378, grad: 0.36021, loss_in_train: 2.15659, loss_in_test: 35.94675\n",
      "iter: 379, grad: 0.35935, loss_in_train: 2.15651, loss_in_test: 35.94469\n",
      "iter: 380, grad: 0.35849, loss_in_train: 2.15644, loss_in_test: 35.94265\n",
      "iter: 381, grad: 0.35763, loss_in_train: 2.15637, loss_in_test: 35.94061\n",
      "iter: 382, grad: 0.35678, loss_in_train: 2.15630, loss_in_test: 35.93859\n",
      "iter: 383, grad: 0.35592, loss_in_train: 2.15623, loss_in_test: 35.93658\n",
      "iter: 384, grad: 0.35507, loss_in_train: 2.15617, loss_in_test: 35.93458\n",
      "iter: 385, grad: 0.35422, loss_in_train: 2.15610, loss_in_test: 35.93259\n",
      "iter: 386, grad: 0.35338, loss_in_train: 2.15604, loss_in_test: 35.93061\n",
      "iter: 387, grad: 0.35253, loss_in_train: 2.15597, loss_in_test: 35.92865\n",
      "iter: 388, grad: 0.35169, loss_in_train: 2.15591, loss_in_test: 35.92669\n",
      "iter: 389, grad: 0.35085, loss_in_train: 2.15585, loss_in_test: 35.92474\n",
      "iter: 390, grad: 0.35001, loss_in_train: 2.15579, loss_in_test: 35.92281\n",
      "iter: 391, grad: 0.34918, loss_in_train: 2.15573, loss_in_test: 35.92089\n",
      "iter: 392, grad: 0.34834, loss_in_train: 2.15568, loss_in_test: 35.91898\n",
      "iter: 393, grad: 0.34751, loss_in_train: 2.15562, loss_in_test: 35.91707\n",
      "iter: 394, grad: 0.34668, loss_in_train: 2.15557, loss_in_test: 35.91518\n",
      "iter: 395, grad: 0.34585, loss_in_train: 2.15551, loss_in_test: 35.91330\n",
      "iter: 396, grad: 0.34503, loss_in_train: 2.15546, loss_in_test: 35.91143\n",
      "iter: 397, grad: 0.34420, loss_in_train: 2.15541, loss_in_test: 35.90957\n",
      "iter: 398, grad: 0.34338, loss_in_train: 2.15536, loss_in_test: 35.90773\n",
      "iter: 399, grad: 0.34256, loss_in_train: 2.15531, loss_in_test: 35.90589\n",
      "iter: 400, grad: 0.34175, loss_in_train: 2.15526, loss_in_test: 35.90406\n",
      "iter: 401, grad: 0.34093, loss_in_train: 2.15521, loss_in_test: 35.90224\n",
      "iter: 402, grad: 0.34012, loss_in_train: 2.15516, loss_in_test: 35.90044\n",
      "iter: 403, grad: 0.33931, loss_in_train: 2.15512, loss_in_test: 35.89864\n",
      "iter: 404, grad: 0.33850, loss_in_train: 2.15507, loss_in_test: 35.89685\n",
      "iter: 405, grad: 0.33769, loss_in_train: 2.15503, loss_in_test: 35.89508\n",
      "iter: 406, grad: 0.33688, loss_in_train: 2.15499, loss_in_test: 35.89331\n",
      "iter: 407, grad: 0.33608, loss_in_train: 2.15494, loss_in_test: 35.89155\n",
      "iter: 408, grad: 0.33528, loss_in_train: 2.15490, loss_in_test: 35.88981\n",
      "iter: 409, grad: 0.33448, loss_in_train: 2.15486, loss_in_test: 35.88807\n",
      "iter: 410, grad: 0.33368, loss_in_train: 2.15482, loss_in_test: 35.88634\n",
      "iter: 411, grad: 0.33289, loss_in_train: 2.15478, loss_in_test: 35.88463\n",
      "iter: 412, grad: 0.33210, loss_in_train: 2.15474, loss_in_test: 35.88292\n",
      "iter: 413, grad: 0.33130, loss_in_train: 2.15471, loss_in_test: 35.88122\n",
      "iter: 414, grad: 0.33052, loss_in_train: 2.15467, loss_in_test: 35.87954\n",
      "iter: 415, grad: 0.32973, loss_in_train: 2.15463, loss_in_test: 35.87786\n",
      "iter: 416, grad: 0.32894, loss_in_train: 2.15460, loss_in_test: 35.87619\n",
      "iter: 417, grad: 0.32816, loss_in_train: 2.15456, loss_in_test: 35.87453\n",
      "iter: 418, grad: 0.32738, loss_in_train: 2.15453, loss_in_test: 35.87288\n",
      "iter: 419, grad: 0.32660, loss_in_train: 2.15450, loss_in_test: 35.87124\n",
      "iter: 420, grad: 0.32582, loss_in_train: 2.15446, loss_in_test: 35.86961\n",
      "iter: 421, grad: 0.32505, loss_in_train: 2.15443, loss_in_test: 35.86799\n",
      "iter: 422, grad: 0.32427, loss_in_train: 2.15440, loss_in_test: 35.86638\n",
      "iter: 423, grad: 0.32350, loss_in_train: 2.15437, loss_in_test: 35.86478\n",
      "iter: 424, grad: 0.32273, loss_in_train: 2.15434, loss_in_test: 35.86319\n",
      "iter: 425, grad: 0.32196, loss_in_train: 2.15431, loss_in_test: 35.86160\n",
      "iter: 426, grad: 0.32120, loss_in_train: 2.15428, loss_in_test: 35.86003\n",
      "iter: 427, grad: 0.32043, loss_in_train: 2.15425, loss_in_test: 35.85846\n",
      "iter: 428, grad: 0.31967, loss_in_train: 2.15422, loss_in_test: 35.85691\n",
      "iter: 429, grad: 0.31891, loss_in_train: 2.15420, loss_in_test: 35.85536\n",
      "iter: 430, grad: 0.31815, loss_in_train: 2.15417, loss_in_test: 35.85382\n",
      "iter: 431, grad: 0.31740, loss_in_train: 2.15414, loss_in_test: 35.85229\n",
      "iter: 432, grad: 0.31664, loss_in_train: 2.15412, loss_in_test: 35.85077\n",
      "iter: 433, grad: 0.31589, loss_in_train: 2.15409, loss_in_test: 35.84926\n",
      "iter: 434, grad: 0.31514, loss_in_train: 2.15407, loss_in_test: 35.84776\n",
      "iter: 435, grad: 0.31439, loss_in_train: 2.15404, loss_in_test: 35.84626\n",
      "iter: 436, grad: 0.31365, loss_in_train: 2.15402, loss_in_test: 35.84478\n",
      "iter: 437, grad: 0.31290, loss_in_train: 2.15400, loss_in_test: 35.84330\n",
      "iter: 438, grad: 0.31216, loss_in_train: 2.15397, loss_in_test: 35.84183\n",
      "iter: 439, grad: 0.31142, loss_in_train: 2.15395, loss_in_test: 35.84037\n",
      "iter: 440, grad: 0.31068, loss_in_train: 2.15393, loss_in_test: 35.83892\n",
      "iter: 441, grad: 0.30994, loss_in_train: 2.15391, loss_in_test: 35.83748\n",
      "iter: 442, grad: 0.30920, loss_in_train: 2.15389, loss_in_test: 35.83605\n",
      "iter: 443, grad: 0.30847, loss_in_train: 2.15386, loss_in_test: 35.83462\n",
      "iter: 444, grad: 0.30774, loss_in_train: 2.15384, loss_in_test: 35.83320\n",
      "iter: 445, grad: 0.30701, loss_in_train: 2.15382, loss_in_test: 35.83179\n",
      "iter: 446, grad: 0.30628, loss_in_train: 2.15380, loss_in_test: 35.83039\n",
      "iter: 447, grad: 0.30555, loss_in_train: 2.15379, loss_in_test: 35.82900\n",
      "iter: 448, grad: 0.30483, loss_in_train: 2.15377, loss_in_test: 35.82761\n",
      "iter: 449, grad: 0.30410, loss_in_train: 2.15375, loss_in_test: 35.82624\n",
      "iter: 450, grad: 0.30338, loss_in_train: 2.15373, loss_in_test: 35.82487\n",
      "iter: 451, grad: 0.30266, loss_in_train: 2.15371, loss_in_test: 35.82351\n",
      "iter: 452, grad: 0.30195, loss_in_train: 2.15369, loss_in_test: 35.82216\n",
      "iter: 453, grad: 0.30123, loss_in_train: 2.15368, loss_in_test: 35.82081\n",
      "iter: 454, grad: 0.30052, loss_in_train: 2.15366, loss_in_test: 35.81948\n",
      "iter: 455, grad: 0.29981, loss_in_train: 2.15364, loss_in_test: 35.81815\n",
      "iter: 456, grad: 0.29909, loss_in_train: 2.15363, loss_in_test: 35.81683\n",
      "iter: 457, grad: 0.29839, loss_in_train: 2.15361, loss_in_test: 35.81551\n",
      "iter: 458, grad: 0.29768, loss_in_train: 2.15360, loss_in_test: 35.81421\n",
      "iter: 459, grad: 0.29697, loss_in_train: 2.15358, loss_in_test: 35.81291\n",
      "iter: 460, grad: 0.29627, loss_in_train: 2.15357, loss_in_test: 35.81162\n",
      "iter: 461, grad: 0.29557, loss_in_train: 2.15355, loss_in_test: 35.81034\n",
      "iter: 462, grad: 0.29487, loss_in_train: 2.15354, loss_in_test: 35.80906\n",
      "iter: 463, grad: 0.29417, loss_in_train: 2.15352, loss_in_test: 35.80780\n",
      "iter: 464, grad: 0.29348, loss_in_train: 2.15351, loss_in_test: 35.80654\n",
      "iter: 465, grad: 0.29278, loss_in_train: 2.15350, loss_in_test: 35.80529\n",
      "iter: 466, grad: 0.29209, loss_in_train: 2.15348, loss_in_test: 35.80404\n",
      "iter: 467, grad: 0.29140, loss_in_train: 2.15347, loss_in_test: 35.80281\n",
      "iter: 468, grad: 0.29071, loss_in_train: 2.15346, loss_in_test: 35.80158\n",
      "iter: 469, grad: 0.29002, loss_in_train: 2.15344, loss_in_test: 35.80035\n",
      "iter: 470, grad: 0.28933, loss_in_train: 2.15343, loss_in_test: 35.79914\n",
      "iter: 471, grad: 0.28865, loss_in_train: 2.15342, loss_in_test: 35.79793\n",
      "iter: 472, grad: 0.28797, loss_in_train: 2.15341, loss_in_test: 35.79673\n",
      "iter: 473, grad: 0.28729, loss_in_train: 2.15340, loss_in_test: 35.79554\n",
      "iter: 474, grad: 0.28661, loss_in_train: 2.15339, loss_in_test: 35.79435\n",
      "iter: 475, grad: 0.28593, loss_in_train: 2.15337, loss_in_test: 35.79317\n",
      "iter: 476, grad: 0.28526, loss_in_train: 2.15336, loss_in_test: 35.79200\n",
      "iter: 477, grad: 0.28458, loss_in_train: 2.15335, loss_in_test: 35.79083\n",
      "iter: 478, grad: 0.28391, loss_in_train: 2.15334, loss_in_test: 35.78968\n",
      "iter: 479, grad: 0.28324, loss_in_train: 2.15333, loss_in_test: 35.78852\n",
      "iter: 480, grad: 0.28257, loss_in_train: 2.15332, loss_in_test: 35.78738\n",
      "iter: 481, grad: 0.28190, loss_in_train: 2.15331, loss_in_test: 35.78624\n",
      "iter: 482, grad: 0.28124, loss_in_train: 2.15330, loss_in_test: 35.78511\n",
      "iter: 483, grad: 0.28057, loss_in_train: 2.15329, loss_in_test: 35.78399\n",
      "iter: 484, grad: 0.27991, loss_in_train: 2.15328, loss_in_test: 35.78287\n",
      "iter: 485, grad: 0.27925, loss_in_train: 2.15327, loss_in_test: 35.78176\n",
      "iter: 486, grad: 0.27859, loss_in_train: 2.15326, loss_in_test: 35.78066\n",
      "iter: 487, grad: 0.27793, loss_in_train: 2.15326, loss_in_test: 35.77956\n",
      "iter: 488, grad: 0.27728, loss_in_train: 2.15325, loss_in_test: 35.77847\n",
      "iter: 489, grad: 0.27662, loss_in_train: 2.15324, loss_in_test: 35.77739\n",
      "iter: 490, grad: 0.27597, loss_in_train: 2.15323, loss_in_test: 35.77631\n",
      "iter: 491, grad: 0.27532, loss_in_train: 2.15322, loss_in_test: 35.77524\n",
      "iter: 492, grad: 0.27467, loss_in_train: 2.15321, loss_in_test: 35.77417\n",
      "iter: 493, grad: 0.27403, loss_in_train: 2.15321, loss_in_test: 35.77312\n",
      "iter: 494, grad: 0.27338, loss_in_train: 2.15320, loss_in_test: 35.77207\n",
      "iter: 495, grad: 0.27274, loss_in_train: 2.15319, loss_in_test: 35.77102\n",
      "iter: 496, grad: 0.27209, loss_in_train: 2.15318, loss_in_test: 35.76998\n",
      "iter: 497, grad: 0.27145, loss_in_train: 2.15318, loss_in_test: 35.76895\n",
      "iter: 498, grad: 0.27081, loss_in_train: 2.15317, loss_in_test: 35.76792\n",
      "iter: 499, grad: 0.27017, loss_in_train: 2.15316, loss_in_test: 35.76690\n",
      "iter: 500, grad: 0.26954, loss_in_train: 2.15316, loss_in_test: 35.76589\n",
      "iter: 501, grad: 0.26890, loss_in_train: 2.15315, loss_in_test: 35.76488\n",
      "iter: 502, grad: 0.26827, loss_in_train: 2.15314, loss_in_test: 35.76388\n",
      "iter: 503, grad: 0.26764, loss_in_train: 2.15314, loss_in_test: 35.76289\n",
      "iter: 504, grad: 0.26701, loss_in_train: 2.15313, loss_in_test: 35.76190\n",
      "iter: 505, grad: 0.26638, loss_in_train: 2.15312, loss_in_test: 35.76092\n",
      "iter: 506, grad: 0.26575, loss_in_train: 2.15312, loss_in_test: 35.75994\n",
      "iter: 507, grad: 0.26513, loss_in_train: 2.15311, loss_in_test: 35.75897\n",
      "iter: 508, grad: 0.26451, loss_in_train: 2.15310, loss_in_test: 35.75800\n",
      "iter: 509, grad: 0.26388, loss_in_train: 2.15310, loss_in_test: 35.75705\n",
      "iter: 510, grad: 0.26326, loss_in_train: 2.15309, loss_in_test: 35.75609\n",
      "iter: 511, grad: 0.26264, loss_in_train: 2.15309, loss_in_test: 35.75515\n",
      "iter: 512, grad: 0.26203, loss_in_train: 2.15308, loss_in_test: 35.75420\n",
      "iter: 513, grad: 0.26141, loss_in_train: 2.15308, loss_in_test: 35.75327\n",
      "iter: 514, grad: 0.26080, loss_in_train: 2.15307, loss_in_test: 35.75234\n",
      "iter: 515, grad: 0.26018, loss_in_train: 2.15307, loss_in_test: 35.75141\n",
      "iter: 516, grad: 0.25957, loss_in_train: 2.15306, loss_in_test: 35.75050\n",
      "iter: 517, grad: 0.25896, loss_in_train: 2.15306, loss_in_test: 35.74958\n",
      "iter: 518, grad: 0.25835, loss_in_train: 2.15305, loss_in_test: 35.74868\n",
      "iter: 519, grad: 0.25775, loss_in_train: 2.15305, loss_in_test: 35.74778\n",
      "iter: 520, grad: 0.25714, loss_in_train: 2.15304, loss_in_test: 35.74688\n",
      "iter: 521, grad: 0.25654, loss_in_train: 2.15304, loss_in_test: 35.74599\n",
      "iter: 522, grad: 0.25594, loss_in_train: 2.15303, loss_in_test: 35.74510\n",
      "iter: 523, grad: 0.25534, loss_in_train: 2.15303, loss_in_test: 35.74423\n",
      "iter: 524, grad: 0.25474, loss_in_train: 2.15302, loss_in_test: 35.74335\n",
      "iter: 525, grad: 0.25414, loss_in_train: 2.15302, loss_in_test: 35.74248\n",
      "iter: 526, grad: 0.25354, loss_in_train: 2.15302, loss_in_test: 35.74162\n",
      "iter: 527, grad: 0.25295, loss_in_train: 2.15301, loss_in_test: 35.74076\n",
      "iter: 528, grad: 0.25236, loss_in_train: 2.15301, loss_in_test: 35.73991\n",
      "iter: 529, grad: 0.25176, loss_in_train: 2.15300, loss_in_test: 35.73906\n",
      "iter: 530, grad: 0.25117, loss_in_train: 2.15300, loss_in_test: 35.73822\n",
      "iter: 531, grad: 0.25059, loss_in_train: 2.15300, loss_in_test: 35.73739\n",
      "iter: 532, grad: 0.25000, loss_in_train: 2.15299, loss_in_test: 35.73655\n",
      "iter: 533, grad: 0.24941, loss_in_train: 2.15299, loss_in_test: 35.73573\n",
      "iter: 534, grad: 0.24883, loss_in_train: 2.15299, loss_in_test: 35.73491\n",
      "iter: 535, grad: 0.24825, loss_in_train: 2.15298, loss_in_test: 35.73409\n",
      "iter: 536, grad: 0.24766, loss_in_train: 2.15298, loss_in_test: 35.73328\n",
      "iter: 537, grad: 0.24708, loss_in_train: 2.15297, loss_in_test: 35.73248\n",
      "iter: 538, grad: 0.24651, loss_in_train: 2.15297, loss_in_test: 35.73168\n",
      "iter: 539, grad: 0.24593, loss_in_train: 2.15297, loss_in_test: 35.73088\n",
      "iter: 540, grad: 0.24535, loss_in_train: 2.15297, loss_in_test: 35.73009\n",
      "iter: 541, grad: 0.24478, loss_in_train: 2.15296, loss_in_test: 35.72931\n",
      "iter: 542, grad: 0.24421, loss_in_train: 2.15296, loss_in_test: 35.72853\n",
      "iter: 543, grad: 0.24363, loss_in_train: 2.15296, loss_in_test: 35.72775\n",
      "iter: 544, grad: 0.24306, loss_in_train: 2.15295, loss_in_test: 35.72698\n",
      "iter: 545, grad: 0.24250, loss_in_train: 2.15295, loss_in_test: 35.72621\n",
      "iter: 546, grad: 0.24193, loss_in_train: 2.15295, loss_in_test: 35.72545\n",
      "iter: 547, grad: 0.24136, loss_in_train: 2.15294, loss_in_test: 35.72470\n",
      "iter: 548, grad: 0.24080, loss_in_train: 2.15294, loss_in_test: 35.72395\n",
      "iter: 549, grad: 0.24024, loss_in_train: 2.15294, loss_in_test: 35.72320\n",
      "iter: 550, grad: 0.23968, loss_in_train: 2.15294, loss_in_test: 35.72246\n",
      "iter: 551, grad: 0.23912, loss_in_train: 2.15293, loss_in_test: 35.72172\n",
      "iter: 552, grad: 0.23856, loss_in_train: 2.15293, loss_in_test: 35.72099\n",
      "iter: 553, grad: 0.23800, loss_in_train: 2.15293, loss_in_test: 35.72026\n",
      "iter: 554, grad: 0.23744, loss_in_train: 2.15293, loss_in_test: 35.71954\n",
      "iter: 555, grad: 0.23689, loss_in_train: 2.15292, loss_in_test: 35.71882\n",
      "iter: 556, grad: 0.23634, loss_in_train: 2.15292, loss_in_test: 35.71811\n",
      "iter: 557, grad: 0.23579, loss_in_train: 2.15292, loss_in_test: 35.71740\n",
      "iter: 558, grad: 0.23523, loss_in_train: 2.15292, loss_in_test: 35.71669\n",
      "iter: 559, grad: 0.23469, loss_in_train: 2.15291, loss_in_test: 35.71599\n",
      "iter: 560, grad: 0.23414, loss_in_train: 2.15291, loss_in_test: 35.71530\n",
      "iter: 561, grad: 0.23359, loss_in_train: 2.15291, loss_in_test: 35.71461\n",
      "iter: 562, grad: 0.23305, loss_in_train: 2.15291, loss_in_test: 35.71392\n",
      "iter: 563, grad: 0.23250, loss_in_train: 2.15291, loss_in_test: 35.71324\n",
      "iter: 564, grad: 0.23196, loss_in_train: 2.15290, loss_in_test: 35.71256\n",
      "iter: 565, grad: 0.23142, loss_in_train: 2.15290, loss_in_test: 35.71189\n",
      "iter: 566, grad: 0.23088, loss_in_train: 2.15290, loss_in_test: 35.71122\n",
      "iter: 567, grad: 0.23034, loss_in_train: 2.15290, loss_in_test: 35.71055\n",
      "iter: 568, grad: 0.22981, loss_in_train: 2.15290, loss_in_test: 35.70989\n",
      "iter: 569, grad: 0.22927, loss_in_train: 2.15290, loss_in_test: 35.70923\n",
      "iter: 570, grad: 0.22874, loss_in_train: 2.15289, loss_in_test: 35.70858\n",
      "iter: 571, grad: 0.22821, loss_in_train: 2.15289, loss_in_test: 35.70793\n",
      "iter: 572, grad: 0.22767, loss_in_train: 2.15289, loss_in_test: 35.70729\n",
      "iter: 573, grad: 0.22714, loss_in_train: 2.15289, loss_in_test: 35.70665\n",
      "iter: 574, grad: 0.22662, loss_in_train: 2.15289, loss_in_test: 35.70602\n",
      "iter: 575, grad: 0.22609, loss_in_train: 2.15288, loss_in_test: 35.70539\n",
      "iter: 576, grad: 0.22556, loss_in_train: 2.15288, loss_in_test: 35.70476\n",
      "iter: 577, grad: 0.22504, loss_in_train: 2.15288, loss_in_test: 35.70414\n",
      "iter: 578, grad: 0.22451, loss_in_train: 2.15288, loss_in_test: 35.70352\n",
      "iter: 579, grad: 0.22399, loss_in_train: 2.15288, loss_in_test: 35.70290\n",
      "iter: 580, grad: 0.22347, loss_in_train: 2.15288, loss_in_test: 35.70229\n",
      "iter: 581, grad: 0.22295, loss_in_train: 2.15288, loss_in_test: 35.70168\n",
      "iter: 582, grad: 0.22243, loss_in_train: 2.15287, loss_in_test: 35.70108\n",
      "iter: 583, grad: 0.22192, loss_in_train: 2.15287, loss_in_test: 35.70048\n",
      "iter: 584, grad: 0.22140, loss_in_train: 2.15287, loss_in_test: 35.69989\n",
      "iter: 585, grad: 0.22089, loss_in_train: 2.15287, loss_in_test: 35.69930\n",
      "iter: 586, grad: 0.22037, loss_in_train: 2.15287, loss_in_test: 35.69871\n",
      "iter: 587, grad: 0.21986, loss_in_train: 2.15287, loss_in_test: 35.69813\n",
      "iter: 588, grad: 0.21935, loss_in_train: 2.15287, loss_in_test: 35.69755\n",
      "iter: 589, grad: 0.21884, loss_in_train: 2.15286, loss_in_test: 35.69697\n",
      "iter: 590, grad: 0.21834, loss_in_train: 2.15286, loss_in_test: 35.69640\n",
      "iter: 591, grad: 0.21783, loss_in_train: 2.15286, loss_in_test: 35.69583\n",
      "iter: 592, grad: 0.21732, loss_in_train: 2.15286, loss_in_test: 35.69527\n",
      "iter: 593, grad: 0.21682, loss_in_train: 2.15286, loss_in_test: 35.69471\n",
      "iter: 594, grad: 0.21632, loss_in_train: 2.15286, loss_in_test: 35.69415\n",
      "iter: 595, grad: 0.21582, loss_in_train: 2.15286, loss_in_test: 35.69360\n",
      "iter: 596, grad: 0.21532, loss_in_train: 2.15286, loss_in_test: 35.69305\n",
      "iter: 597, grad: 0.21482, loss_in_train: 2.15286, loss_in_test: 35.69250\n",
      "iter: 598, grad: 0.21432, loss_in_train: 2.15285, loss_in_test: 35.69196\n",
      "iter: 599, grad: 0.21382, loss_in_train: 2.15285, loss_in_test: 35.69142\n",
      "iter: 600, grad: 0.21333, loss_in_train: 2.15285, loss_in_test: 35.69089\n",
      "iter: 601, grad: 0.21283, loss_in_train: 2.15285, loss_in_test: 35.69035\n",
      "iter: 602, grad: 0.21234, loss_in_train: 2.15285, loss_in_test: 35.68983\n",
      "iter: 603, grad: 0.21185, loss_in_train: 2.15285, loss_in_test: 35.68930\n",
      "iter: 604, grad: 0.21136, loss_in_train: 2.15285, loss_in_test: 35.68878\n",
      "iter: 605, grad: 0.21087, loss_in_train: 2.15285, loss_in_test: 35.68826\n",
      "iter: 606, grad: 0.21038, loss_in_train: 2.15285, loss_in_test: 35.68775\n",
      "iter: 607, grad: 0.20989, loss_in_train: 2.15285, loss_in_test: 35.68724\n",
      "iter: 608, grad: 0.20941, loss_in_train: 2.15285, loss_in_test: 35.68673\n",
      "iter: 609, grad: 0.20892, loss_in_train: 2.15284, loss_in_test: 35.68623\n",
      "iter: 610, grad: 0.20844, loss_in_train: 2.15284, loss_in_test: 35.68573\n",
      "iter: 611, grad: 0.20796, loss_in_train: 2.15284, loss_in_test: 35.68523\n",
      "iter: 612, grad: 0.20748, loss_in_train: 2.15284, loss_in_test: 35.68474\n",
      "iter: 613, grad: 0.20700, loss_in_train: 2.15284, loss_in_test: 35.68425\n",
      "iter: 614, grad: 0.20652, loss_in_train: 2.15284, loss_in_test: 35.68376\n",
      "iter: 615, grad: 0.20604, loss_in_train: 2.15284, loss_in_test: 35.68328\n",
      "iter: 616, grad: 0.20557, loss_in_train: 2.15284, loss_in_test: 35.68280\n",
      "iter: 617, grad: 0.20509, loss_in_train: 2.15284, loss_in_test: 35.68232\n",
      "iter: 618, grad: 0.20462, loss_in_train: 2.15284, loss_in_test: 35.68185\n",
      "iter: 619, grad: 0.20415, loss_in_train: 2.15284, loss_in_test: 35.68138\n",
      "iter: 620, grad: 0.20368, loss_in_train: 2.15284, loss_in_test: 35.68091\n",
      "iter: 621, grad: 0.20321, loss_in_train: 2.15284, loss_in_test: 35.68045\n",
      "iter: 622, grad: 0.20274, loss_in_train: 2.15283, loss_in_test: 35.67999\n",
      "iter: 623, grad: 0.20227, loss_in_train: 2.15283, loss_in_test: 35.67953\n",
      "iter: 624, grad: 0.20180, loss_in_train: 2.15283, loss_in_test: 35.67907\n",
      "iter: 625, grad: 0.20134, loss_in_train: 2.15283, loss_in_test: 35.67862\n",
      "iter: 626, grad: 0.20087, loss_in_train: 2.15283, loss_in_test: 35.67817\n",
      "iter: 627, grad: 0.20041, loss_in_train: 2.15283, loss_in_test: 35.67773\n",
      "iter: 628, grad: 0.19995, loss_in_train: 2.15283, loss_in_test: 35.67729\n",
      "iter: 629, grad: 0.19949, loss_in_train: 2.15283, loss_in_test: 35.67685\n",
      "iter: 630, grad: 0.19903, loss_in_train: 2.15283, loss_in_test: 35.67641\n",
      "iter: 631, grad: 0.19857, loss_in_train: 2.15283, loss_in_test: 35.67598\n",
      "iter: 632, grad: 0.19811, loss_in_train: 2.15283, loss_in_test: 35.67555\n",
      "iter: 633, grad: 0.19766, loss_in_train: 2.15283, loss_in_test: 35.67512\n",
      "iter: 634, grad: 0.19720, loss_in_train: 2.15283, loss_in_test: 35.67470\n",
      "iter: 635, grad: 0.19675, loss_in_train: 2.15283, loss_in_test: 35.67428\n",
      "iter: 636, grad: 0.19630, loss_in_train: 2.15283, loss_in_test: 35.67386\n",
      "iter: 637, grad: 0.19585, loss_in_train: 2.15283, loss_in_test: 35.67344\n",
      "iter: 638, grad: 0.19539, loss_in_train: 2.15283, loss_in_test: 35.67303\n",
      "iter: 639, grad: 0.19495, loss_in_train: 2.15283, loss_in_test: 35.67262\n",
      "iter: 640, grad: 0.19450, loss_in_train: 2.15282, loss_in_test: 35.67222\n",
      "iter: 641, grad: 0.19405, loss_in_train: 2.15282, loss_in_test: 35.67181\n",
      "iter: 642, grad: 0.19360, loss_in_train: 2.15282, loss_in_test: 35.67141\n",
      "iter: 643, grad: 0.19316, loss_in_train: 2.15282, loss_in_test: 35.67101\n",
      "iter: 644, grad: 0.19272, loss_in_train: 2.15282, loss_in_test: 35.67062\n",
      "iter: 645, grad: 0.19227, loss_in_train: 2.15282, loss_in_test: 35.67023\n",
      "iter: 646, grad: 0.19183, loss_in_train: 2.15282, loss_in_test: 35.66984\n",
      "iter: 647, grad: 0.19139, loss_in_train: 2.15282, loss_in_test: 35.66945\n",
      "iter: 648, grad: 0.19095, loss_in_train: 2.15282, loss_in_test: 35.66906\n",
      "iter: 649, grad: 0.19051, loss_in_train: 2.15282, loss_in_test: 35.66868\n",
      "iter: 650, grad: 0.19008, loss_in_train: 2.15282, loss_in_test: 35.66830\n",
      "iter: 651, grad: 0.18964, loss_in_train: 2.15282, loss_in_test: 35.66793\n",
      "iter: 652, grad: 0.18921, loss_in_train: 2.15282, loss_in_test: 35.66755\n",
      "iter: 653, grad: 0.18877, loss_in_train: 2.15282, loss_in_test: 35.66718\n",
      "iter: 654, grad: 0.18834, loss_in_train: 2.15282, loss_in_test: 35.66682\n",
      "iter: 655, grad: 0.18791, loss_in_train: 2.15282, loss_in_test: 35.66645\n",
      "iter: 656, grad: 0.18748, loss_in_train: 2.15282, loss_in_test: 35.66609\n",
      "iter: 657, grad: 0.18705, loss_in_train: 2.15282, loss_in_test: 35.66573\n",
      "iter: 658, grad: 0.18662, loss_in_train: 2.15282, loss_in_test: 35.66537\n",
      "iter: 659, grad: 0.18619, loss_in_train: 2.15282, loss_in_test: 35.66501\n",
      "iter: 660, grad: 0.18577, loss_in_train: 2.15282, loss_in_test: 35.66466\n",
      "iter: 661, grad: 0.18534, loss_in_train: 2.15282, loss_in_test: 35.66431\n",
      "iter: 662, grad: 0.18492, loss_in_train: 2.15282, loss_in_test: 35.66397\n",
      "iter: 663, grad: 0.18449, loss_in_train: 2.15282, loss_in_test: 35.66362\n",
      "iter: 664, grad: 0.18407, loss_in_train: 2.15282, loss_in_test: 35.66328\n",
      "iter: 665, grad: 0.18365, loss_in_train: 2.15282, loss_in_test: 35.66294\n",
      "iter: 666, grad: 0.18323, loss_in_train: 2.15281, loss_in_test: 35.66260\n",
      "iter: 667, grad: 0.18281, loss_in_train: 2.15281, loss_in_test: 35.66227\n",
      "iter: 668, grad: 0.18239, loss_in_train: 2.15281, loss_in_test: 35.66193\n",
      "iter: 669, grad: 0.18198, loss_in_train: 2.15281, loss_in_test: 35.66160\n",
      "iter: 670, grad: 0.18156, loss_in_train: 2.15281, loss_in_test: 35.66127\n",
      "iter: 671, grad: 0.18115, loss_in_train: 2.15281, loss_in_test: 35.66095\n",
      "iter: 672, grad: 0.18073, loss_in_train: 2.15281, loss_in_test: 35.66063\n",
      "iter: 673, grad: 0.18032, loss_in_train: 2.15281, loss_in_test: 35.66031\n",
      "iter: 674, grad: 0.17991, loss_in_train: 2.15281, loss_in_test: 35.65999\n",
      "iter: 675, grad: 0.17950, loss_in_train: 2.15281, loss_in_test: 35.65967\n",
      "iter: 676, grad: 0.17909, loss_in_train: 2.15281, loss_in_test: 35.65936\n",
      "iter: 677, grad: 0.17868, loss_in_train: 2.15281, loss_in_test: 35.65905\n",
      "iter: 678, grad: 0.17827, loss_in_train: 2.15281, loss_in_test: 35.65874\n",
      "iter: 679, grad: 0.17787, loss_in_train: 2.15281, loss_in_test: 35.65843\n",
      "iter: 680, grad: 0.17746, loss_in_train: 2.15281, loss_in_test: 35.65813\n",
      "iter: 681, grad: 0.17706, loss_in_train: 2.15281, loss_in_test: 35.65783\n",
      "iter: 682, grad: 0.17665, loss_in_train: 2.15281, loss_in_test: 35.65753\n",
      "iter: 683, grad: 0.17625, loss_in_train: 2.15281, loss_in_test: 35.65723\n",
      "iter: 684, grad: 0.17585, loss_in_train: 2.15281, loss_in_test: 35.65693\n",
      "iter: 685, grad: 0.17545, loss_in_train: 2.15281, loss_in_test: 35.65664\n",
      "iter: 686, grad: 0.17505, loss_in_train: 2.15281, loss_in_test: 35.65635\n",
      "iter: 687, grad: 0.17465, loss_in_train: 2.15281, loss_in_test: 35.65606\n",
      "iter: 688, grad: 0.17426, loss_in_train: 2.15281, loss_in_test: 35.65577\n",
      "iter: 689, grad: 0.17386, loss_in_train: 2.15281, loss_in_test: 35.65549\n",
      "iter: 690, grad: 0.17346, loss_in_train: 2.15281, loss_in_test: 35.65521\n",
      "iter: 691, grad: 0.17307, loss_in_train: 2.15281, loss_in_test: 35.65493\n",
      "iter: 692, grad: 0.17268, loss_in_train: 2.15281, loss_in_test: 35.65465\n",
      "iter: 693, grad: 0.17228, loss_in_train: 2.15281, loss_in_test: 35.65437\n",
      "iter: 694, grad: 0.17189, loss_in_train: 2.15281, loss_in_test: 35.65410\n",
      "iter: 695, grad: 0.17150, loss_in_train: 2.15281, loss_in_test: 35.65383\n",
      "iter: 696, grad: 0.17111, loss_in_train: 2.15281, loss_in_test: 35.65356\n",
      "iter: 697, grad: 0.17072, loss_in_train: 2.15281, loss_in_test: 35.65329\n",
      "iter: 698, grad: 0.17034, loss_in_train: 2.15281, loss_in_test: 35.65302\n",
      "iter: 699, grad: 0.16995, loss_in_train: 2.15281, loss_in_test: 35.65276\n",
      "iter: 700, grad: 0.16956, loss_in_train: 2.15281, loss_in_test: 35.65250\n",
      "iter: 701, grad: 0.16918, loss_in_train: 2.15281, loss_in_test: 35.65224\n",
      "iter: 702, grad: 0.16880, loss_in_train: 2.15281, loss_in_test: 35.65198\n",
      "iter: 703, grad: 0.16841, loss_in_train: 2.15281, loss_in_test: 35.65173\n",
      "iter: 704, grad: 0.16803, loss_in_train: 2.15281, loss_in_test: 35.65147\n",
      "iter: 705, grad: 0.16765, loss_in_train: 2.15281, loss_in_test: 35.65122\n",
      "iter: 706, grad: 0.16727, loss_in_train: 2.15281, loss_in_test: 35.65097\n",
      "iter: 707, grad: 0.16689, loss_in_train: 2.15281, loss_in_test: 35.65073\n",
      "iter: 708, grad: 0.16651, loss_in_train: 2.15281, loss_in_test: 35.65048\n",
      "iter: 709, grad: 0.16614, loss_in_train: 2.15281, loss_in_test: 35.65024\n",
      "iter: 710, grad: 0.16576, loss_in_train: 2.15281, loss_in_test: 35.64999\n",
      "iter: 711, grad: 0.16538, loss_in_train: 2.15281, loss_in_test: 35.64975\n",
      "iter: 712, grad: 0.16501, loss_in_train: 2.15281, loss_in_test: 35.64952\n",
      "iter: 713, grad: 0.16464, loss_in_train: 2.15281, loss_in_test: 35.64928\n",
      "iter: 714, grad: 0.16426, loss_in_train: 2.15281, loss_in_test: 35.64905\n",
      "iter: 715, grad: 0.16389, loss_in_train: 2.15281, loss_in_test: 35.64881\n",
      "iter: 716, grad: 0.16352, loss_in_train: 2.15281, loss_in_test: 35.64858\n",
      "iter: 717, grad: 0.16315, loss_in_train: 2.15281, loss_in_test: 35.64835\n",
      "iter: 718, grad: 0.16278, loss_in_train: 2.15281, loss_in_test: 35.64813\n",
      "iter: 719, grad: 0.16242, loss_in_train: 2.15281, loss_in_test: 35.64790\n",
      "iter: 720, grad: 0.16205, loss_in_train: 2.15281, loss_in_test: 35.64768\n",
      "iter: 721, grad: 0.16168, loss_in_train: 2.15281, loss_in_test: 35.64746\n",
      "iter: 722, grad: 0.16132, loss_in_train: 2.15281, loss_in_test: 35.64724\n",
      "iter: 723, grad: 0.16096, loss_in_train: 2.15281, loss_in_test: 35.64702\n",
      "iter: 724, grad: 0.16059, loss_in_train: 2.15280, loss_in_test: 35.64680\n",
      "iter: 725, grad: 0.16023, loss_in_train: 2.15280, loss_in_test: 35.64659\n",
      "iter: 726, grad: 0.15987, loss_in_train: 2.15280, loss_in_test: 35.64638\n",
      "iter: 727, grad: 0.15951, loss_in_train: 2.15280, loss_in_test: 35.64617\n",
      "iter: 728, grad: 0.15915, loss_in_train: 2.15280, loss_in_test: 35.64596\n",
      "iter: 729, grad: 0.15879, loss_in_train: 2.15280, loss_in_test: 35.64575\n",
      "iter: 730, grad: 0.15843, loss_in_train: 2.15280, loss_in_test: 35.64554\n",
      "iter: 731, grad: 0.15808, loss_in_train: 2.15280, loss_in_test: 35.64534\n",
      "iter: 732, grad: 0.15772, loss_in_train: 2.15280, loss_in_test: 35.64514\n",
      "iter: 733, grad: 0.15736, loss_in_train: 2.15280, loss_in_test: 35.64494\n",
      "iter: 734, grad: 0.15701, loss_in_train: 2.15280, loss_in_test: 35.64474\n",
      "iter: 735, grad: 0.15666, loss_in_train: 2.15280, loss_in_test: 35.64454\n",
      "iter: 736, grad: 0.15630, loss_in_train: 2.15280, loss_in_test: 35.64434\n",
      "iter: 737, grad: 0.15595, loss_in_train: 2.15280, loss_in_test: 35.64415\n",
      "iter: 738, grad: 0.15560, loss_in_train: 2.15280, loss_in_test: 35.64396\n",
      "iter: 739, grad: 0.15525, loss_in_train: 2.15280, loss_in_test: 35.64376\n",
      "iter: 740, grad: 0.15490, loss_in_train: 2.15280, loss_in_test: 35.64358\n",
      "iter: 741, grad: 0.15456, loss_in_train: 2.15280, loss_in_test: 35.64339\n",
      "iter: 742, grad: 0.15421, loss_in_train: 2.15280, loss_in_test: 35.64320\n",
      "iter: 743, grad: 0.15386, loss_in_train: 2.15280, loss_in_test: 35.64302\n",
      "iter: 744, grad: 0.15352, loss_in_train: 2.15280, loss_in_test: 35.64283\n",
      "iter: 745, grad: 0.15317, loss_in_train: 2.15280, loss_in_test: 35.64265\n",
      "iter: 746, grad: 0.15283, loss_in_train: 2.15280, loss_in_test: 35.64247\n",
      "iter: 747, grad: 0.15249, loss_in_train: 2.15280, loss_in_test: 35.64229\n",
      "iter: 748, grad: 0.15214, loss_in_train: 2.15280, loss_in_test: 35.64212\n",
      "iter: 749, grad: 0.15180, loss_in_train: 2.15280, loss_in_test: 35.64194\n",
      "iter: 750, grad: 0.15146, loss_in_train: 2.15280, loss_in_test: 35.64177\n",
      "iter: 751, grad: 0.15112, loss_in_train: 2.15280, loss_in_test: 35.64159\n",
      "iter: 752, grad: 0.15078, loss_in_train: 2.15280, loss_in_test: 35.64142\n",
      "iter: 753, grad: 0.15045, loss_in_train: 2.15280, loss_in_test: 35.64125\n",
      "iter: 754, grad: 0.15011, loss_in_train: 2.15280, loss_in_test: 35.64109\n",
      "iter: 755, grad: 0.14977, loss_in_train: 2.15280, loss_in_test: 35.64092\n",
      "iter: 756, grad: 0.14944, loss_in_train: 2.15280, loss_in_test: 35.64075\n",
      "iter: 757, grad: 0.14910, loss_in_train: 2.15280, loss_in_test: 35.64059\n",
      "iter: 758, grad: 0.14877, loss_in_train: 2.15280, loss_in_test: 35.64043\n",
      "iter: 759, grad: 0.14844, loss_in_train: 2.15280, loss_in_test: 35.64027\n",
      "iter: 760, grad: 0.14811, loss_in_train: 2.15280, loss_in_test: 35.64011\n",
      "iter: 761, grad: 0.14778, loss_in_train: 2.15280, loss_in_test: 35.63995\n",
      "iter: 762, grad: 0.14745, loss_in_train: 2.15280, loss_in_test: 35.63979\n",
      "iter: 763, grad: 0.14712, loss_in_train: 2.15280, loss_in_test: 35.63964\n",
      "iter: 764, grad: 0.14679, loss_in_train: 2.15280, loss_in_test: 35.63948\n",
      "iter: 765, grad: 0.14646, loss_in_train: 2.15280, loss_in_test: 35.63933\n",
      "iter: 766, grad: 0.14613, loss_in_train: 2.15280, loss_in_test: 35.63918\n",
      "iter: 767, grad: 0.14581, loss_in_train: 2.15280, loss_in_test: 35.63903\n",
      "iter: 768, grad: 0.14548, loss_in_train: 2.15280, loss_in_test: 35.63888\n",
      "iter: 769, grad: 0.14516, loss_in_train: 2.15280, loss_in_test: 35.63874\n",
      "iter: 770, grad: 0.14483, loss_in_train: 2.15280, loss_in_test: 35.63859\n",
      "iter: 771, grad: 0.14451, loss_in_train: 2.15280, loss_in_test: 35.63845\n",
      "iter: 772, grad: 0.14419, loss_in_train: 2.15280, loss_in_test: 35.63830\n",
      "iter: 773, grad: 0.14387, loss_in_train: 2.15280, loss_in_test: 35.63816\n",
      "iter: 774, grad: 0.14355, loss_in_train: 2.15280, loss_in_test: 35.63802\n",
      "iter: 775, grad: 0.14323, loss_in_train: 2.15280, loss_in_test: 35.63788\n",
      "iter: 776, grad: 0.14291, loss_in_train: 2.15280, loss_in_test: 35.63774\n",
      "iter: 777, grad: 0.14259, loss_in_train: 2.15280, loss_in_test: 35.63761\n",
      "iter: 778, grad: 0.14227, loss_in_train: 2.15280, loss_in_test: 35.63747\n",
      "iter: 779, grad: 0.14196, loss_in_train: 2.15280, loss_in_test: 35.63734\n",
      "iter: 780, grad: 0.14164, loss_in_train: 2.15280, loss_in_test: 35.63720\n",
      "iter: 781, grad: 0.14133, loss_in_train: 2.15280, loss_in_test: 35.63707\n",
      "iter: 782, grad: 0.14101, loss_in_train: 2.15280, loss_in_test: 35.63694\n",
      "iter: 783, grad: 0.14070, loss_in_train: 2.15280, loss_in_test: 35.63681\n",
      "iter: 784, grad: 0.14039, loss_in_train: 2.15280, loss_in_test: 35.63669\n",
      "iter: 785, grad: 0.14007, loss_in_train: 2.15280, loss_in_test: 35.63656\n",
      "iter: 786, grad: 0.13976, loss_in_train: 2.15280, loss_in_test: 35.63643\n",
      "iter: 787, grad: 0.13945, loss_in_train: 2.15280, loss_in_test: 35.63631\n",
      "iter: 788, grad: 0.13914, loss_in_train: 2.15280, loss_in_test: 35.63619\n",
      "iter: 789, grad: 0.13884, loss_in_train: 2.15280, loss_in_test: 35.63606\n",
      "iter: 790, grad: 0.13853, loss_in_train: 2.15280, loss_in_test: 35.63594\n",
      "iter: 791, grad: 0.13822, loss_in_train: 2.15280, loss_in_test: 35.63582\n",
      "iter: 792, grad: 0.13791, loss_in_train: 2.15280, loss_in_test: 35.63571\n",
      "iter: 793, grad: 0.13761, loss_in_train: 2.15280, loss_in_test: 35.63559\n",
      "iter: 794, grad: 0.13730, loss_in_train: 2.15280, loss_in_test: 35.63547\n",
      "iter: 795, grad: 0.13700, loss_in_train: 2.15280, loss_in_test: 35.63536\n",
      "iter: 796, grad: 0.13670, loss_in_train: 2.15280, loss_in_test: 35.63524\n",
      "iter: 797, grad: 0.13639, loss_in_train: 2.15280, loss_in_test: 35.63513\n",
      "iter: 798, grad: 0.13609, loss_in_train: 2.15280, loss_in_test: 35.63502\n",
      "iter: 799, grad: 0.13579, loss_in_train: 2.15280, loss_in_test: 35.63491\n",
      "iter: 800, grad: 0.13549, loss_in_train: 2.15280, loss_in_test: 35.63480\n",
      "iter: 801, grad: 0.13519, loss_in_train: 2.15280, loss_in_test: 35.63469\n",
      "iter: 802, grad: 0.13489, loss_in_train: 2.15280, loss_in_test: 35.63458\n",
      "iter: 803, grad: 0.13460, loss_in_train: 2.15280, loss_in_test: 35.63448\n",
      "iter: 804, grad: 0.13430, loss_in_train: 2.15280, loss_in_test: 35.63437\n",
      "iter: 805, grad: 0.13400, loss_in_train: 2.15280, loss_in_test: 35.63427\n",
      "iter: 806, grad: 0.13371, loss_in_train: 2.15280, loss_in_test: 35.63417\n",
      "iter: 807, grad: 0.13341, loss_in_train: 2.15280, loss_in_test: 35.63407\n",
      "iter: 808, grad: 0.13312, loss_in_train: 2.15280, loss_in_test: 35.63396\n",
      "iter: 809, grad: 0.13282, loss_in_train: 2.15280, loss_in_test: 35.63387\n",
      "iter: 810, grad: 0.13253, loss_in_train: 2.15280, loss_in_test: 35.63377\n",
      "iter: 811, grad: 0.13224, loss_in_train: 2.15280, loss_in_test: 35.63367\n",
      "iter: 812, grad: 0.13195, loss_in_train: 2.15280, loss_in_test: 35.63357\n",
      "iter: 813, grad: 0.13166, loss_in_train: 2.15280, loss_in_test: 35.63348\n",
      "iter: 814, grad: 0.13137, loss_in_train: 2.15280, loss_in_test: 35.63338\n",
      "iter: 815, grad: 0.13108, loss_in_train: 2.15280, loss_in_test: 35.63329\n",
      "iter: 816, grad: 0.13079, loss_in_train: 2.15280, loss_in_test: 35.63320\n",
      "iter: 817, grad: 0.13050, loss_in_train: 2.15280, loss_in_test: 35.63310\n",
      "iter: 818, grad: 0.13021, loss_in_train: 2.15280, loss_in_test: 35.63301\n",
      "iter: 819, grad: 0.12993, loss_in_train: 2.15280, loss_in_test: 35.63292\n",
      "iter: 820, grad: 0.12964, loss_in_train: 2.15280, loss_in_test: 35.63284\n",
      "iter: 821, grad: 0.12936, loss_in_train: 2.15280, loss_in_test: 35.63275\n",
      "iter: 822, grad: 0.12907, loss_in_train: 2.15280, loss_in_test: 35.63266\n",
      "iter: 823, grad: 0.12879, loss_in_train: 2.15280, loss_in_test: 35.63258\n",
      "iter: 824, grad: 0.12851, loss_in_train: 2.15280, loss_in_test: 35.63249\n",
      "iter: 825, grad: 0.12823, loss_in_train: 2.15280, loss_in_test: 35.63241\n",
      "iter: 826, grad: 0.12794, loss_in_train: 2.15280, loss_in_test: 35.63233\n",
      "iter: 827, grad: 0.12766, loss_in_train: 2.15280, loss_in_test: 35.63224\n",
      "iter: 828, grad: 0.12738, loss_in_train: 2.15280, loss_in_test: 35.63216\n",
      "iter: 829, grad: 0.12710, loss_in_train: 2.15280, loss_in_test: 35.63208\n",
      "iter: 830, grad: 0.12683, loss_in_train: 2.15280, loss_in_test: 35.63200\n",
      "iter: 831, grad: 0.12655, loss_in_train: 2.15280, loss_in_test: 35.63192\n",
      "iter: 832, grad: 0.12627, loss_in_train: 2.15280, loss_in_test: 35.63185\n",
      "iter: 833, grad: 0.12599, loss_in_train: 2.15280, loss_in_test: 35.63177\n",
      "iter: 834, grad: 0.12572, loss_in_train: 2.15280, loss_in_test: 35.63170\n",
      "iter: 835, grad: 0.12544, loss_in_train: 2.15280, loss_in_test: 35.63162\n",
      "iter: 836, grad: 0.12517, loss_in_train: 2.15280, loss_in_test: 35.63155\n",
      "iter: 837, grad: 0.12490, loss_in_train: 2.15280, loss_in_test: 35.63147\n",
      "iter: 838, grad: 0.12462, loss_in_train: 2.15280, loss_in_test: 35.63140\n",
      "iter: 839, grad: 0.12435, loss_in_train: 2.15280, loss_in_test: 35.63133\n",
      "iter: 840, grad: 0.12408, loss_in_train: 2.15280, loss_in_test: 35.63126\n",
      "iter: 841, grad: 0.12381, loss_in_train: 2.15280, loss_in_test: 35.63119\n",
      "iter: 842, grad: 0.12354, loss_in_train: 2.15280, loss_in_test: 35.63112\n",
      "iter: 843, grad: 0.12327, loss_in_train: 2.15280, loss_in_test: 35.63105\n",
      "iter: 844, grad: 0.12300, loss_in_train: 2.15280, loss_in_test: 35.63099\n",
      "iter: 845, grad: 0.12273, loss_in_train: 2.15280, loss_in_test: 35.63092\n",
      "iter: 846, grad: 0.12246, loss_in_train: 2.15280, loss_in_test: 35.63086\n",
      "iter: 847, grad: 0.12220, loss_in_train: 2.15280, loss_in_test: 35.63079\n",
      "iter: 848, grad: 0.12193, loss_in_train: 2.15280, loss_in_test: 35.63073\n",
      "iter: 849, grad: 0.12166, loss_in_train: 2.15280, loss_in_test: 35.63067\n",
      "iter: 850, grad: 0.12140, loss_in_train: 2.15280, loss_in_test: 35.63060\n",
      "iter: 851, grad: 0.12114, loss_in_train: 2.15280, loss_in_test: 35.63054\n",
      "iter: 852, grad: 0.12087, loss_in_train: 2.15280, loss_in_test: 35.63048\n",
      "iter: 853, grad: 0.12061, loss_in_train: 2.15280, loss_in_test: 35.63042\n",
      "iter: 854, grad: 0.12035, loss_in_train: 2.15280, loss_in_test: 35.63036\n",
      "iter: 855, grad: 0.12008, loss_in_train: 2.15280, loss_in_test: 35.63031\n",
      "iter: 856, grad: 0.11982, loss_in_train: 2.15280, loss_in_test: 35.63025\n",
      "iter: 857, grad: 0.11956, loss_in_train: 2.15280, loss_in_test: 35.63019\n",
      "iter: 858, grad: 0.11930, loss_in_train: 2.15280, loss_in_test: 35.63014\n",
      "iter: 859, grad: 0.11904, loss_in_train: 2.15280, loss_in_test: 35.63008\n",
      "iter: 860, grad: 0.11879, loss_in_train: 2.15280, loss_in_test: 35.63003\n",
      "iter: 861, grad: 0.11853, loss_in_train: 2.15280, loss_in_test: 35.62997\n",
      "iter: 862, grad: 0.11827, loss_in_train: 2.15280, loss_in_test: 35.62992\n",
      "iter: 863, grad: 0.11802, loss_in_train: 2.15280, loss_in_test: 35.62987\n",
      "iter: 864, grad: 0.11776, loss_in_train: 2.15280, loss_in_test: 35.62982\n",
      "iter: 865, grad: 0.11750, loss_in_train: 2.15280, loss_in_test: 35.62977\n",
      "iter: 866, grad: 0.11725, loss_in_train: 2.15280, loss_in_test: 35.62972\n",
      "iter: 867, grad: 0.11700, loss_in_train: 2.15280, loss_in_test: 35.62967\n",
      "iter: 868, grad: 0.11674, loss_in_train: 2.15280, loss_in_test: 35.62962\n",
      "iter: 869, grad: 0.11649, loss_in_train: 2.15280, loss_in_test: 35.62957\n",
      "iter: 870, grad: 0.11624, loss_in_train: 2.15280, loss_in_test: 35.62952\n",
      "iter: 871, grad: 0.11599, loss_in_train: 2.15280, loss_in_test: 35.62948\n",
      "iter: 872, grad: 0.11574, loss_in_train: 2.15280, loss_in_test: 35.62943\n",
      "iter: 873, grad: 0.11549, loss_in_train: 2.15280, loss_in_test: 35.62939\n",
      "iter: 874, grad: 0.11524, loss_in_train: 2.15280, loss_in_test: 35.62934\n",
      "iter: 875, grad: 0.11499, loss_in_train: 2.15280, loss_in_test: 35.62930\n",
      "iter: 876, grad: 0.11474, loss_in_train: 2.15280, loss_in_test: 35.62926\n",
      "iter: 877, grad: 0.11449, loss_in_train: 2.15280, loss_in_test: 35.62921\n",
      "iter: 878, grad: 0.11424, loss_in_train: 2.15280, loss_in_test: 35.62917\n",
      "iter: 879, grad: 0.11400, loss_in_train: 2.15280, loss_in_test: 35.62913\n",
      "iter: 880, grad: 0.11375, loss_in_train: 2.15280, loss_in_test: 35.62909\n",
      "iter: 881, grad: 0.11351, loss_in_train: 2.15280, loss_in_test: 35.62905\n",
      "iter: 882, grad: 0.11326, loss_in_train: 2.15280, loss_in_test: 35.62901\n",
      "iter: 883, grad: 0.11302, loss_in_train: 2.15280, loss_in_test: 35.62898\n",
      "iter: 884, grad: 0.11278, loss_in_train: 2.15280, loss_in_test: 35.62894\n",
      "iter: 885, grad: 0.11253, loss_in_train: 2.15280, loss_in_test: 35.62890\n",
      "iter: 886, grad: 0.11229, loss_in_train: 2.15280, loss_in_test: 35.62886\n",
      "iter: 887, grad: 0.11205, loss_in_train: 2.15280, loss_in_test: 35.62883\n",
      "iter: 888, grad: 0.11181, loss_in_train: 2.15280, loss_in_test: 35.62879\n",
      "iter: 889, grad: 0.11157, loss_in_train: 2.15280, loss_in_test: 35.62876\n",
      "iter: 890, grad: 0.11133, loss_in_train: 2.15280, loss_in_test: 35.62873\n",
      "iter: 891, grad: 0.11109, loss_in_train: 2.15280, loss_in_test: 35.62869\n",
      "iter: 892, grad: 0.11085, loss_in_train: 2.15280, loss_in_test: 35.62866\n",
      "iter: 893, grad: 0.11061, loss_in_train: 2.15280, loss_in_test: 35.62863\n",
      "iter: 894, grad: 0.11038, loss_in_train: 2.15280, loss_in_test: 35.62860\n",
      "iter: 895, grad: 0.11014, loss_in_train: 2.15280, loss_in_test: 35.62857\n",
      "iter: 896, grad: 0.10990, loss_in_train: 2.15280, loss_in_test: 35.62854\n",
      "iter: 897, grad: 0.10967, loss_in_train: 2.15280, loss_in_test: 35.62851\n",
      "iter: 898, grad: 0.10943, loss_in_train: 2.15280, loss_in_test: 35.62848\n",
      "iter: 899, grad: 0.10920, loss_in_train: 2.15280, loss_in_test: 35.62845\n",
      "iter: 900, grad: 0.10896, loss_in_train: 2.15280, loss_in_test: 35.62842\n",
      "iter: 901, grad: 0.10873, loss_in_train: 2.15280, loss_in_test: 35.62839\n",
      "iter: 902, grad: 0.10850, loss_in_train: 2.15280, loss_in_test: 35.62837\n",
      "iter: 903, grad: 0.10827, loss_in_train: 2.15280, loss_in_test: 35.62834\n",
      "iter: 904, grad: 0.10804, loss_in_train: 2.15280, loss_in_test: 35.62831\n",
      "iter: 905, grad: 0.10780, loss_in_train: 2.15280, loss_in_test: 35.62829\n",
      "iter: 906, grad: 0.10757, loss_in_train: 2.15280, loss_in_test: 35.62826\n",
      "iter: 907, grad: 0.10734, loss_in_train: 2.15280, loss_in_test: 35.62824\n",
      "iter: 908, grad: 0.10712, loss_in_train: 2.15280, loss_in_test: 35.62822\n",
      "iter: 909, grad: 0.10689, loss_in_train: 2.15280, loss_in_test: 35.62819\n",
      "iter: 910, grad: 0.10666, loss_in_train: 2.15280, loss_in_test: 35.62817\n",
      "iter: 911, grad: 0.10643, loss_in_train: 2.15280, loss_in_test: 35.62815\n",
      "iter: 912, grad: 0.10621, loss_in_train: 2.15280, loss_in_test: 35.62813\n",
      "iter: 913, grad: 0.10598, loss_in_train: 2.15280, loss_in_test: 35.62811\n",
      "iter: 914, grad: 0.10575, loss_in_train: 2.15280, loss_in_test: 35.62809\n",
      "iter: 915, grad: 0.10553, loss_in_train: 2.15280, loss_in_test: 35.62807\n",
      "iter: 916, grad: 0.10530, loss_in_train: 2.15280, loss_in_test: 35.62805\n",
      "iter: 917, grad: 0.10508, loss_in_train: 2.15280, loss_in_test: 35.62803\n",
      "iter: 918, grad: 0.10486, loss_in_train: 2.15280, loss_in_test: 35.62801\n",
      "iter: 919, grad: 0.10463, loss_in_train: 2.15280, loss_in_test: 35.62799\n",
      "iter: 920, grad: 0.10441, loss_in_train: 2.15280, loss_in_test: 35.62798\n",
      "iter: 921, grad: 0.10419, loss_in_train: 2.15280, loss_in_test: 35.62796\n",
      "iter: 922, grad: 0.10397, loss_in_train: 2.15280, loss_in_test: 35.62794\n",
      "iter: 923, grad: 0.10375, loss_in_train: 2.15280, loss_in_test: 35.62793\n",
      "iter: 924, grad: 0.10353, loss_in_train: 2.15280, loss_in_test: 35.62791\n",
      "iter: 925, grad: 0.10331, loss_in_train: 2.15280, loss_in_test: 35.62790\n",
      "iter: 926, grad: 0.10309, loss_in_train: 2.15280, loss_in_test: 35.62788\n",
      "iter: 927, grad: 0.10287, loss_in_train: 2.15280, loss_in_test: 35.62787\n",
      "iter: 928, grad: 0.10265, loss_in_train: 2.15280, loss_in_test: 35.62786\n",
      "iter: 929, grad: 0.10243, loss_in_train: 2.15280, loss_in_test: 35.62784\n",
      "iter: 930, grad: 0.10222, loss_in_train: 2.15280, loss_in_test: 35.62783\n",
      "iter: 931, grad: 0.10200, loss_in_train: 2.15280, loss_in_test: 35.62782\n",
      "iter: 932, grad: 0.10179, loss_in_train: 2.15280, loss_in_test: 35.62781\n",
      "iter: 933, grad: 0.10157, loss_in_train: 2.15280, loss_in_test: 35.62780\n",
      "iter: 934, grad: 0.10136, loss_in_train: 2.15280, loss_in_test: 35.62779\n",
      "iter: 935, grad: 0.10114, loss_in_train: 2.15280, loss_in_test: 35.62778\n",
      "iter: 936, grad: 0.10093, loss_in_train: 2.15280, loss_in_test: 35.62777\n",
      "iter: 937, grad: 0.10072, loss_in_train: 2.15280, loss_in_test: 35.62776\n",
      "iter: 938, grad: 0.10050, loss_in_train: 2.15280, loss_in_test: 35.62775\n",
      "iter: 939, grad: 0.10029, loss_in_train: 2.15280, loss_in_test: 35.62774\n",
      "iter: 940, grad: 0.10008, loss_in_train: 2.15280, loss_in_test: 35.62773\n",
      "iter: 941, grad: 0.09987, loss_in_train: 2.15280, loss_in_test: 35.62772\n",
      "iter: 942, grad: 0.09966, loss_in_train: 2.15280, loss_in_test: 35.62772\n",
      "iter: 943, grad: 0.09945, loss_in_train: 2.15280, loss_in_test: 35.62771\n",
      "iter: 944, grad: 0.09924, loss_in_train: 2.15280, loss_in_test: 35.62770\n",
      "iter: 945, grad: 0.09903, loss_in_train: 2.15280, loss_in_test: 35.62770\n",
      "iter: 946, grad: 0.09882, loss_in_train: 2.15280, loss_in_test: 35.62769\n",
      "iter: 947, grad: 0.09861, loss_in_train: 2.15280, loss_in_test: 35.62769\n",
      "iter: 948, grad: 0.09841, loss_in_train: 2.15280, loss_in_test: 35.62768\n",
      "iter: 949, grad: 0.09820, loss_in_train: 2.15280, loss_in_test: 35.62768\n",
      "iter: 950, grad: 0.09799, loss_in_train: 2.15280, loss_in_test: 35.62768\n",
      "iter: 951, grad: 0.09779, loss_in_train: 2.15280, loss_in_test: 35.62767\n",
      "iter: 952, grad: 0.09758, loss_in_train: 2.15280, loss_in_test: 35.62767\n",
      "iter: 953, grad: 0.09738, loss_in_train: 2.15280, loss_in_test: 35.62767\n",
      "iter: 954, grad: 0.09717, loss_in_train: 2.15280, loss_in_test: 35.62766\n",
      "iter: 955, grad: 0.09697, loss_in_train: 2.15280, loss_in_test: 35.62766\n",
      "iter: 956, grad: 0.09677, loss_in_train: 2.15280, loss_in_test: 35.62766\n",
      "iter: 957, grad: 0.09656, loss_in_train: 2.15280, loss_in_test: 35.62766\n",
      "iter: 958, grad: 0.09636, loss_in_train: 2.15280, loss_in_test: 35.62766\n",
      "iter: 959, grad: 0.09616, loss_in_train: 2.15280, loss_in_test: 35.62766\n",
      "iter: 960, grad: 0.09596, loss_in_train: 2.15280, loss_in_test: 35.62766\n",
      "iter: 961, grad: 0.09576, loss_in_train: 2.15280, loss_in_test: 35.62766\n",
      "iter: 962, grad: 0.09556, loss_in_train: 2.15280, loss_in_test: 35.62766\n",
      "iter: 963, grad: 0.09536, loss_in_train: 2.15280, loss_in_test: 35.62766\n",
      "iter: 964, grad: 0.09516, loss_in_train: 2.15280, loss_in_test: 35.62766\n",
      "iter: 965, grad: 0.09496, loss_in_train: 2.15280, loss_in_test: 35.62767\n",
      "iter: 966, grad: 0.09476, loss_in_train: 2.15280, loss_in_test: 35.62767\n",
      "iter: 967, grad: 0.09456, loss_in_train: 2.15280, loss_in_test: 35.62767\n",
      "iter: 968, grad: 0.09437, loss_in_train: 2.15280, loss_in_test: 35.62767\n",
      "iter: 969, grad: 0.09417, loss_in_train: 2.15280, loss_in_test: 35.62768\n",
      "iter: 970, grad: 0.09397, loss_in_train: 2.15280, loss_in_test: 35.62768\n",
      "iter: 971, grad: 0.09378, loss_in_train: 2.15280, loss_in_test: 35.62768\n",
      "iter: 972, grad: 0.09358, loss_in_train: 2.15280, loss_in_test: 35.62769\n",
      "iter: 973, grad: 0.09339, loss_in_train: 2.15280, loss_in_test: 35.62769\n",
      "iter: 974, grad: 0.09320, loss_in_train: 2.15280, loss_in_test: 35.62770\n",
      "iter: 975, grad: 0.09300, loss_in_train: 2.15280, loss_in_test: 35.62770\n",
      "iter: 976, grad: 0.09281, loss_in_train: 2.15280, loss_in_test: 35.62771\n",
      "iter: 977, grad: 0.09262, loss_in_train: 2.15280, loss_in_test: 35.62772\n",
      "iter: 978, grad: 0.09242, loss_in_train: 2.15280, loss_in_test: 35.62772\n",
      "iter: 979, grad: 0.09223, loss_in_train: 2.15280, loss_in_test: 35.62773\n",
      "iter: 980, grad: 0.09204, loss_in_train: 2.15280, loss_in_test: 35.62774\n",
      "iter: 981, grad: 0.09185, loss_in_train: 2.15280, loss_in_test: 35.62774\n",
      "iter: 982, grad: 0.09166, loss_in_train: 2.15280, loss_in_test: 35.62775\n",
      "iter: 983, grad: 0.09147, loss_in_train: 2.15280, loss_in_test: 35.62776\n",
      "iter: 984, grad: 0.09128, loss_in_train: 2.15280, loss_in_test: 35.62777\n",
      "iter: 985, grad: 0.09109, loss_in_train: 2.15280, loss_in_test: 35.62778\n",
      "iter: 986, grad: 0.09090, loss_in_train: 2.15280, loss_in_test: 35.62779\n",
      "iter: 987, grad: 0.09071, loss_in_train: 2.15280, loss_in_test: 35.62779\n",
      "iter: 988, grad: 0.09053, loss_in_train: 2.15280, loss_in_test: 35.62780\n",
      "iter: 989, grad: 0.09034, loss_in_train: 2.15280, loss_in_test: 35.62781\n",
      "iter: 990, grad: 0.09015, loss_in_train: 2.15280, loss_in_test: 35.62782\n",
      "iter: 991, grad: 0.08997, loss_in_train: 2.15280, loss_in_test: 35.62783\n",
      "iter: 992, grad: 0.08978, loss_in_train: 2.15280, loss_in_test: 35.62784\n",
      "iter: 993, grad: 0.08960, loss_in_train: 2.15280, loss_in_test: 35.62786\n",
      "iter: 994, grad: 0.08941, loss_in_train: 2.15280, loss_in_test: 35.62787\n",
      "iter: 995, grad: 0.08923, loss_in_train: 2.15280, loss_in_test: 35.62788\n",
      "iter: 996, grad: 0.08904, loss_in_train: 2.15280, loss_in_test: 35.62789\n",
      "iter: 997, grad: 0.08886, loss_in_train: 2.15280, loss_in_test: 35.62790\n",
      "iter: 998, grad: 0.08868, loss_in_train: 2.15280, loss_in_test: 35.62791\n",
      "iter: 999, grad: 0.08849, loss_in_train: 2.15280, loss_in_test: 35.62793\n",
      "iter: 1000, grad: 0.08831, loss_in_train: 2.15280, loss_in_test: 35.62794\n",
      "iter: 1001, grad: 0.08813, loss_in_train: 2.15280, loss_in_test: 35.62795\n",
      "iter: 1002, grad: 0.08795, loss_in_train: 2.15280, loss_in_test: 35.62797\n",
      "iter: 1003, grad: 0.08777, loss_in_train: 2.15280, loss_in_test: 35.62798\n",
      "iter: 1004, grad: 0.08759, loss_in_train: 2.15280, loss_in_test: 35.62799\n",
      "iter: 1005, grad: 0.08741, loss_in_train: 2.15280, loss_in_test: 35.62801\n",
      "iter: 1006, grad: 0.08723, loss_in_train: 2.15280, loss_in_test: 35.62802\n",
      "iter: 1007, grad: 0.08705, loss_in_train: 2.15280, loss_in_test: 35.62804\n",
      "iter: 1008, grad: 0.08687, loss_in_train: 2.15280, loss_in_test: 35.62805\n",
      "iter: 1009, grad: 0.08669, loss_in_train: 2.15280, loss_in_test: 35.62807\n",
      "iter: 1010, grad: 0.08652, loss_in_train: 2.15280, loss_in_test: 35.62808\n",
      "iter: 1011, grad: 0.08634, loss_in_train: 2.15280, loss_in_test: 35.62810\n",
      "iter: 1012, grad: 0.08616, loss_in_train: 2.15280, loss_in_test: 35.62812\n",
      "iter: 1013, grad: 0.08599, loss_in_train: 2.15280, loss_in_test: 35.62813\n",
      "iter: 1014, grad: 0.08581, loss_in_train: 2.15280, loss_in_test: 35.62815\n",
      "iter: 1015, grad: 0.08564, loss_in_train: 2.15280, loss_in_test: 35.62817\n",
      "iter: 1016, grad: 0.08546, loss_in_train: 2.15280, loss_in_test: 35.62818\n",
      "iter: 1017, grad: 0.08529, loss_in_train: 2.15280, loss_in_test: 35.62820\n",
      "iter: 1018, grad: 0.08511, loss_in_train: 2.15280, loss_in_test: 35.62822\n",
      "iter: 1019, grad: 0.08494, loss_in_train: 2.15280, loss_in_test: 35.62824\n",
      "iter: 1020, grad: 0.08477, loss_in_train: 2.15280, loss_in_test: 35.62825\n",
      "iter: 1021, grad: 0.08459, loss_in_train: 2.15280, loss_in_test: 35.62827\n",
      "iter: 1022, grad: 0.08442, loss_in_train: 2.15280, loss_in_test: 35.62829\n",
      "iter: 1023, grad: 0.08425, loss_in_train: 2.15280, loss_in_test: 35.62831\n",
      "iter: 1024, grad: 0.08408, loss_in_train: 2.15280, loss_in_test: 35.62833\n",
      "iter: 1025, grad: 0.08391, loss_in_train: 2.15280, loss_in_test: 35.62835\n",
      "iter: 1026, grad: 0.08374, loss_in_train: 2.15280, loss_in_test: 35.62837\n",
      "iter: 1027, grad: 0.08357, loss_in_train: 2.15280, loss_in_test: 35.62839\n",
      "iter: 1028, grad: 0.08340, loss_in_train: 2.15280, loss_in_test: 35.62841\n",
      "iter: 1029, grad: 0.08323, loss_in_train: 2.15280, loss_in_test: 35.62843\n",
      "iter: 1030, grad: 0.08306, loss_in_train: 2.15280, loss_in_test: 35.62845\n",
      "iter: 1031, grad: 0.08289, loss_in_train: 2.15280, loss_in_test: 35.62847\n",
      "iter: 1032, grad: 0.08272, loss_in_train: 2.15280, loss_in_test: 35.62849\n",
      "iter: 1033, grad: 0.08256, loss_in_train: 2.15280, loss_in_test: 35.62851\n",
      "iter: 1034, grad: 0.08239, loss_in_train: 2.15280, loss_in_test: 35.62853\n",
      "iter: 1035, grad: 0.08222, loss_in_train: 2.15280, loss_in_test: 35.62855\n",
      "iter: 1036, grad: 0.08206, loss_in_train: 2.15280, loss_in_test: 35.62857\n",
      "iter: 1037, grad: 0.08189, loss_in_train: 2.15280, loss_in_test: 35.62859\n",
      "iter: 1038, grad: 0.08172, loss_in_train: 2.15280, loss_in_test: 35.62861\n",
      "iter: 1039, grad: 0.08156, loss_in_train: 2.15280, loss_in_test: 35.62864\n",
      "iter: 1040, grad: 0.08139, loss_in_train: 2.15280, loss_in_test: 35.62866\n",
      "iter: 1041, grad: 0.08123, loss_in_train: 2.15280, loss_in_test: 35.62868\n",
      "iter: 1042, grad: 0.08107, loss_in_train: 2.15280, loss_in_test: 35.62870\n",
      "iter: 1043, grad: 0.08090, loss_in_train: 2.15280, loss_in_test: 35.62873\n",
      "iter: 1044, grad: 0.08074, loss_in_train: 2.15280, loss_in_test: 35.62875\n",
      "iter: 1045, grad: 0.08058, loss_in_train: 2.15280, loss_in_test: 35.62877\n",
      "iter: 1046, grad: 0.08041, loss_in_train: 2.15280, loss_in_test: 35.62880\n",
      "iter: 1047, grad: 0.08025, loss_in_train: 2.15280, loss_in_test: 35.62882\n",
      "iter: 1048, grad: 0.08009, loss_in_train: 2.15280, loss_in_test: 35.62884\n",
      "iter: 1049, grad: 0.07993, loss_in_train: 2.15280, loss_in_test: 35.62887\n",
      "iter: 1050, grad: 0.07977, loss_in_train: 2.15280, loss_in_test: 35.62889\n",
      "iter: 1051, grad: 0.07961, loss_in_train: 2.15280, loss_in_test: 35.62892\n",
      "iter: 1052, grad: 0.07945, loss_in_train: 2.15280, loss_in_test: 35.62894\n",
      "iter: 1053, grad: 0.07929, loss_in_train: 2.15280, loss_in_test: 35.62896\n",
      "iter: 1054, grad: 0.07913, loss_in_train: 2.15280, loss_in_test: 35.62899\n",
      "iter: 1055, grad: 0.07897, loss_in_train: 2.15280, loss_in_test: 35.62901\n",
      "iter: 1056, grad: 0.07882, loss_in_train: 2.15280, loss_in_test: 35.62904\n",
      "iter: 1057, grad: 0.07866, loss_in_train: 2.15280, loss_in_test: 35.62906\n",
      "iter: 1058, grad: 0.07850, loss_in_train: 2.15280, loss_in_test: 35.62909\n",
      "iter: 1059, grad: 0.07834, loss_in_train: 2.15280, loss_in_test: 35.62912\n",
      "iter: 1060, grad: 0.07819, loss_in_train: 2.15280, loss_in_test: 35.62914\n",
      "iter: 1061, grad: 0.07803, loss_in_train: 2.15280, loss_in_test: 35.62917\n",
      "iter: 1062, grad: 0.07787, loss_in_train: 2.15280, loss_in_test: 35.62919\n",
      "iter: 1063, grad: 0.07772, loss_in_train: 2.15280, loss_in_test: 35.62922\n",
      "iter: 1064, grad: 0.07756, loss_in_train: 2.15280, loss_in_test: 35.62925\n",
      "iter: 1065, grad: 0.07741, loss_in_train: 2.15280, loss_in_test: 35.62927\n",
      "iter: 1066, grad: 0.07725, loss_in_train: 2.15280, loss_in_test: 35.62930\n",
      "iter: 1067, grad: 0.07710, loss_in_train: 2.15280, loss_in_test: 35.62933\n",
      "iter: 1068, grad: 0.07695, loss_in_train: 2.15280, loss_in_test: 35.62935\n",
      "iter: 1069, grad: 0.07679, loss_in_train: 2.15280, loss_in_test: 35.62938\n",
      "iter: 1070, grad: 0.07664, loss_in_train: 2.15280, loss_in_test: 35.62941\n",
      "iter: 1071, grad: 0.07649, loss_in_train: 2.15280, loss_in_test: 35.62943\n",
      "iter: 1072, grad: 0.07634, loss_in_train: 2.15280, loss_in_test: 35.62946\n",
      "iter: 1073, grad: 0.07619, loss_in_train: 2.15280, loss_in_test: 35.62949\n",
      "iter: 1074, grad: 0.07603, loss_in_train: 2.15280, loss_in_test: 35.62952\n",
      "iter: 1075, grad: 0.07588, loss_in_train: 2.15280, loss_in_test: 35.62955\n",
      "iter: 1076, grad: 0.07573, loss_in_train: 2.15280, loss_in_test: 35.62957\n",
      "iter: 1077, grad: 0.07558, loss_in_train: 2.15280, loss_in_test: 35.62960\n",
      "iter: 1078, grad: 0.07543, loss_in_train: 2.15280, loss_in_test: 35.62963\n",
      "iter: 1079, grad: 0.07528, loss_in_train: 2.15280, loss_in_test: 35.62966\n",
      "iter: 1080, grad: 0.07514, loss_in_train: 2.15280, loss_in_test: 35.62969\n",
      "iter: 1081, grad: 0.07499, loss_in_train: 2.15280, loss_in_test: 35.62972\n",
      "iter: 1082, grad: 0.07484, loss_in_train: 2.15280, loss_in_test: 35.62975\n",
      "iter: 1083, grad: 0.07469, loss_in_train: 2.15280, loss_in_test: 35.62977\n",
      "iter: 1084, grad: 0.07454, loss_in_train: 2.15280, loss_in_test: 35.62980\n",
      "iter: 1085, grad: 0.07440, loss_in_train: 2.15280, loss_in_test: 35.62983\n",
      "iter: 1086, grad: 0.07425, loss_in_train: 2.15280, loss_in_test: 35.62986\n",
      "iter: 1087, grad: 0.07410, loss_in_train: 2.15280, loss_in_test: 35.62989\n",
      "iter: 1088, grad: 0.07396, loss_in_train: 2.15280, loss_in_test: 35.62992\n",
      "iter: 1089, grad: 0.07381, loss_in_train: 2.15280, loss_in_test: 35.62995\n",
      "iter: 1090, grad: 0.07367, loss_in_train: 2.15280, loss_in_test: 35.62998\n",
      "iter: 1091, grad: 0.07352, loss_in_train: 2.15280, loss_in_test: 35.63001\n",
      "iter: 1092, grad: 0.07338, loss_in_train: 2.15280, loss_in_test: 35.63004\n",
      "iter: 1093, grad: 0.07323, loss_in_train: 2.15280, loss_in_test: 35.63007\n",
      "iter: 1094, grad: 0.07309, loss_in_train: 2.15280, loss_in_test: 35.63010\n",
      "iter: 1095, grad: 0.07294, loss_in_train: 2.15280, loss_in_test: 35.63013\n",
      "iter: 1096, grad: 0.07280, loss_in_train: 2.15280, loss_in_test: 35.63016\n",
      "iter: 1097, grad: 0.07266, loss_in_train: 2.15280, loss_in_test: 35.63019\n",
      "iter: 1098, grad: 0.07252, loss_in_train: 2.15280, loss_in_test: 35.63023\n",
      "iter: 1099, grad: 0.07237, loss_in_train: 2.15280, loss_in_test: 35.63026\n",
      "iter: 1100, grad: 0.07223, loss_in_train: 2.15280, loss_in_test: 35.63029\n",
      "iter: 1101, grad: 0.07209, loss_in_train: 2.15280, loss_in_test: 35.63032\n",
      "iter: 1102, grad: 0.07195, loss_in_train: 2.15280, loss_in_test: 35.63035\n",
      "iter: 1103, grad: 0.07181, loss_in_train: 2.15280, loss_in_test: 35.63038\n",
      "iter: 1104, grad: 0.07167, loss_in_train: 2.15280, loss_in_test: 35.63041\n",
      "iter: 1105, grad: 0.07153, loss_in_train: 2.15280, loss_in_test: 35.63044\n",
      "iter: 1106, grad: 0.07139, loss_in_train: 2.15280, loss_in_test: 35.63048\n",
      "iter: 1107, grad: 0.07125, loss_in_train: 2.15280, loss_in_test: 35.63051\n",
      "iter: 1108, grad: 0.07111, loss_in_train: 2.15280, loss_in_test: 35.63054\n",
      "iter: 1109, grad: 0.07097, loss_in_train: 2.15280, loss_in_test: 35.63057\n",
      "iter: 1110, grad: 0.07084, loss_in_train: 2.15280, loss_in_test: 35.63060\n",
      "iter: 1111, grad: 0.07070, loss_in_train: 2.15280, loss_in_test: 35.63064\n",
      "iter: 1112, grad: 0.07056, loss_in_train: 2.15280, loss_in_test: 35.63067\n",
      "iter: 1113, grad: 0.07042, loss_in_train: 2.15280, loss_in_test: 35.63070\n",
      "iter: 1114, grad: 0.07029, loss_in_train: 2.15280, loss_in_test: 35.63073\n",
      "iter: 1115, grad: 0.07015, loss_in_train: 2.15280, loss_in_test: 35.63077\n",
      "iter: 1116, grad: 0.07001, loss_in_train: 2.15280, loss_in_test: 35.63080\n",
      "iter: 1117, grad: 0.06988, loss_in_train: 2.15280, loss_in_test: 35.63083\n",
      "iter: 1118, grad: 0.06974, loss_in_train: 2.15280, loss_in_test: 35.63086\n",
      "iter: 1119, grad: 0.06961, loss_in_train: 2.15280, loss_in_test: 35.63090\n",
      "iter: 1120, grad: 0.06947, loss_in_train: 2.15280, loss_in_test: 35.63093\n",
      "iter: 1121, grad: 0.06934, loss_in_train: 2.15280, loss_in_test: 35.63096\n",
      "iter: 1122, grad: 0.06920, loss_in_train: 2.15280, loss_in_test: 35.63100\n",
      "iter: 1123, grad: 0.06907, loss_in_train: 2.15280, loss_in_test: 35.63103\n",
      "iter: 1124, grad: 0.06894, loss_in_train: 2.15280, loss_in_test: 35.63106\n",
      "iter: 1125, grad: 0.06880, loss_in_train: 2.15280, loss_in_test: 35.63110\n",
      "iter: 1126, grad: 0.06867, loss_in_train: 2.15280, loss_in_test: 35.63113\n",
      "iter: 1127, grad: 0.06854, loss_in_train: 2.15280, loss_in_test: 35.63116\n",
      "iter: 1128, grad: 0.06841, loss_in_train: 2.15280, loss_in_test: 35.63120\n",
      "iter: 1129, grad: 0.06827, loss_in_train: 2.15280, loss_in_test: 35.63123\n",
      "iter: 1130, grad: 0.06814, loss_in_train: 2.15280, loss_in_test: 35.63126\n",
      "iter: 1131, grad: 0.06801, loss_in_train: 2.15280, loss_in_test: 35.63130\n",
      "iter: 1132, grad: 0.06788, loss_in_train: 2.15280, loss_in_test: 35.63133\n",
      "iter: 1133, grad: 0.06775, loss_in_train: 2.15280, loss_in_test: 35.63137\n",
      "iter: 1134, grad: 0.06762, loss_in_train: 2.15280, loss_in_test: 35.63140\n",
      "iter: 1135, grad: 0.06749, loss_in_train: 2.15280, loss_in_test: 35.63143\n",
      "iter: 1136, grad: 0.06736, loss_in_train: 2.15280, loss_in_test: 35.63147\n",
      "iter: 1137, grad: 0.06723, loss_in_train: 2.15280, loss_in_test: 35.63150\n",
      "iter: 1138, grad: 0.06710, loss_in_train: 2.15280, loss_in_test: 35.63154\n",
      "iter: 1139, grad: 0.06698, loss_in_train: 2.15280, loss_in_test: 35.63157\n",
      "iter: 1140, grad: 0.06685, loss_in_train: 2.15280, loss_in_test: 35.63161\n",
      "iter: 1141, grad: 0.06672, loss_in_train: 2.15280, loss_in_test: 35.63164\n",
      "iter: 1142, grad: 0.06659, loss_in_train: 2.15280, loss_in_test: 35.63168\n",
      "iter: 1143, grad: 0.06646, loss_in_train: 2.15280, loss_in_test: 35.63171\n",
      "iter: 1144, grad: 0.06634, loss_in_train: 2.15280, loss_in_test: 35.63175\n",
      "iter: 1145, grad: 0.06621, loss_in_train: 2.15280, loss_in_test: 35.63178\n",
      "iter: 1146, grad: 0.06608, loss_in_train: 2.15280, loss_in_test: 35.63182\n",
      "iter: 1147, grad: 0.06596, loss_in_train: 2.15280, loss_in_test: 35.63185\n",
      "iter: 1148, grad: 0.06583, loss_in_train: 2.15280, loss_in_test: 35.63188\n",
      "iter: 1149, grad: 0.06571, loss_in_train: 2.15280, loss_in_test: 35.63192\n",
      "iter: 1150, grad: 0.06558, loss_in_train: 2.15280, loss_in_test: 35.63196\n",
      "iter: 1151, grad: 0.06546, loss_in_train: 2.15280, loss_in_test: 35.63199\n",
      "iter: 1152, grad: 0.06533, loss_in_train: 2.15280, loss_in_test: 35.63203\n",
      "iter: 1153, grad: 0.06521, loss_in_train: 2.15280, loss_in_test: 35.63206\n",
      "iter: 1154, grad: 0.06509, loss_in_train: 2.15280, loss_in_test: 35.63210\n",
      "iter: 1155, grad: 0.06496, loss_in_train: 2.15280, loss_in_test: 35.63213\n",
      "iter: 1156, grad: 0.06484, loss_in_train: 2.15280, loss_in_test: 35.63217\n",
      "iter: 1157, grad: 0.06472, loss_in_train: 2.15280, loss_in_test: 35.63220\n",
      "iter: 1158, grad: 0.06459, loss_in_train: 2.15280, loss_in_test: 35.63224\n",
      "iter: 1159, grad: 0.06447, loss_in_train: 2.15280, loss_in_test: 35.63227\n",
      "iter: 1160, grad: 0.06435, loss_in_train: 2.15280, loss_in_test: 35.63231\n",
      "iter: 1161, grad: 0.06423, loss_in_train: 2.15280, loss_in_test: 35.63234\n",
      "iter: 1162, grad: 0.06411, loss_in_train: 2.15280, loss_in_test: 35.63238\n",
      "iter: 1163, grad: 0.06399, loss_in_train: 2.15280, loss_in_test: 35.63242\n",
      "iter: 1164, grad: 0.06386, loss_in_train: 2.15280, loss_in_test: 35.63245\n",
      "iter: 1165, grad: 0.06374, loss_in_train: 2.15280, loss_in_test: 35.63249\n",
      "iter: 1166, grad: 0.06362, loss_in_train: 2.15280, loss_in_test: 35.63252\n",
      "iter: 1167, grad: 0.06350, loss_in_train: 2.15280, loss_in_test: 35.63256\n",
      "iter: 1168, grad: 0.06338, loss_in_train: 2.15280, loss_in_test: 35.63260\n",
      "iter: 1169, grad: 0.06327, loss_in_train: 2.15280, loss_in_test: 35.63263\n",
      "iter: 1170, grad: 0.06315, loss_in_train: 2.15280, loss_in_test: 35.63267\n",
      "iter: 1171, grad: 0.06303, loss_in_train: 2.15280, loss_in_test: 35.63270\n",
      "iter: 1172, grad: 0.06291, loss_in_train: 2.15280, loss_in_test: 35.63274\n",
      "iter: 1173, grad: 0.06279, loss_in_train: 2.15280, loss_in_test: 35.63278\n",
      "iter: 1174, grad: 0.06267, loss_in_train: 2.15280, loss_in_test: 35.63281\n",
      "iter: 1175, grad: 0.06256, loss_in_train: 2.15280, loss_in_test: 35.63285\n",
      "iter: 1176, grad: 0.06244, loss_in_train: 2.15280, loss_in_test: 35.63288\n",
      "iter: 1177, grad: 0.06232, loss_in_train: 2.15280, loss_in_test: 35.63292\n",
      "iter: 1178, grad: 0.06221, loss_in_train: 2.15280, loss_in_test: 35.63296\n",
      "iter: 1179, grad: 0.06209, loss_in_train: 2.15280, loss_in_test: 35.63299\n",
      "iter: 1180, grad: 0.06197, loss_in_train: 2.15280, loss_in_test: 35.63303\n",
      "iter: 1181, grad: 0.06186, loss_in_train: 2.15280, loss_in_test: 35.63307\n",
      "iter: 1182, grad: 0.06174, loss_in_train: 2.15280, loss_in_test: 35.63310\n",
      "iter: 1183, grad: 0.06163, loss_in_train: 2.15280, loss_in_test: 35.63314\n",
      "iter: 1184, grad: 0.06151, loss_in_train: 2.15280, loss_in_test: 35.63318\n",
      "iter: 1185, grad: 0.06140, loss_in_train: 2.15280, loss_in_test: 35.63321\n",
      "iter: 1186, grad: 0.06128, loss_in_train: 2.15280, loss_in_test: 35.63325\n",
      "iter: 1187, grad: 0.06117, loss_in_train: 2.15280, loss_in_test: 35.63329\n",
      "iter: 1188, grad: 0.06106, loss_in_train: 2.15280, loss_in_test: 35.63332\n",
      "iter: 1189, grad: 0.06094, loss_in_train: 2.15280, loss_in_test: 35.63336\n",
      "iter: 1190, grad: 0.06083, loss_in_train: 2.15280, loss_in_test: 35.63340\n",
      "iter: 1191, grad: 0.06072, loss_in_train: 2.15280, loss_in_test: 35.63343\n",
      "iter: 1192, grad: 0.06060, loss_in_train: 2.15280, loss_in_test: 35.63347\n",
      "iter: 1193, grad: 0.06049, loss_in_train: 2.15280, loss_in_test: 35.63351\n",
      "iter: 1194, grad: 0.06038, loss_in_train: 2.15280, loss_in_test: 35.63354\n",
      "iter: 1195, grad: 0.06027, loss_in_train: 2.15280, loss_in_test: 35.63358\n",
      "iter: 1196, grad: 0.06016, loss_in_train: 2.15280, loss_in_test: 35.63362\n",
      "iter: 1197, grad: 0.06005, loss_in_train: 2.15280, loss_in_test: 35.63365\n",
      "iter: 1198, grad: 0.05994, loss_in_train: 2.15280, loss_in_test: 35.63369\n",
      "iter: 1199, grad: 0.05982, loss_in_train: 2.15280, loss_in_test: 35.63373\n",
      "iter: 1200, grad: 0.05971, loss_in_train: 2.15280, loss_in_test: 35.63376\n",
      "iter: 1201, grad: 0.05960, loss_in_train: 2.15280, loss_in_test: 35.63380\n",
      "iter: 1202, grad: 0.05949, loss_in_train: 2.15280, loss_in_test: 35.63384\n",
      "iter: 1203, grad: 0.05939, loss_in_train: 2.15280, loss_in_test: 35.63388\n",
      "iter: 1204, grad: 0.05928, loss_in_train: 2.15280, loss_in_test: 35.63391\n",
      "iter: 1205, grad: 0.05917, loss_in_train: 2.15280, loss_in_test: 35.63395\n",
      "iter: 1206, grad: 0.05906, loss_in_train: 2.15280, loss_in_test: 35.63399\n",
      "iter: 1207, grad: 0.05895, loss_in_train: 2.15280, loss_in_test: 35.63402\n",
      "iter: 1208, grad: 0.05884, loss_in_train: 2.15280, loss_in_test: 35.63406\n",
      "iter: 1209, grad: 0.05873, loss_in_train: 2.15280, loss_in_test: 35.63410\n",
      "iter: 1210, grad: 0.05863, loss_in_train: 2.15280, loss_in_test: 35.63413\n",
      "iter: 1211, grad: 0.05852, loss_in_train: 2.15280, loss_in_test: 35.63417\n",
      "iter: 1212, grad: 0.05841, loss_in_train: 2.15280, loss_in_test: 35.63421\n",
      "iter: 1213, grad: 0.05831, loss_in_train: 2.15280, loss_in_test: 35.63425\n",
      "iter: 1214, grad: 0.05820, loss_in_train: 2.15280, loss_in_test: 35.63428\n",
      "iter: 1215, grad: 0.05809, loss_in_train: 2.15280, loss_in_test: 35.63432\n",
      "iter: 1216, grad: 0.05799, loss_in_train: 2.15280, loss_in_test: 35.63436\n",
      "iter: 1217, grad: 0.05788, loss_in_train: 2.15280, loss_in_test: 35.63439\n",
      "iter: 1218, grad: 0.05778, loss_in_train: 2.15280, loss_in_test: 35.63443\n",
      "iter: 1219, grad: 0.05767, loss_in_train: 2.15280, loss_in_test: 35.63447\n",
      "iter: 1220, grad: 0.05757, loss_in_train: 2.15280, loss_in_test: 35.63451\n",
      "iter: 1221, grad: 0.05746, loss_in_train: 2.15280, loss_in_test: 35.63454\n",
      "iter: 1222, grad: 0.05736, loss_in_train: 2.15280, loss_in_test: 35.63458\n",
      "iter: 1223, grad: 0.05725, loss_in_train: 2.15280, loss_in_test: 35.63462\n",
      "iter: 1224, grad: 0.05715, loss_in_train: 2.15280, loss_in_test: 35.63466\n",
      "iter: 1225, grad: 0.05704, loss_in_train: 2.15280, loss_in_test: 35.63469\n",
      "iter: 1226, grad: 0.05694, loss_in_train: 2.15280, loss_in_test: 35.63473\n",
      "iter: 1227, grad: 0.05684, loss_in_train: 2.15280, loss_in_test: 35.63477\n",
      "iter: 1228, grad: 0.05674, loss_in_train: 2.15280, loss_in_test: 35.63480\n",
      "iter: 1229, grad: 0.05663, loss_in_train: 2.15280, loss_in_test: 35.63484\n",
      "iter: 1230, grad: 0.05653, loss_in_train: 2.15280, loss_in_test: 35.63488\n",
      "iter: 1231, grad: 0.05643, loss_in_train: 2.15280, loss_in_test: 35.63492\n",
      "iter: 1232, grad: 0.05633, loss_in_train: 2.15280, loss_in_test: 35.63495\n",
      "iter: 1233, grad: 0.05622, loss_in_train: 2.15280, loss_in_test: 35.63499\n",
      "iter: 1234, grad: 0.05612, loss_in_train: 2.15280, loss_in_test: 35.63503\n",
      "iter: 1235, grad: 0.05602, loss_in_train: 2.15280, loss_in_test: 35.63507\n",
      "iter: 1236, grad: 0.05592, loss_in_train: 2.15280, loss_in_test: 35.63510\n",
      "iter: 1237, grad: 0.05582, loss_in_train: 2.15280, loss_in_test: 35.63514\n",
      "iter: 1238, grad: 0.05572, loss_in_train: 2.15280, loss_in_test: 35.63518\n",
      "iter: 1239, grad: 0.05562, loss_in_train: 2.15280, loss_in_test: 35.63521\n",
      "iter: 1240, grad: 0.05552, loss_in_train: 2.15280, loss_in_test: 35.63525\n",
      "iter: 1241, grad: 0.05542, loss_in_train: 2.15280, loss_in_test: 35.63529\n",
      "iter: 1242, grad: 0.05532, loss_in_train: 2.15280, loss_in_test: 35.63533\n",
      "iter: 1243, grad: 0.05522, loss_in_train: 2.15280, loss_in_test: 35.63536\n",
      "iter: 1244, grad: 0.05512, loss_in_train: 2.15280, loss_in_test: 35.63540\n",
      "iter: 1245, grad: 0.05503, loss_in_train: 2.15280, loss_in_test: 35.63544\n",
      "iter: 1246, grad: 0.05493, loss_in_train: 2.15280, loss_in_test: 35.63548\n",
      "iter: 1247, grad: 0.05483, loss_in_train: 2.15280, loss_in_test: 35.63551\n",
      "iter: 1248, grad: 0.05473, loss_in_train: 2.15280, loss_in_test: 35.63555\n",
      "iter: 1249, grad: 0.05463, loss_in_train: 2.15280, loss_in_test: 35.63559\n",
      "iter: 1250, grad: 0.05454, loss_in_train: 2.15280, loss_in_test: 35.63562\n",
      "iter: 1251, grad: 0.05444, loss_in_train: 2.15280, loss_in_test: 35.63566\n",
      "iter: 1252, grad: 0.05434, loss_in_train: 2.15280, loss_in_test: 35.63570\n",
      "iter: 1253, grad: 0.05425, loss_in_train: 2.15280, loss_in_test: 35.63574\n",
      "iter: 1254, grad: 0.05415, loss_in_train: 2.15280, loss_in_test: 35.63577\n",
      "iter: 1255, grad: 0.05405, loss_in_train: 2.15280, loss_in_test: 35.63581\n",
      "iter: 1256, grad: 0.05396, loss_in_train: 2.15280, loss_in_test: 35.63585\n",
      "iter: 1257, grad: 0.05386, loss_in_train: 2.15280, loss_in_test: 35.63588\n",
      "iter: 1258, grad: 0.05377, loss_in_train: 2.15280, loss_in_test: 35.63592\n",
      "iter: 1259, grad: 0.05367, loss_in_train: 2.15280, loss_in_test: 35.63596\n",
      "iter: 1260, grad: 0.05358, loss_in_train: 2.15280, loss_in_test: 35.63600\n",
      "iter: 1261, grad: 0.05348, loss_in_train: 2.15280, loss_in_test: 35.63603\n",
      "iter: 1262, grad: 0.05339, loss_in_train: 2.15280, loss_in_test: 35.63607\n",
      "iter: 1263, grad: 0.05329, loss_in_train: 2.15280, loss_in_test: 35.63611\n",
      "iter: 1264, grad: 0.05320, loss_in_train: 2.15280, loss_in_test: 35.63614\n",
      "iter: 1265, grad: 0.05311, loss_in_train: 2.15280, loss_in_test: 35.63618\n",
      "iter: 1266, grad: 0.05301, loss_in_train: 2.15280, loss_in_test: 35.63622\n",
      "iter: 1267, grad: 0.05292, loss_in_train: 2.15280, loss_in_test: 35.63626\n",
      "iter: 1268, grad: 0.05283, loss_in_train: 2.15280, loss_in_test: 35.63629\n",
      "iter: 1269, grad: 0.05273, loss_in_train: 2.15280, loss_in_test: 35.63633\n",
      "iter: 1270, grad: 0.05264, loss_in_train: 2.15280, loss_in_test: 35.63637\n",
      "iter: 1271, grad: 0.05255, loss_in_train: 2.15280, loss_in_test: 35.63640\n",
      "iter: 1272, grad: 0.05246, loss_in_train: 2.15280, loss_in_test: 35.63644\n",
      "iter: 1273, grad: 0.05236, loss_in_train: 2.15280, loss_in_test: 35.63648\n",
      "iter: 1274, grad: 0.05227, loss_in_train: 2.15280, loss_in_test: 35.63652\n",
      "iter: 1275, grad: 0.05218, loss_in_train: 2.15280, loss_in_test: 35.63655\n",
      "iter: 1276, grad: 0.05209, loss_in_train: 2.15280, loss_in_test: 35.63659\n",
      "iter: 1277, grad: 0.05200, loss_in_train: 2.15280, loss_in_test: 35.63663\n",
      "iter: 1278, grad: 0.05191, loss_in_train: 2.15280, loss_in_test: 35.63666\n",
      "iter: 1279, grad: 0.05182, loss_in_train: 2.15280, loss_in_test: 35.63670\n",
      "iter: 1280, grad: 0.05173, loss_in_train: 2.15280, loss_in_test: 35.63674\n",
      "iter: 1281, grad: 0.05164, loss_in_train: 2.15280, loss_in_test: 35.63677\n",
      "iter: 1282, grad: 0.05155, loss_in_train: 2.15280, loss_in_test: 35.63681\n",
      "iter: 1283, grad: 0.05146, loss_in_train: 2.15280, loss_in_test: 35.63685\n",
      "iter: 1284, grad: 0.05137, loss_in_train: 2.15280, loss_in_test: 35.63688\n",
      "iter: 1285, grad: 0.05128, loss_in_train: 2.15280, loss_in_test: 35.63692\n",
      "iter: 1286, grad: 0.05119, loss_in_train: 2.15280, loss_in_test: 35.63696\n",
      "iter: 1287, grad: 0.05110, loss_in_train: 2.15280, loss_in_test: 35.63699\n",
      "iter: 1288, grad: 0.05101, loss_in_train: 2.15280, loss_in_test: 35.63703\n",
      "iter: 1289, grad: 0.05092, loss_in_train: 2.15280, loss_in_test: 35.63707\n",
      "iter: 1290, grad: 0.05084, loss_in_train: 2.15280, loss_in_test: 35.63710\n",
      "iter: 1291, grad: 0.05075, loss_in_train: 2.15280, loss_in_test: 35.63714\n",
      "iter: 1292, grad: 0.05066, loss_in_train: 2.15280, loss_in_test: 35.63718\n",
      "iter: 1293, grad: 0.05057, loss_in_train: 2.15280, loss_in_test: 35.63721\n",
      "iter: 1294, grad: 0.05049, loss_in_train: 2.15280, loss_in_test: 35.63725\n",
      "iter: 1295, grad: 0.05040, loss_in_train: 2.15280, loss_in_test: 35.63729\n",
      "iter: 1296, grad: 0.05031, loss_in_train: 2.15280, loss_in_test: 35.63732\n",
      "iter: 1297, grad: 0.05023, loss_in_train: 2.15280, loss_in_test: 35.63736\n",
      "iter: 1298, grad: 0.05014, loss_in_train: 2.15280, loss_in_test: 35.63740\n",
      "iter: 1299, grad: 0.05005, loss_in_train: 2.15280, loss_in_test: 35.63743\n",
      "iter: 1300, grad: 0.04997, loss_in_train: 2.15280, loss_in_test: 35.63747\n",
      "iter: 1301, grad: 0.04988, loss_in_train: 2.15280, loss_in_test: 35.63751\n",
      "iter: 1302, grad: 0.04980, loss_in_train: 2.15280, loss_in_test: 35.63754\n",
      "iter: 1303, grad: 0.04971, loss_in_train: 2.15280, loss_in_test: 35.63758\n",
      "iter: 1304, grad: 0.04963, loss_in_train: 2.15280, loss_in_test: 35.63762\n",
      "iter: 1305, grad: 0.04954, loss_in_train: 2.15280, loss_in_test: 35.63765\n",
      "iter: 1306, grad: 0.04946, loss_in_train: 2.15280, loss_in_test: 35.63769\n",
      "iter: 1307, grad: 0.04937, loss_in_train: 2.15280, loss_in_test: 35.63772\n",
      "iter: 1308, grad: 0.04929, loss_in_train: 2.15280, loss_in_test: 35.63776\n",
      "iter: 1309, grad: 0.04920, loss_in_train: 2.15280, loss_in_test: 35.63780\n",
      "iter: 1310, grad: 0.04912, loss_in_train: 2.15280, loss_in_test: 35.63783\n",
      "iter: 1311, grad: 0.04904, loss_in_train: 2.15280, loss_in_test: 35.63787\n",
      "iter: 1312, grad: 0.04895, loss_in_train: 2.15280, loss_in_test: 35.63791\n",
      "iter: 1313, grad: 0.04887, loss_in_train: 2.15280, loss_in_test: 35.63794\n",
      "iter: 1314, grad: 0.04879, loss_in_train: 2.15280, loss_in_test: 35.63798\n",
      "iter: 1315, grad: 0.04871, loss_in_train: 2.15280, loss_in_test: 35.63801\n",
      "iter: 1316, grad: 0.04862, loss_in_train: 2.15280, loss_in_test: 35.63805\n",
      "iter: 1317, grad: 0.04854, loss_in_train: 2.15280, loss_in_test: 35.63809\n",
      "iter: 1318, grad: 0.04846, loss_in_train: 2.15280, loss_in_test: 35.63812\n",
      "iter: 1319, grad: 0.04838, loss_in_train: 2.15280, loss_in_test: 35.63816\n",
      "iter: 1320, grad: 0.04829, loss_in_train: 2.15280, loss_in_test: 35.63819\n",
      "iter: 1321, grad: 0.04821, loss_in_train: 2.15280, loss_in_test: 35.63823\n",
      "iter: 1322, grad: 0.04813, loss_in_train: 2.15280, loss_in_test: 35.63827\n",
      "iter: 1323, grad: 0.04805, loss_in_train: 2.15280, loss_in_test: 35.63830\n",
      "iter: 1324, grad: 0.04797, loss_in_train: 2.15280, loss_in_test: 35.63834\n",
      "iter: 1325, grad: 0.04789, loss_in_train: 2.15280, loss_in_test: 35.63837\n",
      "iter: 1326, grad: 0.04781, loss_in_train: 2.15280, loss_in_test: 35.63841\n",
      "iter: 1327, grad: 0.04773, loss_in_train: 2.15280, loss_in_test: 35.63845\n",
      "iter: 1328, grad: 0.04765, loss_in_train: 2.15280, loss_in_test: 35.63848\n",
      "iter: 1329, grad: 0.04757, loss_in_train: 2.15280, loss_in_test: 35.63852\n",
      "iter: 1330, grad: 0.04749, loss_in_train: 2.15280, loss_in_test: 35.63855\n",
      "iter: 1331, grad: 0.04741, loss_in_train: 2.15280, loss_in_test: 35.63859\n",
      "iter: 1332, grad: 0.04733, loss_in_train: 2.15280, loss_in_test: 35.63862\n",
      "iter: 1333, grad: 0.04725, loss_in_train: 2.15280, loss_in_test: 35.63866\n",
      "iter: 1334, grad: 0.04717, loss_in_train: 2.15280, loss_in_test: 35.63869\n",
      "iter: 1335, grad: 0.04709, loss_in_train: 2.15280, loss_in_test: 35.63873\n",
      "iter: 1336, grad: 0.04702, loss_in_train: 2.15280, loss_in_test: 35.63877\n",
      "iter: 1337, grad: 0.04694, loss_in_train: 2.15280, loss_in_test: 35.63880\n",
      "iter: 1338, grad: 0.04686, loss_in_train: 2.15280, loss_in_test: 35.63884\n",
      "iter: 1339, grad: 0.04678, loss_in_train: 2.15280, loss_in_test: 35.63887\n",
      "iter: 1340, grad: 0.04670, loss_in_train: 2.15280, loss_in_test: 35.63891\n",
      "iter: 1341, grad: 0.04663, loss_in_train: 2.15280, loss_in_test: 35.63894\n",
      "iter: 1342, grad: 0.04655, loss_in_train: 2.15280, loss_in_test: 35.63898\n",
      "iter: 1343, grad: 0.04647, loss_in_train: 2.15280, loss_in_test: 35.63901\n",
      "iter: 1344, grad: 0.04639, loss_in_train: 2.15280, loss_in_test: 35.63905\n",
      "iter: 1345, grad: 0.04632, loss_in_train: 2.15280, loss_in_test: 35.63908\n",
      "iter: 1346, grad: 0.04624, loss_in_train: 2.15280, loss_in_test: 35.63912\n",
      "iter: 1347, grad: 0.04616, loss_in_train: 2.15280, loss_in_test: 35.63915\n",
      "iter: 1348, grad: 0.04609, loss_in_train: 2.15280, loss_in_test: 35.63919\n",
      "iter: 1349, grad: 0.04601, loss_in_train: 2.15280, loss_in_test: 35.63922\n",
      "iter: 1350, grad: 0.04594, loss_in_train: 2.15280, loss_in_test: 35.63926\n",
      "iter: 1351, grad: 0.04586, loss_in_train: 2.15280, loss_in_test: 35.63929\n",
      "iter: 1352, grad: 0.04579, loss_in_train: 2.15280, loss_in_test: 35.63933\n",
      "iter: 1353, grad: 0.04571, loss_in_train: 2.15280, loss_in_test: 35.63936\n",
      "iter: 1354, grad: 0.04564, loss_in_train: 2.15280, loss_in_test: 35.63940\n",
      "iter: 1355, grad: 0.04556, loss_in_train: 2.15280, loss_in_test: 35.63943\n",
      "iter: 1356, grad: 0.04549, loss_in_train: 2.15280, loss_in_test: 35.63947\n",
      "iter: 1357, grad: 0.04541, loss_in_train: 2.15280, loss_in_test: 35.63950\n",
      "iter: 1358, grad: 0.04534, loss_in_train: 2.15280, loss_in_test: 35.63954\n",
      "iter: 1359, grad: 0.04526, loss_in_train: 2.15280, loss_in_test: 35.63957\n",
      "iter: 1360, grad: 0.04519, loss_in_train: 2.15280, loss_in_test: 35.63961\n",
      "iter: 1361, grad: 0.04512, loss_in_train: 2.15280, loss_in_test: 35.63964\n",
      "iter: 1362, grad: 0.04504, loss_in_train: 2.15280, loss_in_test: 35.63968\n",
      "iter: 1363, grad: 0.04497, loss_in_train: 2.15280, loss_in_test: 35.63971\n",
      "iter: 1364, grad: 0.04490, loss_in_train: 2.15280, loss_in_test: 35.63974\n",
      "iter: 1365, grad: 0.04482, loss_in_train: 2.15280, loss_in_test: 35.63978\n",
      "iter: 1366, grad: 0.04475, loss_in_train: 2.15280, loss_in_test: 35.63981\n",
      "iter: 1367, grad: 0.04468, loss_in_train: 2.15280, loss_in_test: 35.63985\n",
      "iter: 1368, grad: 0.04460, loss_in_train: 2.15280, loss_in_test: 35.63988\n",
      "iter: 1369, grad: 0.04453, loss_in_train: 2.15280, loss_in_test: 35.63992\n",
      "iter: 1370, grad: 0.04446, loss_in_train: 2.15280, loss_in_test: 35.63995\n",
      "iter: 1371, grad: 0.04439, loss_in_train: 2.15280, loss_in_test: 35.63999\n",
      "iter: 1372, grad: 0.04432, loss_in_train: 2.15280, loss_in_test: 35.64002\n",
      "iter: 1373, grad: 0.04425, loss_in_train: 2.15280, loss_in_test: 35.64005\n",
      "iter: 1374, grad: 0.04417, loss_in_train: 2.15280, loss_in_test: 35.64009\n",
      "iter: 1375, grad: 0.04410, loss_in_train: 2.15280, loss_in_test: 35.64012\n",
      "iter: 1376, grad: 0.04403, loss_in_train: 2.15280, loss_in_test: 35.64016\n",
      "iter: 1377, grad: 0.04396, loss_in_train: 2.15280, loss_in_test: 35.64019\n",
      "iter: 1378, grad: 0.04389, loss_in_train: 2.15280, loss_in_test: 35.64022\n",
      "iter: 1379, grad: 0.04382, loss_in_train: 2.15280, loss_in_test: 35.64026\n",
      "iter: 1380, grad: 0.04375, loss_in_train: 2.15280, loss_in_test: 35.64029\n",
      "iter: 1381, grad: 0.04368, loss_in_train: 2.15280, loss_in_test: 35.64033\n",
      "iter: 1382, grad: 0.04361, loss_in_train: 2.15280, loss_in_test: 35.64036\n",
      "iter: 1383, grad: 0.04354, loss_in_train: 2.15280, loss_in_test: 35.64039\n",
      "iter: 1384, grad: 0.04347, loss_in_train: 2.15280, loss_in_test: 35.64043\n",
      "iter: 1385, grad: 0.04340, loss_in_train: 2.15280, loss_in_test: 35.64046\n",
      "iter: 1386, grad: 0.04333, loss_in_train: 2.15280, loss_in_test: 35.64049\n",
      "iter: 1387, grad: 0.04326, loss_in_train: 2.15280, loss_in_test: 35.64053\n",
      "iter: 1388, grad: 0.04319, loss_in_train: 2.15280, loss_in_test: 35.64056\n",
      "iter: 1389, grad: 0.04312, loss_in_train: 2.15280, loss_in_test: 35.64059\n",
      "iter: 1390, grad: 0.04306, loss_in_train: 2.15280, loss_in_test: 35.64063\n",
      "iter: 1391, grad: 0.04299, loss_in_train: 2.15280, loss_in_test: 35.64066\n",
      "iter: 1392, grad: 0.04292, loss_in_train: 2.15280, loss_in_test: 35.64070\n",
      "iter: 1393, grad: 0.04285, loss_in_train: 2.15280, loss_in_test: 35.64073\n",
      "iter: 1394, grad: 0.04278, loss_in_train: 2.15280, loss_in_test: 35.64076\n",
      "iter: 1395, grad: 0.04272, loss_in_train: 2.15280, loss_in_test: 35.64079\n",
      "iter: 1396, grad: 0.04265, loss_in_train: 2.15280, loss_in_test: 35.64083\n",
      "iter: 1397, grad: 0.04258, loss_in_train: 2.15280, loss_in_test: 35.64086\n",
      "iter: 1398, grad: 0.04251, loss_in_train: 2.15280, loss_in_test: 35.64089\n",
      "iter: 1399, grad: 0.04245, loss_in_train: 2.15280, loss_in_test: 35.64093\n",
      "iter: 1400, grad: 0.04238, loss_in_train: 2.15280, loss_in_test: 35.64096\n",
      "iter: 1401, grad: 0.04231, loss_in_train: 2.15280, loss_in_test: 35.64099\n",
      "iter: 1402, grad: 0.04225, loss_in_train: 2.15280, loss_in_test: 35.64103\n",
      "iter: 1403, grad: 0.04218, loss_in_train: 2.15280, loss_in_test: 35.64106\n",
      "iter: 1404, grad: 0.04211, loss_in_train: 2.15280, loss_in_test: 35.64109\n",
      "iter: 1405, grad: 0.04205, loss_in_train: 2.15280, loss_in_test: 35.64113\n",
      "iter: 1406, grad: 0.04198, loss_in_train: 2.15280, loss_in_test: 35.64116\n",
      "iter: 1407, grad: 0.04192, loss_in_train: 2.15280, loss_in_test: 35.64119\n",
      "iter: 1408, grad: 0.04185, loss_in_train: 2.15280, loss_in_test: 35.64122\n",
      "iter: 1409, grad: 0.04179, loss_in_train: 2.15280, loss_in_test: 35.64126\n",
      "iter: 1410, grad: 0.04172, loss_in_train: 2.15280, loss_in_test: 35.64129\n",
      "iter: 1411, grad: 0.04166, loss_in_train: 2.15280, loss_in_test: 35.64132\n",
      "iter: 1412, grad: 0.04159, loss_in_train: 2.15280, loss_in_test: 35.64135\n",
      "iter: 1413, grad: 0.04153, loss_in_train: 2.15280, loss_in_test: 35.64139\n",
      "iter: 1414, grad: 0.04146, loss_in_train: 2.15280, loss_in_test: 35.64142\n",
      "iter: 1415, grad: 0.04140, loss_in_train: 2.15280, loss_in_test: 35.64145\n",
      "iter: 1416, grad: 0.04133, loss_in_train: 2.15280, loss_in_test: 35.64148\n",
      "iter: 1417, grad: 0.04127, loss_in_train: 2.15280, loss_in_test: 35.64152\n",
      "iter: 1418, grad: 0.04120, loss_in_train: 2.15280, loss_in_test: 35.64155\n",
      "iter: 1419, grad: 0.04114, loss_in_train: 2.15280, loss_in_test: 35.64158\n",
      "iter: 1420, grad: 0.04108, loss_in_train: 2.15280, loss_in_test: 35.64161\n",
      "iter: 1421, grad: 0.04101, loss_in_train: 2.15280, loss_in_test: 35.64165\n",
      "iter: 1422, grad: 0.04095, loss_in_train: 2.15280, loss_in_test: 35.64168\n",
      "iter: 1423, grad: 0.04089, loss_in_train: 2.15280, loss_in_test: 35.64171\n",
      "iter: 1424, grad: 0.04082, loss_in_train: 2.15280, loss_in_test: 35.64174\n",
      "iter: 1425, grad: 0.04076, loss_in_train: 2.15280, loss_in_test: 35.64177\n",
      "iter: 1426, grad: 0.04070, loss_in_train: 2.15280, loss_in_test: 35.64181\n",
      "iter: 1427, grad: 0.04064, loss_in_train: 2.15280, loss_in_test: 35.64184\n",
      "iter: 1428, grad: 0.04057, loss_in_train: 2.15280, loss_in_test: 35.64187\n",
      "iter: 1429, grad: 0.04051, loss_in_train: 2.15280, loss_in_test: 35.64190\n",
      "iter: 1430, grad: 0.04045, loss_in_train: 2.15280, loss_in_test: 35.64193\n",
      "iter: 1431, grad: 0.04039, loss_in_train: 2.15280, loss_in_test: 35.64196\n",
      "iter: 1432, grad: 0.04033, loss_in_train: 2.15280, loss_in_test: 35.64200\n",
      "iter: 1433, grad: 0.04026, loss_in_train: 2.15280, loss_in_test: 35.64203\n",
      "iter: 1434, grad: 0.04020, loss_in_train: 2.15280, loss_in_test: 35.64206\n",
      "iter: 1435, grad: 0.04014, loss_in_train: 2.15280, loss_in_test: 35.64209\n",
      "iter: 1436, grad: 0.04008, loss_in_train: 2.15280, loss_in_test: 35.64212\n",
      "iter: 1437, grad: 0.04002, loss_in_train: 2.15280, loss_in_test: 35.64215\n",
      "iter: 1438, grad: 0.03996, loss_in_train: 2.15280, loss_in_test: 35.64219\n",
      "iter: 1439, grad: 0.03990, loss_in_train: 2.15280, loss_in_test: 35.64222\n",
      "iter: 1440, grad: 0.03984, loss_in_train: 2.15280, loss_in_test: 35.64225\n",
      "iter: 1441, grad: 0.03978, loss_in_train: 2.15280, loss_in_test: 35.64228\n",
      "iter: 1442, grad: 0.03972, loss_in_train: 2.15280, loss_in_test: 35.64231\n",
      "iter: 1443, grad: 0.03966, loss_in_train: 2.15280, loss_in_test: 35.64234\n",
      "iter: 1444, grad: 0.03960, loss_in_train: 2.15280, loss_in_test: 35.64237\n",
      "iter: 1445, grad: 0.03954, loss_in_train: 2.15280, loss_in_test: 35.64240\n",
      "iter: 1446, grad: 0.03948, loss_in_train: 2.15280, loss_in_test: 35.64244\n",
      "iter: 1447, grad: 0.03942, loss_in_train: 2.15280, loss_in_test: 35.64247\n",
      "iter: 1448, grad: 0.03936, loss_in_train: 2.15280, loss_in_test: 35.64250\n",
      "iter: 1449, grad: 0.03930, loss_in_train: 2.15280, loss_in_test: 35.64253\n",
      "iter: 1450, grad: 0.03924, loss_in_train: 2.15280, loss_in_test: 35.64256\n",
      "iter: 1451, grad: 0.03918, loss_in_train: 2.15280, loss_in_test: 35.64259\n",
      "iter: 1452, grad: 0.03912, loss_in_train: 2.15280, loss_in_test: 35.64262\n",
      "iter: 1453, grad: 0.03907, loss_in_train: 2.15280, loss_in_test: 35.64265\n",
      "iter: 1454, grad: 0.03901, loss_in_train: 2.15280, loss_in_test: 35.64268\n",
      "iter: 1455, grad: 0.03895, loss_in_train: 2.15280, loss_in_test: 35.64271\n",
      "iter: 1456, grad: 0.03889, loss_in_train: 2.15280, loss_in_test: 35.64274\n",
      "iter: 1457, grad: 0.03883, loss_in_train: 2.15280, loss_in_test: 35.64277\n",
      "iter: 1458, grad: 0.03877, loss_in_train: 2.15280, loss_in_test: 35.64280\n",
      "iter: 1459, grad: 0.03872, loss_in_train: 2.15280, loss_in_test: 35.64284\n",
      "iter: 1460, grad: 0.03866, loss_in_train: 2.15280, loss_in_test: 35.64287\n",
      "iter: 1461, grad: 0.03860, loss_in_train: 2.15280, loss_in_test: 35.64290\n",
      "iter: 1462, grad: 0.03854, loss_in_train: 2.15280, loss_in_test: 35.64293\n",
      "iter: 1463, grad: 0.03849, loss_in_train: 2.15280, loss_in_test: 35.64296\n",
      "iter: 1464, grad: 0.03843, loss_in_train: 2.15280, loss_in_test: 35.64299\n",
      "iter: 1465, grad: 0.03837, loss_in_train: 2.15280, loss_in_test: 35.64302\n",
      "iter: 1466, grad: 0.03832, loss_in_train: 2.15280, loss_in_test: 35.64305\n",
      "iter: 1467, grad: 0.03826, loss_in_train: 2.15280, loss_in_test: 35.64308\n",
      "iter: 1468, grad: 0.03820, loss_in_train: 2.15280, loss_in_test: 35.64311\n",
      "iter: 1469, grad: 0.03815, loss_in_train: 2.15280, loss_in_test: 35.64314\n",
      "iter: 1470, grad: 0.03809, loss_in_train: 2.15280, loss_in_test: 35.64317\n",
      "iter: 1471, grad: 0.03804, loss_in_train: 2.15280, loss_in_test: 35.64320\n",
      "iter: 1472, grad: 0.03798, loss_in_train: 2.15280, loss_in_test: 35.64323\n",
      "iter: 1473, grad: 0.03792, loss_in_train: 2.15280, loss_in_test: 35.64326\n",
      "iter: 1474, grad: 0.03787, loss_in_train: 2.15280, loss_in_test: 35.64329\n",
      "iter: 1475, grad: 0.03781, loss_in_train: 2.15280, loss_in_test: 35.64332\n",
      "iter: 1476, grad: 0.03776, loss_in_train: 2.15280, loss_in_test: 35.64335\n",
      "iter: 1477, grad: 0.03770, loss_in_train: 2.15280, loss_in_test: 35.64338\n",
      "iter: 1478, grad: 0.03765, loss_in_train: 2.15280, loss_in_test: 35.64341\n",
      "iter: 1479, grad: 0.03759, loss_in_train: 2.15280, loss_in_test: 35.64344\n",
      "iter: 1480, grad: 0.03754, loss_in_train: 2.15280, loss_in_test: 35.64346\n",
      "iter: 1481, grad: 0.03748, loss_in_train: 2.15280, loss_in_test: 35.64349\n",
      "iter: 1482, grad: 0.03743, loss_in_train: 2.15280, loss_in_test: 35.64352\n",
      "iter: 1483, grad: 0.03737, loss_in_train: 2.15280, loss_in_test: 35.64355\n",
      "iter: 1484, grad: 0.03732, loss_in_train: 2.15280, loss_in_test: 35.64358\n",
      "iter: 1485, grad: 0.03727, loss_in_train: 2.15280, loss_in_test: 35.64361\n",
      "iter: 1486, grad: 0.03721, loss_in_train: 2.15280, loss_in_test: 35.64364\n",
      "iter: 1487, grad: 0.03716, loss_in_train: 2.15280, loss_in_test: 35.64367\n",
      "iter: 1488, grad: 0.03711, loss_in_train: 2.15280, loss_in_test: 35.64370\n",
      "iter: 1489, grad: 0.03705, loss_in_train: 2.15280, loss_in_test: 35.64373\n",
      "iter: 1490, grad: 0.03700, loss_in_train: 2.15280, loss_in_test: 35.64376\n",
      "iter: 1491, grad: 0.03695, loss_in_train: 2.15280, loss_in_test: 35.64379\n",
      "iter: 1492, grad: 0.03689, loss_in_train: 2.15280, loss_in_test: 35.64382\n",
      "iter: 1493, grad: 0.03684, loss_in_train: 2.15280, loss_in_test: 35.64384\n",
      "iter: 1494, grad: 0.03679, loss_in_train: 2.15280, loss_in_test: 35.64387\n",
      "iter: 1495, grad: 0.03673, loss_in_train: 2.15280, loss_in_test: 35.64390\n",
      "iter: 1496, grad: 0.03668, loss_in_train: 2.15280, loss_in_test: 35.64393\n",
      "iter: 1497, grad: 0.03663, loss_in_train: 2.15280, loss_in_test: 35.64396\n",
      "iter: 1498, grad: 0.03658, loss_in_train: 2.15280, loss_in_test: 35.64399\n",
      "iter: 1499, grad: 0.03652, loss_in_train: 2.15280, loss_in_test: 35.64402\n",
      "iter: 1500, grad: 0.03647, loss_in_train: 2.15280, loss_in_test: 35.64405\n",
      "iter: 1501, grad: 0.03642, loss_in_train: 2.15280, loss_in_test: 35.64407\n",
      "iter: 1502, grad: 0.03637, loss_in_train: 2.15280, loss_in_test: 35.64410\n",
      "iter: 1503, grad: 0.03632, loss_in_train: 2.15280, loss_in_test: 35.64413\n",
      "iter: 1504, grad: 0.03626, loss_in_train: 2.15280, loss_in_test: 35.64416\n",
      "iter: 1505, grad: 0.03621, loss_in_train: 2.15280, loss_in_test: 35.64419\n",
      "iter: 1506, grad: 0.03616, loss_in_train: 2.15280, loss_in_test: 35.64422\n",
      "iter: 1507, grad: 0.03611, loss_in_train: 2.15280, loss_in_test: 35.64424\n",
      "iter: 1508, grad: 0.03606, loss_in_train: 2.15280, loss_in_test: 35.64427\n",
      "iter: 1509, grad: 0.03601, loss_in_train: 2.15280, loss_in_test: 35.64430\n",
      "iter: 1510, grad: 0.03596, loss_in_train: 2.15280, loss_in_test: 35.64433\n",
      "iter: 1511, grad: 0.03591, loss_in_train: 2.15280, loss_in_test: 35.64436\n",
      "iter: 1512, grad: 0.03586, loss_in_train: 2.15280, loss_in_test: 35.64438\n",
      "iter: 1513, grad: 0.03581, loss_in_train: 2.15280, loss_in_test: 35.64441\n",
      "iter: 1514, grad: 0.03576, loss_in_train: 2.15280, loss_in_test: 35.64444\n",
      "iter: 1515, grad: 0.03571, loss_in_train: 2.15280, loss_in_test: 35.64447\n",
      "iter: 1516, grad: 0.03566, loss_in_train: 2.15280, loss_in_test: 35.64450\n",
      "iter: 1517, grad: 0.03561, loss_in_train: 2.15280, loss_in_test: 35.64452\n",
      "iter: 1518, grad: 0.03556, loss_in_train: 2.15280, loss_in_test: 35.64455\n",
      "iter: 1519, grad: 0.03551, loss_in_train: 2.15280, loss_in_test: 35.64458\n",
      "iter: 1520, grad: 0.03546, loss_in_train: 2.15280, loss_in_test: 35.64461\n",
      "iter: 1521, grad: 0.03541, loss_in_train: 2.15280, loss_in_test: 35.64463\n",
      "iter: 1522, grad: 0.03536, loss_in_train: 2.15280, loss_in_test: 35.64466\n",
      "iter: 1523, grad: 0.03531, loss_in_train: 2.15280, loss_in_test: 35.64469\n",
      "iter: 1524, grad: 0.03526, loss_in_train: 2.15280, loss_in_test: 35.64472\n",
      "iter: 1525, grad: 0.03521, loss_in_train: 2.15280, loss_in_test: 35.64474\n",
      "iter: 1526, grad: 0.03516, loss_in_train: 2.15280, loss_in_test: 35.64477\n",
      "iter: 1527, grad: 0.03511, loss_in_train: 2.15280, loss_in_test: 35.64480\n",
      "iter: 1528, grad: 0.03507, loss_in_train: 2.15280, loss_in_test: 35.64483\n",
      "iter: 1529, grad: 0.03502, loss_in_train: 2.15280, loss_in_test: 35.64485\n",
      "iter: 1530, grad: 0.03497, loss_in_train: 2.15280, loss_in_test: 35.64488\n",
      "iter: 1531, grad: 0.03492, loss_in_train: 2.15280, loss_in_test: 35.64491\n",
      "iter: 1532, grad: 0.03487, loss_in_train: 2.15280, loss_in_test: 35.64494\n",
      "iter: 1533, grad: 0.03483, loss_in_train: 2.15280, loss_in_test: 35.64496\n",
      "iter: 1534, grad: 0.03478, loss_in_train: 2.15280, loss_in_test: 35.64499\n",
      "iter: 1535, grad: 0.03473, loss_in_train: 2.15280, loss_in_test: 35.64502\n",
      "iter: 1536, grad: 0.03468, loss_in_train: 2.15280, loss_in_test: 35.64504\n",
      "iter: 1537, grad: 0.03463, loss_in_train: 2.15280, loss_in_test: 35.64507\n",
      "iter: 1538, grad: 0.03459, loss_in_train: 2.15280, loss_in_test: 35.64510\n",
      "iter: 1539, grad: 0.03454, loss_in_train: 2.15280, loss_in_test: 35.64512\n",
      "iter: 1540, grad: 0.03449, loss_in_train: 2.15280, loss_in_test: 35.64515\n",
      "iter: 1541, grad: 0.03445, loss_in_train: 2.15280, loss_in_test: 35.64518\n",
      "iter: 1542, grad: 0.03440, loss_in_train: 2.15280, loss_in_test: 35.64520\n",
      "iter: 1543, grad: 0.03435, loss_in_train: 2.15280, loss_in_test: 35.64523\n",
      "iter: 1544, grad: 0.03431, loss_in_train: 2.15280, loss_in_test: 35.64526\n",
      "iter: 1545, grad: 0.03426, loss_in_train: 2.15280, loss_in_test: 35.64528\n",
      "iter: 1546, grad: 0.03421, loss_in_train: 2.15280, loss_in_test: 35.64531\n",
      "iter: 1547, grad: 0.03417, loss_in_train: 2.15280, loss_in_test: 35.64534\n",
      "iter: 1548, grad: 0.03412, loss_in_train: 2.15280, loss_in_test: 35.64536\n",
      "iter: 1549, grad: 0.03407, loss_in_train: 2.15280, loss_in_test: 35.64539\n",
      "iter: 1550, grad: 0.03403, loss_in_train: 2.15280, loss_in_test: 35.64541\n",
      "iter: 1551, grad: 0.03398, loss_in_train: 2.15280, loss_in_test: 35.64544\n",
      "iter: 1552, grad: 0.03394, loss_in_train: 2.15280, loss_in_test: 35.64547\n",
      "iter: 1553, grad: 0.03389, loss_in_train: 2.15280, loss_in_test: 35.64549\n",
      "iter: 1554, grad: 0.03385, loss_in_train: 2.15280, loss_in_test: 35.64552\n",
      "iter: 1555, grad: 0.03380, loss_in_train: 2.15280, loss_in_test: 35.64555\n",
      "iter: 1556, grad: 0.03376, loss_in_train: 2.15280, loss_in_test: 35.64557\n",
      "iter: 1557, grad: 0.03371, loss_in_train: 2.15280, loss_in_test: 35.64560\n",
      "iter: 1558, grad: 0.03367, loss_in_train: 2.15280, loss_in_test: 35.64562\n",
      "iter: 1559, grad: 0.03362, loss_in_train: 2.15280, loss_in_test: 35.64565\n",
      "iter: 1560, grad: 0.03358, loss_in_train: 2.15280, loss_in_test: 35.64567\n",
      "iter: 1561, grad: 0.03353, loss_in_train: 2.15280, loss_in_test: 35.64570\n",
      "iter: 1562, grad: 0.03349, loss_in_train: 2.15280, loss_in_test: 35.64573\n",
      "iter: 1563, grad: 0.03344, loss_in_train: 2.15280, loss_in_test: 35.64575\n",
      "iter: 1564, grad: 0.03340, loss_in_train: 2.15280, loss_in_test: 35.64578\n",
      "iter: 1565, grad: 0.03335, loss_in_train: 2.15280, loss_in_test: 35.64580\n",
      "iter: 1566, grad: 0.03331, loss_in_train: 2.15280, loss_in_test: 35.64583\n",
      "iter: 1567, grad: 0.03326, loss_in_train: 2.15280, loss_in_test: 35.64585\n",
      "iter: 1568, grad: 0.03322, loss_in_train: 2.15280, loss_in_test: 35.64588\n",
      "iter: 1569, grad: 0.03318, loss_in_train: 2.15280, loss_in_test: 35.64591\n",
      "iter: 1570, grad: 0.03313, loss_in_train: 2.15280, loss_in_test: 35.64593\n",
      "iter: 1571, grad: 0.03309, loss_in_train: 2.15280, loss_in_test: 35.64596\n",
      "iter: 1572, grad: 0.03305, loss_in_train: 2.15280, loss_in_test: 35.64598\n",
      "iter: 1573, grad: 0.03300, loss_in_train: 2.15280, loss_in_test: 35.64601\n",
      "iter: 1574, grad: 0.03296, loss_in_train: 2.15280, loss_in_test: 35.64603\n",
      "iter: 1575, grad: 0.03292, loss_in_train: 2.15280, loss_in_test: 35.64606\n",
      "iter: 1576, grad: 0.03287, loss_in_train: 2.15280, loss_in_test: 35.64608\n",
      "iter: 1577, grad: 0.03283, loss_in_train: 2.15280, loss_in_test: 35.64611\n",
      "iter: 1578, grad: 0.03279, loss_in_train: 2.15280, loss_in_test: 35.64613\n",
      "iter: 1579, grad: 0.03275, loss_in_train: 2.15280, loss_in_test: 35.64616\n",
      "iter: 1580, grad: 0.03270, loss_in_train: 2.15280, loss_in_test: 35.64618\n",
      "iter: 1581, grad: 0.03266, loss_in_train: 2.15280, loss_in_test: 35.64621\n",
      "iter: 1582, grad: 0.03262, loss_in_train: 2.15280, loss_in_test: 35.64623\n",
      "iter: 1583, grad: 0.03258, loss_in_train: 2.15280, loss_in_test: 35.64626\n",
      "iter: 1584, grad: 0.03253, loss_in_train: 2.15280, loss_in_test: 35.64628\n",
      "iter: 1585, grad: 0.03249, loss_in_train: 2.15280, loss_in_test: 35.64630\n",
      "iter: 1586, grad: 0.03245, loss_in_train: 2.15280, loss_in_test: 35.64633\n",
      "iter: 1587, grad: 0.03241, loss_in_train: 2.15280, loss_in_test: 35.64635\n",
      "iter: 1588, grad: 0.03237, loss_in_train: 2.15280, loss_in_test: 35.64638\n",
      "iter: 1589, grad: 0.03232, loss_in_train: 2.15280, loss_in_test: 35.64640\n",
      "iter: 1590, grad: 0.03228, loss_in_train: 2.15280, loss_in_test: 35.64643\n",
      "iter: 1591, grad: 0.03224, loss_in_train: 2.15280, loss_in_test: 35.64645\n",
      "iter: 1592, grad: 0.03220, loss_in_train: 2.15280, loss_in_test: 35.64648\n",
      "iter: 1593, grad: 0.03216, loss_in_train: 2.15280, loss_in_test: 35.64650\n",
      "iter: 1594, grad: 0.03212, loss_in_train: 2.15280, loss_in_test: 35.64652\n",
      "iter: 1595, grad: 0.03208, loss_in_train: 2.15280, loss_in_test: 35.64655\n",
      "iter: 1596, grad: 0.03204, loss_in_train: 2.15280, loss_in_test: 35.64657\n",
      "iter: 1597, grad: 0.03199, loss_in_train: 2.15280, loss_in_test: 35.64660\n",
      "iter: 1598, grad: 0.03195, loss_in_train: 2.15280, loss_in_test: 35.64662\n",
      "iter: 1599, grad: 0.03191, loss_in_train: 2.15280, loss_in_test: 35.64665\n",
      "iter: 1600, grad: 0.03187, loss_in_train: 2.15280, loss_in_test: 35.64667\n",
      "iter: 1601, grad: 0.03183, loss_in_train: 2.15280, loss_in_test: 35.64669\n",
      "iter: 1602, grad: 0.03179, loss_in_train: 2.15280, loss_in_test: 35.64672\n",
      "iter: 1603, grad: 0.03175, loss_in_train: 2.15280, loss_in_test: 35.64674\n",
      "iter: 1604, grad: 0.03171, loss_in_train: 2.15280, loss_in_test: 35.64676\n",
      "iter: 1605, grad: 0.03167, loss_in_train: 2.15280, loss_in_test: 35.64679\n",
      "iter: 1606, grad: 0.03163, loss_in_train: 2.15280, loss_in_test: 35.64681\n",
      "iter: 1607, grad: 0.03159, loss_in_train: 2.15280, loss_in_test: 35.64684\n",
      "iter: 1608, grad: 0.03155, loss_in_train: 2.15280, loss_in_test: 35.64686\n",
      "iter: 1609, grad: 0.03151, loss_in_train: 2.15280, loss_in_test: 35.64688\n",
      "iter: 1610, grad: 0.03147, loss_in_train: 2.15280, loss_in_test: 35.64691\n",
      "iter: 1611, grad: 0.03143, loss_in_train: 2.15280, loss_in_test: 35.64693\n",
      "iter: 1612, grad: 0.03139, loss_in_train: 2.15280, loss_in_test: 35.64695\n",
      "iter: 1613, grad: 0.03135, loss_in_train: 2.15280, loss_in_test: 35.64698\n",
      "iter: 1614, grad: 0.03132, loss_in_train: 2.15280, loss_in_test: 35.64700\n",
      "iter: 1615, grad: 0.03128, loss_in_train: 2.15280, loss_in_test: 35.64702\n",
      "iter: 1616, grad: 0.03124, loss_in_train: 2.15280, loss_in_test: 35.64705\n",
      "iter: 1617, grad: 0.03120, loss_in_train: 2.15280, loss_in_test: 35.64707\n",
      "iter: 1618, grad: 0.03116, loss_in_train: 2.15280, loss_in_test: 35.64709\n",
      "iter: 1619, grad: 0.03112, loss_in_train: 2.15280, loss_in_test: 35.64712\n",
      "iter: 1620, grad: 0.03108, loss_in_train: 2.15280, loss_in_test: 35.64714\n",
      "iter: 1621, grad: 0.03104, loss_in_train: 2.15280, loss_in_test: 35.64716\n",
      "iter: 1622, grad: 0.03101, loss_in_train: 2.15280, loss_in_test: 35.64719\n",
      "iter: 1623, grad: 0.03097, loss_in_train: 2.15280, loss_in_test: 35.64721\n",
      "iter: 1624, grad: 0.03093, loss_in_train: 2.15280, loss_in_test: 35.64723\n",
      "iter: 1625, grad: 0.03089, loss_in_train: 2.15280, loss_in_test: 35.64725\n",
      "iter: 1626, grad: 0.03085, loss_in_train: 2.15280, loss_in_test: 35.64728\n",
      "iter: 1627, grad: 0.03082, loss_in_train: 2.15280, loss_in_test: 35.64730\n",
      "iter: 1628, grad: 0.03078, loss_in_train: 2.15280, loss_in_test: 35.64732\n",
      "iter: 1629, grad: 0.03074, loss_in_train: 2.15280, loss_in_test: 35.64734\n",
      "iter: 1630, grad: 0.03070, loss_in_train: 2.15280, loss_in_test: 35.64737\n",
      "iter: 1631, grad: 0.03066, loss_in_train: 2.15280, loss_in_test: 35.64739\n",
      "iter: 1632, grad: 0.03063, loss_in_train: 2.15280, loss_in_test: 35.64741\n",
      "iter: 1633, grad: 0.03059, loss_in_train: 2.15280, loss_in_test: 35.64744\n",
      "iter: 1634, grad: 0.03055, loss_in_train: 2.15280, loss_in_test: 35.64746\n",
      "iter: 1635, grad: 0.03052, loss_in_train: 2.15280, loss_in_test: 35.64748\n",
      "iter: 1636, grad: 0.03048, loss_in_train: 2.15280, loss_in_test: 35.64750\n",
      "iter: 1637, grad: 0.03044, loss_in_train: 2.15280, loss_in_test: 35.64752\n",
      "iter: 1638, grad: 0.03041, loss_in_train: 2.15280, loss_in_test: 35.64755\n",
      "iter: 1639, grad: 0.03037, loss_in_train: 2.15280, loss_in_test: 35.64757\n",
      "iter: 1640, grad: 0.03033, loss_in_train: 2.15280, loss_in_test: 35.64759\n",
      "iter: 1641, grad: 0.03030, loss_in_train: 2.15280, loss_in_test: 35.64761\n",
      "iter: 1642, grad: 0.03026, loss_in_train: 2.15280, loss_in_test: 35.64764\n",
      "iter: 1643, grad: 0.03022, loss_in_train: 2.15280, loss_in_test: 35.64766\n",
      "iter: 1644, grad: 0.03019, loss_in_train: 2.15280, loss_in_test: 35.64768\n",
      "iter: 1645, grad: 0.03015, loss_in_train: 2.15280, loss_in_test: 35.64770\n",
      "iter: 1646, grad: 0.03011, loss_in_train: 2.15280, loss_in_test: 35.64772\n",
      "iter: 1647, grad: 0.03008, loss_in_train: 2.15280, loss_in_test: 35.64775\n",
      "iter: 1648, grad: 0.03004, loss_in_train: 2.15280, loss_in_test: 35.64777\n",
      "iter: 1649, grad: 0.03001, loss_in_train: 2.15280, loss_in_test: 35.64779\n",
      "iter: 1650, grad: 0.02997, loss_in_train: 2.15280, loss_in_test: 35.64781\n",
      "iter: 1651, grad: 0.02993, loss_in_train: 2.15280, loss_in_test: 35.64783\n",
      "iter: 1652, grad: 0.02990, loss_in_train: 2.15280, loss_in_test: 35.64785\n",
      "iter: 1653, grad: 0.02986, loss_in_train: 2.15280, loss_in_test: 35.64788\n",
      "iter: 1654, grad: 0.02983, loss_in_train: 2.15280, loss_in_test: 35.64790\n",
      "iter: 1655, grad: 0.02979, loss_in_train: 2.15280, loss_in_test: 35.64792\n",
      "iter: 1656, grad: 0.02976, loss_in_train: 2.15280, loss_in_test: 35.64794\n",
      "iter: 1657, grad: 0.02972, loss_in_train: 2.15280, loss_in_test: 35.64796\n",
      "iter: 1658, grad: 0.02969, loss_in_train: 2.15280, loss_in_test: 35.64798\n",
      "iter: 1659, grad: 0.02965, loss_in_train: 2.15280, loss_in_test: 35.64801\n",
      "iter: 1660, grad: 0.02962, loss_in_train: 2.15280, loss_in_test: 35.64803\n",
      "iter: 1661, grad: 0.02958, loss_in_train: 2.15280, loss_in_test: 35.64805\n",
      "iter: 1662, grad: 0.02955, loss_in_train: 2.15280, loss_in_test: 35.64807\n",
      "iter: 1663, grad: 0.02951, loss_in_train: 2.15280, loss_in_test: 35.64809\n",
      "iter: 1664, grad: 0.02948, loss_in_train: 2.15280, loss_in_test: 35.64811\n",
      "iter: 1665, grad: 0.02944, loss_in_train: 2.15280, loss_in_test: 35.64813\n",
      "iter: 1666, grad: 0.02941, loss_in_train: 2.15280, loss_in_test: 35.64815\n",
      "iter: 1667, grad: 0.02938, loss_in_train: 2.15280, loss_in_test: 35.64817\n",
      "iter: 1668, grad: 0.02934, loss_in_train: 2.15280, loss_in_test: 35.64820\n",
      "iter: 1669, grad: 0.02931, loss_in_train: 2.15280, loss_in_test: 35.64822\n",
      "iter: 1670, grad: 0.02927, loss_in_train: 2.15280, loss_in_test: 35.64824\n",
      "iter: 1671, grad: 0.02924, loss_in_train: 2.15280, loss_in_test: 35.64826\n",
      "iter: 1672, grad: 0.02921, loss_in_train: 2.15280, loss_in_test: 35.64828\n",
      "iter: 1673, grad: 0.02917, loss_in_train: 2.15280, loss_in_test: 35.64830\n",
      "iter: 1674, grad: 0.02914, loss_in_train: 2.15280, loss_in_test: 35.64832\n",
      "iter: 1675, grad: 0.02910, loss_in_train: 2.15280, loss_in_test: 35.64834\n",
      "iter: 1676, grad: 0.02907, loss_in_train: 2.15280, loss_in_test: 35.64836\n",
      "iter: 1677, grad: 0.02904, loss_in_train: 2.15280, loss_in_test: 35.64838\n",
      "iter: 1678, grad: 0.02900, loss_in_train: 2.15280, loss_in_test: 35.64840\n",
      "iter: 1679, grad: 0.02897, loss_in_train: 2.15280, loss_in_test: 35.64842\n",
      "iter: 1680, grad: 0.02894, loss_in_train: 2.15280, loss_in_test: 35.64844\n",
      "iter: 1681, grad: 0.02890, loss_in_train: 2.15280, loss_in_test: 35.64847\n",
      "iter: 1682, grad: 0.02887, loss_in_train: 2.15280, loss_in_test: 35.64849\n",
      "iter: 1683, grad: 0.02884, loss_in_train: 2.15280, loss_in_test: 35.64851\n",
      "iter: 1684, grad: 0.02881, loss_in_train: 2.15280, loss_in_test: 35.64853\n",
      "iter: 1685, grad: 0.02877, loss_in_train: 2.15280, loss_in_test: 35.64855\n",
      "iter: 1686, grad: 0.02874, loss_in_train: 2.15280, loss_in_test: 35.64857\n",
      "iter: 1687, grad: 0.02871, loss_in_train: 2.15280, loss_in_test: 35.64859\n",
      "iter: 1688, grad: 0.02868, loss_in_train: 2.15280, loss_in_test: 35.64861\n",
      "iter: 1689, grad: 0.02864, loss_in_train: 2.15280, loss_in_test: 35.64863\n",
      "iter: 1690, grad: 0.02861, loss_in_train: 2.15280, loss_in_test: 35.64865\n",
      "iter: 1691, grad: 0.02858, loss_in_train: 2.15280, loss_in_test: 35.64867\n",
      "iter: 1692, grad: 0.02855, loss_in_train: 2.15280, loss_in_test: 35.64869\n",
      "iter: 1693, grad: 0.02851, loss_in_train: 2.15280, loss_in_test: 35.64871\n",
      "iter: 1694, grad: 0.02848, loss_in_train: 2.15280, loss_in_test: 35.64873\n",
      "iter: 1695, grad: 0.02845, loss_in_train: 2.15280, loss_in_test: 35.64875\n",
      "iter: 1696, grad: 0.02842, loss_in_train: 2.15280, loss_in_test: 35.64877\n",
      "iter: 1697, grad: 0.02839, loss_in_train: 2.15280, loss_in_test: 35.64879\n",
      "iter: 1698, grad: 0.02835, loss_in_train: 2.15280, loss_in_test: 35.64881\n",
      "iter: 1699, grad: 0.02832, loss_in_train: 2.15280, loss_in_test: 35.64883\n",
      "iter: 1700, grad: 0.02829, loss_in_train: 2.15280, loss_in_test: 35.64885\n",
      "iter: 1701, grad: 0.02826, loss_in_train: 2.15280, loss_in_test: 35.64887\n",
      "iter: 1702, grad: 0.02823, loss_in_train: 2.15280, loss_in_test: 35.64889\n",
      "iter: 1703, grad: 0.02820, loss_in_train: 2.15280, loss_in_test: 35.64891\n",
      "iter: 1704, grad: 0.02817, loss_in_train: 2.15280, loss_in_test: 35.64893\n",
      "iter: 1705, grad: 0.02813, loss_in_train: 2.15280, loss_in_test: 35.64894\n",
      "iter: 1706, grad: 0.02810, loss_in_train: 2.15280, loss_in_test: 35.64896\n",
      "iter: 1707, grad: 0.02807, loss_in_train: 2.15280, loss_in_test: 35.64898\n",
      "iter: 1708, grad: 0.02804, loss_in_train: 2.15280, loss_in_test: 35.64900\n",
      "iter: 1709, grad: 0.02801, loss_in_train: 2.15280, loss_in_test: 35.64902\n",
      "iter: 1710, grad: 0.02798, loss_in_train: 2.15280, loss_in_test: 35.64904\n",
      "iter: 1711, grad: 0.02795, loss_in_train: 2.15280, loss_in_test: 35.64906\n",
      "iter: 1712, grad: 0.02792, loss_in_train: 2.15280, loss_in_test: 35.64908\n",
      "iter: 1713, grad: 0.02789, loss_in_train: 2.15280, loss_in_test: 35.64910\n",
      "iter: 1714, grad: 0.02786, loss_in_train: 2.15280, loss_in_test: 35.64912\n",
      "iter: 1715, grad: 0.02783, loss_in_train: 2.15280, loss_in_test: 35.64914\n",
      "iter: 1716, grad: 0.02780, loss_in_train: 2.15280, loss_in_test: 35.64916\n",
      "iter: 1717, grad: 0.02777, loss_in_train: 2.15280, loss_in_test: 35.64918\n",
      "iter: 1718, grad: 0.02774, loss_in_train: 2.15280, loss_in_test: 35.64919\n",
      "iter: 1719, grad: 0.02771, loss_in_train: 2.15280, loss_in_test: 35.64921\n",
      "iter: 1720, grad: 0.02768, loss_in_train: 2.15280, loss_in_test: 35.64923\n",
      "iter: 1721, grad: 0.02765, loss_in_train: 2.15280, loss_in_test: 35.64925\n",
      "iter: 1722, grad: 0.02762, loss_in_train: 2.15280, loss_in_test: 35.64927\n",
      "iter: 1723, grad: 0.02759, loss_in_train: 2.15280, loss_in_test: 35.64929\n",
      "iter: 1724, grad: 0.02756, loss_in_train: 2.15280, loss_in_test: 35.64931\n",
      "iter: 1725, grad: 0.02753, loss_in_train: 2.15280, loss_in_test: 35.64933\n",
      "iter: 1726, grad: 0.02750, loss_in_train: 2.15280, loss_in_test: 35.64934\n",
      "iter: 1727, grad: 0.02747, loss_in_train: 2.15280, loss_in_test: 35.64936\n",
      "iter: 1728, grad: 0.02744, loss_in_train: 2.15280, loss_in_test: 35.64938\n",
      "iter: 1729, grad: 0.02741, loss_in_train: 2.15280, loss_in_test: 35.64940\n",
      "iter: 1730, grad: 0.02738, loss_in_train: 2.15280, loss_in_test: 35.64942\n",
      "iter: 1731, grad: 0.02735, loss_in_train: 2.15280, loss_in_test: 35.64944\n",
      "iter: 1732, grad: 0.02732, loss_in_train: 2.15280, loss_in_test: 35.64946\n",
      "iter: 1733, grad: 0.02729, loss_in_train: 2.15280, loss_in_test: 35.64947\n",
      "iter: 1734, grad: 0.02726, loss_in_train: 2.15280, loss_in_test: 35.64949\n",
      "iter: 1735, grad: 0.02724, loss_in_train: 2.15280, loss_in_test: 35.64951\n",
      "iter: 1736, grad: 0.02721, loss_in_train: 2.15280, loss_in_test: 35.64953\n",
      "iter: 1737, grad: 0.02718, loss_in_train: 2.15280, loss_in_test: 35.64955\n",
      "iter: 1738, grad: 0.02715, loss_in_train: 2.15280, loss_in_test: 35.64957\n",
      "iter: 1739, grad: 0.02712, loss_in_train: 2.15280, loss_in_test: 35.64958\n",
      "iter: 1740, grad: 0.02709, loss_in_train: 2.15280, loss_in_test: 35.64960\n",
      "iter: 1741, grad: 0.02706, loss_in_train: 2.15280, loss_in_test: 35.64962\n",
      "iter: 1742, grad: 0.02703, loss_in_train: 2.15280, loss_in_test: 35.64964\n",
      "iter: 1743, grad: 0.02701, loss_in_train: 2.15280, loss_in_test: 35.64966\n",
      "iter: 1744, grad: 0.02698, loss_in_train: 2.15280, loss_in_test: 35.64967\n",
      "iter: 1745, grad: 0.02695, loss_in_train: 2.15280, loss_in_test: 35.64969\n",
      "iter: 1746, grad: 0.02692, loss_in_train: 2.15280, loss_in_test: 35.64971\n",
      "iter: 1747, grad: 0.02689, loss_in_train: 2.15280, loss_in_test: 35.64973\n",
      "iter: 1748, grad: 0.02687, loss_in_train: 2.15280, loss_in_test: 35.64975\n",
      "iter: 1749, grad: 0.02684, loss_in_train: 2.15280, loss_in_test: 35.64976\n",
      "iter: 1750, grad: 0.02681, loss_in_train: 2.15280, loss_in_test: 35.64978\n",
      "iter: 1751, grad: 0.02678, loss_in_train: 2.15280, loss_in_test: 35.64980\n",
      "iter: 1752, grad: 0.02675, loss_in_train: 2.15280, loss_in_test: 35.64982\n",
      "iter: 1753, grad: 0.02673, loss_in_train: 2.15280, loss_in_test: 35.64983\n",
      "iter: 1754, grad: 0.02670, loss_in_train: 2.15280, loss_in_test: 35.64985\n",
      "iter: 1755, grad: 0.02667, loss_in_train: 2.15280, loss_in_test: 35.64987\n",
      "iter: 1756, grad: 0.02664, loss_in_train: 2.15280, loss_in_test: 35.64989\n",
      "iter: 1757, grad: 0.02662, loss_in_train: 2.15280, loss_in_test: 35.64991\n",
      "iter: 1758, grad: 0.02659, loss_in_train: 2.15280, loss_in_test: 35.64992\n",
      "iter: 1759, grad: 0.02656, loss_in_train: 2.15280, loss_in_test: 35.64994\n",
      "iter: 1760, grad: 0.02654, loss_in_train: 2.15280, loss_in_test: 35.64996\n",
      "iter: 1761, grad: 0.02651, loss_in_train: 2.15280, loss_in_test: 35.64997\n",
      "iter: 1762, grad: 0.02648, loss_in_train: 2.15280, loss_in_test: 35.64999\n",
      "iter: 1763, grad: 0.02645, loss_in_train: 2.15280, loss_in_test: 35.65001\n",
      "iter: 1764, grad: 0.02643, loss_in_train: 2.15280, loss_in_test: 35.65003\n",
      "iter: 1765, grad: 0.02640, loss_in_train: 2.15280, loss_in_test: 35.65004\n",
      "iter: 1766, grad: 0.02637, loss_in_train: 2.15280, loss_in_test: 35.65006\n",
      "iter: 1767, grad: 0.02635, loss_in_train: 2.15280, loss_in_test: 35.65008\n",
      "iter: 1768, grad: 0.02632, loss_in_train: 2.15280, loss_in_test: 35.65010\n",
      "iter: 1769, grad: 0.02629, loss_in_train: 2.15280, loss_in_test: 35.65011\n",
      "iter: 1770, grad: 0.02627, loss_in_train: 2.15280, loss_in_test: 35.65013\n",
      "iter: 1771, grad: 0.02624, loss_in_train: 2.15280, loss_in_test: 35.65015\n",
      "iter: 1772, grad: 0.02621, loss_in_train: 2.15280, loss_in_test: 35.65016\n",
      "iter: 1773, grad: 0.02619, loss_in_train: 2.15280, loss_in_test: 35.65018\n",
      "iter: 1774, grad: 0.02616, loss_in_train: 2.15280, loss_in_test: 35.65020\n",
      "iter: 1775, grad: 0.02614, loss_in_train: 2.15280, loss_in_test: 35.65021\n",
      "iter: 1776, grad: 0.02611, loss_in_train: 2.15280, loss_in_test: 35.65023\n",
      "iter: 1777, grad: 0.02608, loss_in_train: 2.15280, loss_in_test: 35.65025\n",
      "iter: 1778, grad: 0.02606, loss_in_train: 2.15280, loss_in_test: 35.65026\n",
      "iter: 1779, grad: 0.02603, loss_in_train: 2.15280, loss_in_test: 35.65028\n",
      "iter: 1780, grad: 0.02601, loss_in_train: 2.15280, loss_in_test: 35.65030\n",
      "iter: 1781, grad: 0.02598, loss_in_train: 2.15280, loss_in_test: 35.65031\n",
      "iter: 1782, grad: 0.02596, loss_in_train: 2.15280, loss_in_test: 35.65033\n",
      "iter: 1783, grad: 0.02593, loss_in_train: 2.15280, loss_in_test: 35.65035\n",
      "iter: 1784, grad: 0.02590, loss_in_train: 2.15280, loss_in_test: 35.65036\n",
      "iter: 1785, grad: 0.02588, loss_in_train: 2.15280, loss_in_test: 35.65038\n",
      "iter: 1786, grad: 0.02585, loss_in_train: 2.15280, loss_in_test: 35.65040\n",
      "iter: 1787, grad: 0.02583, loss_in_train: 2.15280, loss_in_test: 35.65041\n",
      "iter: 1788, grad: 0.02580, loss_in_train: 2.15280, loss_in_test: 35.65043\n",
      "iter: 1789, grad: 0.02578, loss_in_train: 2.15280, loss_in_test: 35.65045\n",
      "iter: 1790, grad: 0.02575, loss_in_train: 2.15280, loss_in_test: 35.65046\n",
      "iter: 1791, grad: 0.02573, loss_in_train: 2.15280, loss_in_test: 35.65048\n",
      "iter: 1792, grad: 0.02570, loss_in_train: 2.15280, loss_in_test: 35.65049\n",
      "iter: 1793, grad: 0.02568, loss_in_train: 2.15280, loss_in_test: 35.65051\n",
      "iter: 1794, grad: 0.02565, loss_in_train: 2.15280, loss_in_test: 35.65053\n",
      "iter: 1795, grad: 0.02563, loss_in_train: 2.15280, loss_in_test: 35.65054\n",
      "iter: 1796, grad: 0.02560, loss_in_train: 2.15280, loss_in_test: 35.65056\n",
      "iter: 1797, grad: 0.02558, loss_in_train: 2.15280, loss_in_test: 35.65058\n",
      "iter: 1798, grad: 0.02555, loss_in_train: 2.15280, loss_in_test: 35.65059\n",
      "iter: 1799, grad: 0.02553, loss_in_train: 2.15280, loss_in_test: 35.65061\n",
      "iter: 1800, grad: 0.02550, loss_in_train: 2.15280, loss_in_test: 35.65062\n",
      "iter: 1801, grad: 0.02548, loss_in_train: 2.15280, loss_in_test: 35.65064\n",
      "iter: 1802, grad: 0.02545, loss_in_train: 2.15280, loss_in_test: 35.65066\n",
      "iter: 1803, grad: 0.02543, loss_in_train: 2.15280, loss_in_test: 35.65067\n",
      "iter: 1804, grad: 0.02541, loss_in_train: 2.15280, loss_in_test: 35.65069\n",
      "iter: 1805, grad: 0.02538, loss_in_train: 2.15280, loss_in_test: 35.65070\n",
      "iter: 1806, grad: 0.02536, loss_in_train: 2.15280, loss_in_test: 35.65072\n",
      "iter: 1807, grad: 0.02533, loss_in_train: 2.15280, loss_in_test: 35.65073\n",
      "iter: 1808, grad: 0.02531, loss_in_train: 2.15280, loss_in_test: 35.65075\n",
      "iter: 1809, grad: 0.02528, loss_in_train: 2.15280, loss_in_test: 35.65077\n",
      "iter: 1810, grad: 0.02526, loss_in_train: 2.15280, loss_in_test: 35.65078\n",
      "iter: 1811, grad: 0.02524, loss_in_train: 2.15280, loss_in_test: 35.65080\n",
      "iter: 1812, grad: 0.02521, loss_in_train: 2.15280, loss_in_test: 35.65081\n",
      "iter: 1813, grad: 0.02519, loss_in_train: 2.15280, loss_in_test: 35.65083\n",
      "iter: 1814, grad: 0.02517, loss_in_train: 2.15280, loss_in_test: 35.65084\n",
      "iter: 1815, grad: 0.02514, loss_in_train: 2.15280, loss_in_test: 35.65086\n",
      "iter: 1816, grad: 0.02512, loss_in_train: 2.15280, loss_in_test: 35.65087\n",
      "iter: 1817, grad: 0.02509, loss_in_train: 2.15280, loss_in_test: 35.65089\n",
      "iter: 1818, grad: 0.02507, loss_in_train: 2.15280, loss_in_test: 35.65091\n",
      "iter: 1819, grad: 0.02505, loss_in_train: 2.15280, loss_in_test: 35.65092\n",
      "iter: 1820, grad: 0.02502, loss_in_train: 2.15280, loss_in_test: 35.65094\n",
      "iter: 1821, grad: 0.02500, loss_in_train: 2.15280, loss_in_test: 35.65095\n",
      "iter: 1822, grad: 0.02498, loss_in_train: 2.15280, loss_in_test: 35.65097\n",
      "iter: 1823, grad: 0.02495, loss_in_train: 2.15280, loss_in_test: 35.65098\n",
      "iter: 1824, grad: 0.02493, loss_in_train: 2.15280, loss_in_test: 35.65100\n",
      "iter: 1825, grad: 0.02491, loss_in_train: 2.15280, loss_in_test: 35.65101\n",
      "iter: 1826, grad: 0.02489, loss_in_train: 2.15280, loss_in_test: 35.65103\n",
      "iter: 1827, grad: 0.02486, loss_in_train: 2.15280, loss_in_test: 35.65104\n",
      "iter: 1828, grad: 0.02484, loss_in_train: 2.15280, loss_in_test: 35.65106\n",
      "iter: 1829, grad: 0.02482, loss_in_train: 2.15280, loss_in_test: 35.65107\n",
      "iter: 1830, grad: 0.02479, loss_in_train: 2.15280, loss_in_test: 35.65109\n",
      "iter: 1831, grad: 0.02477, loss_in_train: 2.15280, loss_in_test: 35.65110\n",
      "iter: 1832, grad: 0.02475, loss_in_train: 2.15280, loss_in_test: 35.65112\n",
      "iter: 1833, grad: 0.02473, loss_in_train: 2.15280, loss_in_test: 35.65113\n",
      "iter: 1834, grad: 0.02470, loss_in_train: 2.15280, loss_in_test: 35.65115\n",
      "iter: 1835, grad: 0.02468, loss_in_train: 2.15280, loss_in_test: 35.65116\n",
      "iter: 1836, grad: 0.02466, loss_in_train: 2.15280, loss_in_test: 35.65118\n",
      "iter: 1837, grad: 0.02464, loss_in_train: 2.15280, loss_in_test: 35.65119\n",
      "iter: 1838, grad: 0.02461, loss_in_train: 2.15280, loss_in_test: 35.65121\n",
      "iter: 1839, grad: 0.02459, loss_in_train: 2.15280, loss_in_test: 35.65122\n",
      "iter: 1840, grad: 0.02457, loss_in_train: 2.15280, loss_in_test: 35.65123\n",
      "iter: 1841, grad: 0.02455, loss_in_train: 2.15280, loss_in_test: 35.65125\n",
      "iter: 1842, grad: 0.02453, loss_in_train: 2.15280, loss_in_test: 35.65126\n",
      "iter: 1843, grad: 0.02450, loss_in_train: 2.15280, loss_in_test: 35.65128\n",
      "iter: 1844, grad: 0.02448, loss_in_train: 2.15280, loss_in_test: 35.65129\n",
      "iter: 1845, grad: 0.02446, loss_in_train: 2.15280, loss_in_test: 35.65131\n",
      "iter: 1846, grad: 0.02444, loss_in_train: 2.15280, loss_in_test: 35.65132\n",
      "iter: 1847, grad: 0.02442, loss_in_train: 2.15280, loss_in_test: 35.65134\n",
      "iter: 1848, grad: 0.02439, loss_in_train: 2.15280, loss_in_test: 35.65135\n",
      "iter: 1849, grad: 0.02437, loss_in_train: 2.15280, loss_in_test: 35.65137\n",
      "iter: 1850, grad: 0.02435, loss_in_train: 2.15280, loss_in_test: 35.65138\n",
      "iter: 1851, grad: 0.02433, loss_in_train: 2.15280, loss_in_test: 35.65139\n",
      "iter: 1852, grad: 0.02431, loss_in_train: 2.15280, loss_in_test: 35.65141\n",
      "iter: 1853, grad: 0.02429, loss_in_train: 2.15280, loss_in_test: 35.65142\n",
      "iter: 1854, grad: 0.02426, loss_in_train: 2.15280, loss_in_test: 35.65144\n",
      "iter: 1855, grad: 0.02424, loss_in_train: 2.15280, loss_in_test: 35.65145\n",
      "iter: 1856, grad: 0.02422, loss_in_train: 2.15280, loss_in_test: 35.65146\n",
      "iter: 1857, grad: 0.02420, loss_in_train: 2.15280, loss_in_test: 35.65148\n",
      "iter: 1858, grad: 0.02418, loss_in_train: 2.15280, loss_in_test: 35.65149\n",
      "iter: 1859, grad: 0.02416, loss_in_train: 2.15280, loss_in_test: 35.65151\n",
      "iter: 1860, grad: 0.02414, loss_in_train: 2.15280, loss_in_test: 35.65152\n",
      "iter: 1861, grad: 0.02412, loss_in_train: 2.15280, loss_in_test: 35.65153\n",
      "iter: 1862, grad: 0.02409, loss_in_train: 2.15280, loss_in_test: 35.65155\n",
      "iter: 1863, grad: 0.02407, loss_in_train: 2.15280, loss_in_test: 35.65156\n",
      "iter: 1864, grad: 0.02405, loss_in_train: 2.15280, loss_in_test: 35.65158\n",
      "iter: 1865, grad: 0.02403, loss_in_train: 2.15280, loss_in_test: 35.65159\n",
      "iter: 1866, grad: 0.02401, loss_in_train: 2.15280, loss_in_test: 35.65160\n",
      "iter: 1867, grad: 0.02399, loss_in_train: 2.15280, loss_in_test: 35.65162\n",
      "iter: 1868, grad: 0.02397, loss_in_train: 2.15280, loss_in_test: 35.65163\n",
      "iter: 1869, grad: 0.02395, loss_in_train: 2.15280, loss_in_test: 35.65165\n",
      "iter: 1870, grad: 0.02393, loss_in_train: 2.15280, loss_in_test: 35.65166\n",
      "iter: 1871, grad: 0.02391, loss_in_train: 2.15280, loss_in_test: 35.65167\n",
      "iter: 1872, grad: 0.02389, loss_in_train: 2.15280, loss_in_test: 35.65169\n",
      "iter: 1873, grad: 0.02387, loss_in_train: 2.15280, loss_in_test: 35.65170\n",
      "iter: 1874, grad: 0.02385, loss_in_train: 2.15280, loss_in_test: 35.65171\n",
      "iter: 1875, grad: 0.02383, loss_in_train: 2.15280, loss_in_test: 35.65173\n",
      "iter: 1876, grad: 0.02381, loss_in_train: 2.15280, loss_in_test: 35.65174\n",
      "iter: 1877, grad: 0.02378, loss_in_train: 2.15280, loss_in_test: 35.65175\n",
      "iter: 1878, grad: 0.02376, loss_in_train: 2.15280, loss_in_test: 35.65177\n",
      "iter: 1879, grad: 0.02374, loss_in_train: 2.15280, loss_in_test: 35.65178\n",
      "iter: 1880, grad: 0.02372, loss_in_train: 2.15280, loss_in_test: 35.65179\n",
      "iter: 1881, grad: 0.02370, loss_in_train: 2.15280, loss_in_test: 35.65181\n",
      "iter: 1882, grad: 0.02368, loss_in_train: 2.15280, loss_in_test: 35.65182\n",
      "iter: 1883, grad: 0.02366, loss_in_train: 2.15280, loss_in_test: 35.65183\n",
      "iter: 1884, grad: 0.02364, loss_in_train: 2.15280, loss_in_test: 35.65185\n",
      "iter: 1885, grad: 0.02362, loss_in_train: 2.15280, loss_in_test: 35.65186\n",
      "iter: 1886, grad: 0.02360, loss_in_train: 2.15280, loss_in_test: 35.65187\n",
      "iter: 1887, grad: 0.02359, loss_in_train: 2.15280, loss_in_test: 35.65189\n",
      "iter: 1888, grad: 0.02357, loss_in_train: 2.15280, loss_in_test: 35.65190\n",
      "iter: 1889, grad: 0.02355, loss_in_train: 2.15280, loss_in_test: 35.65191\n",
      "iter: 1890, grad: 0.02353, loss_in_train: 2.15280, loss_in_test: 35.65193\n",
      "iter: 1891, grad: 0.02351, loss_in_train: 2.15280, loss_in_test: 35.65194\n",
      "iter: 1892, grad: 0.02349, loss_in_train: 2.15280, loss_in_test: 35.65195\n",
      "iter: 1893, grad: 0.02347, loss_in_train: 2.15280, loss_in_test: 35.65197\n",
      "iter: 1894, grad: 0.02345, loss_in_train: 2.15280, loss_in_test: 35.65198\n",
      "iter: 1895, grad: 0.02343, loss_in_train: 2.15280, loss_in_test: 35.65199\n",
      "iter: 1896, grad: 0.02341, loss_in_train: 2.15280, loss_in_test: 35.65201\n",
      "iter: 1897, grad: 0.02339, loss_in_train: 2.15280, loss_in_test: 35.65202\n",
      "iter: 1898, grad: 0.02337, loss_in_train: 2.15280, loss_in_test: 35.65203\n",
      "iter: 1899, grad: 0.02335, loss_in_train: 2.15280, loss_in_test: 35.65204\n",
      "iter: 1900, grad: 0.02333, loss_in_train: 2.15280, loss_in_test: 35.65206\n",
      "iter: 1901, grad: 0.02331, loss_in_train: 2.15280, loss_in_test: 35.65207\n",
      "iter: 1902, grad: 0.02329, loss_in_train: 2.15280, loss_in_test: 35.65208\n",
      "iter: 1903, grad: 0.02328, loss_in_train: 2.15280, loss_in_test: 35.65209\n",
      "iter: 1904, grad: 0.02326, loss_in_train: 2.15280, loss_in_test: 35.65211\n",
      "iter: 1905, grad: 0.02324, loss_in_train: 2.15280, loss_in_test: 35.65212\n",
      "iter: 1906, grad: 0.02322, loss_in_train: 2.15280, loss_in_test: 35.65213\n",
      "iter: 1907, grad: 0.02320, loss_in_train: 2.15280, loss_in_test: 35.65215\n",
      "iter: 1908, grad: 0.02318, loss_in_train: 2.15280, loss_in_test: 35.65216\n",
      "iter: 1909, grad: 0.02316, loss_in_train: 2.15280, loss_in_test: 35.65217\n",
      "iter: 1910, grad: 0.02314, loss_in_train: 2.15280, loss_in_test: 35.65218\n",
      "iter: 1911, grad: 0.02313, loss_in_train: 2.15280, loss_in_test: 35.65220\n",
      "iter: 1912, grad: 0.02311, loss_in_train: 2.15280, loss_in_test: 35.65221\n",
      "iter: 1913, grad: 0.02309, loss_in_train: 2.15280, loss_in_test: 35.65222\n",
      "iter: 1914, grad: 0.02307, loss_in_train: 2.15280, loss_in_test: 35.65223\n",
      "iter: 1915, grad: 0.02305, loss_in_train: 2.15280, loss_in_test: 35.65225\n",
      "iter: 1916, grad: 0.02303, loss_in_train: 2.15280, loss_in_test: 35.65226\n",
      "iter: 1917, grad: 0.02301, loss_in_train: 2.15280, loss_in_test: 35.65227\n",
      "iter: 1918, grad: 0.02300, loss_in_train: 2.15280, loss_in_test: 35.65228\n",
      "iter: 1919, grad: 0.02298, loss_in_train: 2.15280, loss_in_test: 35.65229\n",
      "iter: 1920, grad: 0.02296, loss_in_train: 2.15280, loss_in_test: 35.65231\n",
      "iter: 1921, grad: 0.02294, loss_in_train: 2.15280, loss_in_test: 35.65232\n",
      "iter: 1922, grad: 0.02292, loss_in_train: 2.15280, loss_in_test: 35.65233\n",
      "iter: 1923, grad: 0.02291, loss_in_train: 2.15280, loss_in_test: 35.65234\n",
      "iter: 1924, grad: 0.02289, loss_in_train: 2.15280, loss_in_test: 35.65236\n",
      "iter: 1925, grad: 0.02287, loss_in_train: 2.15280, loss_in_test: 35.65237\n",
      "iter: 1926, grad: 0.02285, loss_in_train: 2.15280, loss_in_test: 35.65238\n",
      "iter: 1927, grad: 0.02283, loss_in_train: 2.15280, loss_in_test: 35.65239\n",
      "iter: 1928, grad: 0.02282, loss_in_train: 2.15280, loss_in_test: 35.65240\n",
      "iter: 1929, grad: 0.02280, loss_in_train: 2.15280, loss_in_test: 35.65242\n",
      "iter: 1930, grad: 0.02278, loss_in_train: 2.15280, loss_in_test: 35.65243\n",
      "iter: 1931, grad: 0.02276, loss_in_train: 2.15280, loss_in_test: 35.65244\n",
      "iter: 1932, grad: 0.02275, loss_in_train: 2.15280, loss_in_test: 35.65245\n",
      "iter: 1933, grad: 0.02273, loss_in_train: 2.15280, loss_in_test: 35.65246\n",
      "iter: 1934, grad: 0.02271, loss_in_train: 2.15280, loss_in_test: 35.65248\n",
      "iter: 1935, grad: 0.02269, loss_in_train: 2.15280, loss_in_test: 35.65249\n",
      "iter: 1936, grad: 0.02267, loss_in_train: 2.15280, loss_in_test: 35.65250\n",
      "iter: 1937, grad: 0.02266, loss_in_train: 2.15280, loss_in_test: 35.65251\n",
      "iter: 1938, grad: 0.02264, loss_in_train: 2.15280, loss_in_test: 35.65252\n",
      "iter: 1939, grad: 0.02262, loss_in_train: 2.15280, loss_in_test: 35.65253\n",
      "iter: 1940, grad: 0.02261, loss_in_train: 2.15280, loss_in_test: 35.65255\n",
      "iter: 1941, grad: 0.02259, loss_in_train: 2.15280, loss_in_test: 35.65256\n",
      "iter: 1942, grad: 0.02257, loss_in_train: 2.15280, loss_in_test: 35.65257\n",
      "iter: 1943, grad: 0.02255, loss_in_train: 2.15280, loss_in_test: 35.65258\n",
      "iter: 1944, grad: 0.02254, loss_in_train: 2.15280, loss_in_test: 35.65259\n",
      "iter: 1945, grad: 0.02252, loss_in_train: 2.15280, loss_in_test: 35.65260\n",
      "iter: 1946, grad: 0.02250, loss_in_train: 2.15280, loss_in_test: 35.65262\n",
      "iter: 1947, grad: 0.02249, loss_in_train: 2.15280, loss_in_test: 35.65263\n",
      "iter: 1948, grad: 0.02247, loss_in_train: 2.15280, loss_in_test: 35.65264\n",
      "iter: 1949, grad: 0.02245, loss_in_train: 2.15280, loss_in_test: 35.65265\n",
      "iter: 1950, grad: 0.02243, loss_in_train: 2.15280, loss_in_test: 35.65266\n",
      "iter: 1951, grad: 0.02242, loss_in_train: 2.15280, loss_in_test: 35.65267\n",
      "iter: 1952, grad: 0.02240, loss_in_train: 2.15280, loss_in_test: 35.65268\n",
      "iter: 1953, grad: 0.02238, loss_in_train: 2.15280, loss_in_test: 35.65270\n",
      "iter: 1954, grad: 0.02237, loss_in_train: 2.15280, loss_in_test: 35.65271\n",
      "iter: 1955, grad: 0.02235, loss_in_train: 2.15280, loss_in_test: 35.65272\n",
      "iter: 1956, grad: 0.02233, loss_in_train: 2.15280, loss_in_test: 35.65273\n",
      "iter: 1957, grad: 0.02232, loss_in_train: 2.15280, loss_in_test: 35.65274\n",
      "iter: 1958, grad: 0.02230, loss_in_train: 2.15280, loss_in_test: 35.65275\n",
      "iter: 1959, grad: 0.02228, loss_in_train: 2.15280, loss_in_test: 35.65276\n",
      "iter: 1960, grad: 0.02227, loss_in_train: 2.15280, loss_in_test: 35.65278\n",
      "iter: 1961, grad: 0.02225, loss_in_train: 2.15280, loss_in_test: 35.65279\n",
      "iter: 1962, grad: 0.02224, loss_in_train: 2.15280, loss_in_test: 35.65280\n",
      "iter: 1963, grad: 0.02222, loss_in_train: 2.15280, loss_in_test: 35.65281\n",
      "iter: 1964, grad: 0.02220, loss_in_train: 2.15280, loss_in_test: 35.65282\n",
      "iter: 1965, grad: 0.02219, loss_in_train: 2.15280, loss_in_test: 35.65283\n",
      "iter: 1966, grad: 0.02217, loss_in_train: 2.15280, loss_in_test: 35.65284\n",
      "iter: 1967, grad: 0.02215, loss_in_train: 2.15280, loss_in_test: 35.65285\n",
      "iter: 1968, grad: 0.02214, loss_in_train: 2.15280, loss_in_test: 35.65286\n",
      "iter: 1969, grad: 0.02212, loss_in_train: 2.15280, loss_in_test: 35.65287\n",
      "iter: 1970, grad: 0.02211, loss_in_train: 2.15280, loss_in_test: 35.65289\n",
      "iter: 1971, grad: 0.02209, loss_in_train: 2.15280, loss_in_test: 35.65290\n",
      "iter: 1972, grad: 0.02207, loss_in_train: 2.15280, loss_in_test: 35.65291\n",
      "iter: 1973, grad: 0.02206, loss_in_train: 2.15280, loss_in_test: 35.65292\n",
      "iter: 1974, grad: 0.02204, loss_in_train: 2.15280, loss_in_test: 35.65293\n",
      "iter: 1975, grad: 0.02203, loss_in_train: 2.15280, loss_in_test: 35.65294\n",
      "iter: 1976, grad: 0.02201, loss_in_train: 2.15280, loss_in_test: 35.65295\n",
      "iter: 1977, grad: 0.02199, loss_in_train: 2.15280, loss_in_test: 35.65296\n",
      "iter: 1978, grad: 0.02198, loss_in_train: 2.15280, loss_in_test: 35.65297\n",
      "iter: 1979, grad: 0.02196, loss_in_train: 2.15280, loss_in_test: 35.65298\n",
      "iter: 1980, grad: 0.02195, loss_in_train: 2.15280, loss_in_test: 35.65299\n",
      "iter: 1981, grad: 0.02193, loss_in_train: 2.15280, loss_in_test: 35.65300\n",
      "iter: 1982, grad: 0.02192, loss_in_train: 2.15280, loss_in_test: 35.65301\n",
      "iter: 1983, grad: 0.02190, loss_in_train: 2.15280, loss_in_test: 35.65303\n",
      "iter: 1984, grad: 0.02189, loss_in_train: 2.15280, loss_in_test: 35.65304\n",
      "iter: 1985, grad: 0.02187, loss_in_train: 2.15280, loss_in_test: 35.65305\n",
      "iter: 1986, grad: 0.02185, loss_in_train: 2.15280, loss_in_test: 35.65306\n",
      "iter: 1987, grad: 0.02184, loss_in_train: 2.15280, loss_in_test: 35.65307\n",
      "iter: 1988, grad: 0.02182, loss_in_train: 2.15280, loss_in_test: 35.65308\n",
      "iter: 1989, grad: 0.02181, loss_in_train: 2.15280, loss_in_test: 35.65309\n",
      "iter: 1990, grad: 0.02179, loss_in_train: 2.15280, loss_in_test: 35.65310\n",
      "iter: 1991, grad: 0.02178, loss_in_train: 2.15280, loss_in_test: 35.65311\n",
      "iter: 1992, grad: 0.02176, loss_in_train: 2.15280, loss_in_test: 35.65312\n",
      "iter: 1993, grad: 0.02175, loss_in_train: 2.15280, loss_in_test: 35.65313\n",
      "iter: 1994, grad: 0.02173, loss_in_train: 2.15280, loss_in_test: 35.65314\n",
      "iter: 1995, grad: 0.02172, loss_in_train: 2.15280, loss_in_test: 35.65315\n",
      "iter: 1996, grad: 0.02170, loss_in_train: 2.15280, loss_in_test: 35.65316\n",
      "iter: 1997, grad: 0.02169, loss_in_train: 2.15280, loss_in_test: 35.65317\n",
      "iter: 1998, grad: 0.02167, loss_in_train: 2.15280, loss_in_test: 35.65318\n",
      "iter: 1999, grad: 0.02166, loss_in_train: 2.15280, loss_in_test: 35.65319\n",
      "iter: 2000, grad: 0.02164, loss_in_train: 2.15280, loss_in_test: 35.65320\n",
      "iter: 2001, grad: 0.02163, loss_in_train: 2.15280, loss_in_test: 35.65321\n",
      "iter: 2002, grad: 0.02161, loss_in_train: 2.15280, loss_in_test: 35.65322\n",
      "iter: 2003, grad: 0.02160, loss_in_train: 2.15280, loss_in_test: 35.65323\n",
      "iter: 2004, grad: 0.02158, loss_in_train: 2.15280, loss_in_test: 35.65324\n",
      "iter: 2005, grad: 0.02157, loss_in_train: 2.15280, loss_in_test: 35.65325\n",
      "iter: 2006, grad: 0.02155, loss_in_train: 2.15280, loss_in_test: 35.65326\n",
      "iter: 2007, grad: 0.02154, loss_in_train: 2.15280, loss_in_test: 35.65327\n",
      "iter: 2008, grad: 0.02152, loss_in_train: 2.15280, loss_in_test: 35.65328\n",
      "iter: 2009, grad: 0.02151, loss_in_train: 2.15280, loss_in_test: 35.65329\n",
      "iter: 2010, grad: 0.02150, loss_in_train: 2.15280, loss_in_test: 35.65330\n",
      "iter: 2011, grad: 0.02148, loss_in_train: 2.15280, loss_in_test: 35.65331\n",
      "iter: 2012, grad: 0.02147, loss_in_train: 2.15280, loss_in_test: 35.65332\n",
      "iter: 2013, grad: 0.02145, loss_in_train: 2.15280, loss_in_test: 35.65333\n",
      "iter: 2014, grad: 0.02144, loss_in_train: 2.15280, loss_in_test: 35.65334\n",
      "iter: 2015, grad: 0.02142, loss_in_train: 2.15280, loss_in_test: 35.65335\n",
      "iter: 2016, grad: 0.02141, loss_in_train: 2.15280, loss_in_test: 35.65336\n",
      "iter: 2017, grad: 0.02140, loss_in_train: 2.15280, loss_in_test: 35.65337\n",
      "iter: 2018, grad: 0.02138, loss_in_train: 2.15280, loss_in_test: 35.65338\n",
      "iter: 2019, grad: 0.02137, loss_in_train: 2.15280, loss_in_test: 35.65339\n",
      "iter: 2020, grad: 0.02135, loss_in_train: 2.15280, loss_in_test: 35.65340\n",
      "iter: 2021, grad: 0.02134, loss_in_train: 2.15280, loss_in_test: 35.65341\n",
      "iter: 2022, grad: 0.02132, loss_in_train: 2.15280, loss_in_test: 35.65342\n",
      "iter: 2023, grad: 0.02131, loss_in_train: 2.15280, loss_in_test: 35.65343\n",
      "iter: 2024, grad: 0.02130, loss_in_train: 2.15280, loss_in_test: 35.65344\n",
      "iter: 2025, grad: 0.02128, loss_in_train: 2.15280, loss_in_test: 35.65345\n",
      "iter: 2026, grad: 0.02127, loss_in_train: 2.15280, loss_in_test: 35.65346\n",
      "iter: 2027, grad: 0.02125, loss_in_train: 2.15280, loss_in_test: 35.65347\n",
      "iter: 2028, grad: 0.02124, loss_in_train: 2.15280, loss_in_test: 35.65348\n",
      "iter: 2029, grad: 0.02123, loss_in_train: 2.15280, loss_in_test: 35.65349\n",
      "iter: 2030, grad: 0.02121, loss_in_train: 2.15280, loss_in_test: 35.65350\n",
      "iter: 2031, grad: 0.02120, loss_in_train: 2.15280, loss_in_test: 35.65351\n",
      "iter: 2032, grad: 0.02119, loss_in_train: 2.15280, loss_in_test: 35.65352\n",
      "iter: 2033, grad: 0.02117, loss_in_train: 2.15280, loss_in_test: 35.65353\n",
      "iter: 2034, grad: 0.02116, loss_in_train: 2.15280, loss_in_test: 35.65354\n",
      "iter: 2035, grad: 0.02114, loss_in_train: 2.15280, loss_in_test: 35.65355\n",
      "iter: 2036, grad: 0.02113, loss_in_train: 2.15280, loss_in_test: 35.65355\n",
      "iter: 2037, grad: 0.02112, loss_in_train: 2.15280, loss_in_test: 35.65356\n",
      "iter: 2038, grad: 0.02110, loss_in_train: 2.15280, loss_in_test: 35.65357\n",
      "iter: 2039, grad: 0.02109, loss_in_train: 2.15280, loss_in_test: 35.65358\n",
      "iter: 2040, grad: 0.02108, loss_in_train: 2.15280, loss_in_test: 35.65359\n",
      "iter: 2041, grad: 0.02106, loss_in_train: 2.15280, loss_in_test: 35.65360\n",
      "iter: 2042, grad: 0.02105, loss_in_train: 2.15280, loss_in_test: 35.65361\n",
      "iter: 2043, grad: 0.02104, loss_in_train: 2.15280, loss_in_test: 35.65362\n",
      "iter: 2044, grad: 0.02102, loss_in_train: 2.15280, loss_in_test: 35.65363\n",
      "iter: 2045, grad: 0.02101, loss_in_train: 2.15280, loss_in_test: 35.65364\n",
      "iter: 2046, grad: 0.02100, loss_in_train: 2.15280, loss_in_test: 35.65365\n",
      "iter: 2047, grad: 0.02098, loss_in_train: 2.15280, loss_in_test: 35.65366\n",
      "iter: 2048, grad: 0.02097, loss_in_train: 2.15280, loss_in_test: 35.65367\n",
      "iter: 2049, grad: 0.02096, loss_in_train: 2.15280, loss_in_test: 35.65367\n",
      "iter: 2050, grad: 0.02094, loss_in_train: 2.15280, loss_in_test: 35.65368\n",
      "iter: 2051, grad: 0.02093, loss_in_train: 2.15280, loss_in_test: 35.65369\n",
      "iter: 2052, grad: 0.02092, loss_in_train: 2.15280, loss_in_test: 35.65370\n",
      "iter: 2053, grad: 0.02090, loss_in_train: 2.15280, loss_in_test: 35.65371\n",
      "iter: 2054, grad: 0.02089, loss_in_train: 2.15280, loss_in_test: 35.65372\n",
      "iter: 2055, grad: 0.02088, loss_in_train: 2.15280, loss_in_test: 35.65373\n",
      "iter: 2056, grad: 0.02087, loss_in_train: 2.15280, loss_in_test: 35.65374\n",
      "iter: 2057, grad: 0.02085, loss_in_train: 2.15280, loss_in_test: 35.65375\n",
      "iter: 2058, grad: 0.02084, loss_in_train: 2.15280, loss_in_test: 35.65376\n",
      "iter: 2059, grad: 0.02083, loss_in_train: 2.15280, loss_in_test: 35.65376\n",
      "iter: 2060, grad: 0.02081, loss_in_train: 2.15280, loss_in_test: 35.65377\n",
      "iter: 2061, grad: 0.02080, loss_in_train: 2.15280, loss_in_test: 35.65378\n",
      "iter: 2062, grad: 0.02079, loss_in_train: 2.15280, loss_in_test: 35.65379\n",
      "iter: 2063, grad: 0.02078, loss_in_train: 2.15280, loss_in_test: 35.65380\n",
      "iter: 2064, grad: 0.02076, loss_in_train: 2.15280, loss_in_test: 35.65381\n",
      "iter: 2065, grad: 0.02075, loss_in_train: 2.15280, loss_in_test: 35.65382\n",
      "iter: 2066, grad: 0.02074, loss_in_train: 2.15280, loss_in_test: 35.65383\n",
      "iter: 2067, grad: 0.02073, loss_in_train: 2.15280, loss_in_test: 35.65384\n",
      "iter: 2068, grad: 0.02071, loss_in_train: 2.15280, loss_in_test: 35.65384\n",
      "iter: 2069, grad: 0.02070, loss_in_train: 2.15280, loss_in_test: 35.65385\n",
      "iter: 2070, grad: 0.02069, loss_in_train: 2.15280, loss_in_test: 35.65386\n",
      "iter: 2071, grad: 0.02068, loss_in_train: 2.15280, loss_in_test: 35.65387\n",
      "iter: 2072, grad: 0.02066, loss_in_train: 2.15280, loss_in_test: 35.65388\n",
      "iter: 2073, grad: 0.02065, loss_in_train: 2.15280, loss_in_test: 35.65389\n",
      "iter: 2074, grad: 0.02064, loss_in_train: 2.15280, loss_in_test: 35.65390\n",
      "iter: 2075, grad: 0.02063, loss_in_train: 2.15280, loss_in_test: 35.65390\n",
      "iter: 2076, grad: 0.02061, loss_in_train: 2.15280, loss_in_test: 35.65391\n",
      "iter: 2077, grad: 0.02060, loss_in_train: 2.15280, loss_in_test: 35.65392\n",
      "iter: 2078, grad: 0.02059, loss_in_train: 2.15280, loss_in_test: 35.65393\n",
      "iter: 2079, grad: 0.02058, loss_in_train: 2.15280, loss_in_test: 35.65394\n",
      "iter: 2080, grad: 0.02056, loss_in_train: 2.15280, loss_in_test: 35.65395\n",
      "iter: 2081, grad: 0.02055, loss_in_train: 2.15280, loss_in_test: 35.65396\n",
      "iter: 2082, grad: 0.02054, loss_in_train: 2.15280, loss_in_test: 35.65396\n",
      "iter: 2083, grad: 0.02053, loss_in_train: 2.15280, loss_in_test: 35.65397\n",
      "iter: 2084, grad: 0.02052, loss_in_train: 2.15280, loss_in_test: 35.65398\n",
      "iter: 2085, grad: 0.02050, loss_in_train: 2.15280, loss_in_test: 35.65399\n",
      "iter: 2086, grad: 0.02049, loss_in_train: 2.15280, loss_in_test: 35.65400\n",
      "iter: 2087, grad: 0.02048, loss_in_train: 2.15280, loss_in_test: 35.65401\n",
      "iter: 2088, grad: 0.02047, loss_in_train: 2.15280, loss_in_test: 35.65401\n",
      "iter: 2089, grad: 0.02046, loss_in_train: 2.15280, loss_in_test: 35.65402\n",
      "iter: 2090, grad: 0.02044, loss_in_train: 2.15280, loss_in_test: 35.65403\n",
      "iter: 2091, grad: 0.02043, loss_in_train: 2.15280, loss_in_test: 35.65404\n",
      "iter: 2092, grad: 0.02042, loss_in_train: 2.15280, loss_in_test: 35.65405\n",
      "iter: 2093, grad: 0.02041, loss_in_train: 2.15280, loss_in_test: 35.65406\n",
      "iter: 2094, grad: 0.02040, loss_in_train: 2.15280, loss_in_test: 35.65406\n",
      "iter: 2095, grad: 0.02039, loss_in_train: 2.15280, loss_in_test: 35.65407\n",
      "iter: 2096, grad: 0.02037, loss_in_train: 2.15280, loss_in_test: 35.65408\n",
      "iter: 2097, grad: 0.02036, loss_in_train: 2.15280, loss_in_test: 35.65409\n",
      "iter: 2098, grad: 0.02035, loss_in_train: 2.15280, loss_in_test: 35.65410\n",
      "iter: 2099, grad: 0.02034, loss_in_train: 2.15280, loss_in_test: 35.65410\n",
      "iter: 2100, grad: 0.02033, loss_in_train: 2.15280, loss_in_test: 35.65411\n",
      "iter: 2101, grad: 0.02032, loss_in_train: 2.15280, loss_in_test: 35.65412\n",
      "iter: 2102, grad: 0.02030, loss_in_train: 2.15280, loss_in_test: 35.65413\n",
      "iter: 2103, grad: 0.02029, loss_in_train: 2.15280, loss_in_test: 35.65414\n",
      "iter: 2104, grad: 0.02028, loss_in_train: 2.15280, loss_in_test: 35.65414\n",
      "iter: 2105, grad: 0.02027, loss_in_train: 2.15280, loss_in_test: 35.65415\n",
      "iter: 2106, grad: 0.02026, loss_in_train: 2.15280, loss_in_test: 35.65416\n",
      "iter: 2107, grad: 0.02025, loss_in_train: 2.15280, loss_in_test: 35.65417\n",
      "iter: 2108, grad: 0.02024, loss_in_train: 2.15280, loss_in_test: 35.65418\n",
      "iter: 2109, grad: 0.02022, loss_in_train: 2.15280, loss_in_test: 35.65418\n",
      "iter: 2110, grad: 0.02021, loss_in_train: 2.15280, loss_in_test: 35.65419\n",
      "iter: 2111, grad: 0.02020, loss_in_train: 2.15280, loss_in_test: 35.65420\n",
      "iter: 2112, grad: 0.02019, loss_in_train: 2.15280, loss_in_test: 35.65421\n",
      "iter: 2113, grad: 0.02018, loss_in_train: 2.15280, loss_in_test: 35.65422\n",
      "iter: 2114, grad: 0.02017, loss_in_train: 2.15280, loss_in_test: 35.65422\n",
      "iter: 2115, grad: 0.02016, loss_in_train: 2.15280, loss_in_test: 35.65423\n",
      "iter: 2116, grad: 0.02015, loss_in_train: 2.15280, loss_in_test: 35.65424\n",
      "iter: 2117, grad: 0.02013, loss_in_train: 2.15280, loss_in_test: 35.65425\n",
      "iter: 2118, grad: 0.02012, loss_in_train: 2.15280, loss_in_test: 35.65425\n",
      "iter: 2119, grad: 0.02011, loss_in_train: 2.15280, loss_in_test: 35.65426\n",
      "iter: 2120, grad: 0.02010, loss_in_train: 2.15280, loss_in_test: 35.65427\n",
      "iter: 2121, grad: 0.02009, loss_in_train: 2.15280, loss_in_test: 35.65428\n",
      "iter: 2122, grad: 0.02008, loss_in_train: 2.15280, loss_in_test: 35.65429\n",
      "iter: 2123, grad: 0.02007, loss_in_train: 2.15280, loss_in_test: 35.65429\n",
      "iter: 2124, grad: 0.02006, loss_in_train: 2.15280, loss_in_test: 35.65430\n",
      "iter: 2125, grad: 0.02005, loss_in_train: 2.15280, loss_in_test: 35.65431\n",
      "iter: 2126, grad: 0.02004, loss_in_train: 2.15280, loss_in_test: 35.65432\n",
      "iter: 2127, grad: 0.02002, loss_in_train: 2.15280, loss_in_test: 35.65432\n",
      "iter: 2128, grad: 0.02001, loss_in_train: 2.15280, loss_in_test: 35.65433\n",
      "iter: 2129, grad: 0.02000, loss_in_train: 2.15280, loss_in_test: 35.65434\n",
      "iter: 2130, grad: 0.01999, loss_in_train: 2.15280, loss_in_test: 35.65435\n",
      "iter: 2131, grad: 0.01998, loss_in_train: 2.15280, loss_in_test: 35.65435\n",
      "iter: 2132, grad: 0.01997, loss_in_train: 2.15280, loss_in_test: 35.65436\n",
      "iter: 2133, grad: 0.01996, loss_in_train: 2.15280, loss_in_test: 35.65437\n",
      "iter: 2134, grad: 0.01995, loss_in_train: 2.15280, loss_in_test: 35.65438\n",
      "iter: 2135, grad: 0.01994, loss_in_train: 2.15280, loss_in_test: 35.65438\n",
      "iter: 2136, grad: 0.01993, loss_in_train: 2.15280, loss_in_test: 35.65439\n",
      "iter: 2137, grad: 0.01992, loss_in_train: 2.15280, loss_in_test: 35.65440\n",
      "iter: 2138, grad: 0.01991, loss_in_train: 2.15280, loss_in_test: 35.65441\n",
      "iter: 2139, grad: 0.01990, loss_in_train: 2.15280, loss_in_test: 35.65441\n",
      "iter: 2140, grad: 0.01989, loss_in_train: 2.15280, loss_in_test: 35.65442\n",
      "iter: 2141, grad: 0.01988, loss_in_train: 2.15280, loss_in_test: 35.65443\n",
      "iter: 2142, grad: 0.01987, loss_in_train: 2.15280, loss_in_test: 35.65444\n",
      "iter: 2143, grad: 0.01985, loss_in_train: 2.15280, loss_in_test: 35.65444\n",
      "iter: 2144, grad: 0.01984, loss_in_train: 2.15280, loss_in_test: 35.65445\n",
      "iter: 2145, grad: 0.01983, loss_in_train: 2.15280, loss_in_test: 35.65446\n",
      "iter: 2146, grad: 0.01982, loss_in_train: 2.15280, loss_in_test: 35.65446\n",
      "iter: 2147, grad: 0.01981, loss_in_train: 2.15280, loss_in_test: 35.65447\n",
      "iter: 2148, grad: 0.01980, loss_in_train: 2.15280, loss_in_test: 35.65448\n",
      "iter: 2149, grad: 0.01979, loss_in_train: 2.15280, loss_in_test: 35.65449\n",
      "iter: 2150, grad: 0.01978, loss_in_train: 2.15280, loss_in_test: 35.65449\n",
      "iter: 2151, grad: 0.01977, loss_in_train: 2.15280, loss_in_test: 35.65450\n",
      "iter: 2152, grad: 0.01976, loss_in_train: 2.15280, loss_in_test: 35.65451\n",
      "iter: 2153, grad: 0.01975, loss_in_train: 2.15280, loss_in_test: 35.65451\n",
      "iter: 2154, grad: 0.01974, loss_in_train: 2.15280, loss_in_test: 35.65452\n",
      "iter: 2155, grad: 0.01973, loss_in_train: 2.15280, loss_in_test: 35.65453\n",
      "iter: 2156, grad: 0.01972, loss_in_train: 2.15280, loss_in_test: 35.65454\n",
      "iter: 2157, grad: 0.01971, loss_in_train: 2.15280, loss_in_test: 35.65454\n",
      "iter: 2158, grad: 0.01970, loss_in_train: 2.15280, loss_in_test: 35.65455\n",
      "iter: 2159, grad: 0.01969, loss_in_train: 2.15280, loss_in_test: 35.65456\n",
      "iter: 2160, grad: 0.01968, loss_in_train: 2.15280, loss_in_test: 35.65456\n",
      "iter: 2161, grad: 0.01967, loss_in_train: 2.15280, loss_in_test: 35.65457\n",
      "iter: 2162, grad: 0.01966, loss_in_train: 2.15280, loss_in_test: 35.65458\n",
      "iter: 2163, grad: 0.01965, loss_in_train: 2.15280, loss_in_test: 35.65459\n",
      "iter: 2164, grad: 0.01964, loss_in_train: 2.15280, loss_in_test: 35.65459\n",
      "iter: 2165, grad: 0.01963, loss_in_train: 2.15280, loss_in_test: 35.65460\n",
      "iter: 2166, grad: 0.01962, loss_in_train: 2.15280, loss_in_test: 35.65461\n",
      "iter: 2167, grad: 0.01961, loss_in_train: 2.15280, loss_in_test: 35.65461\n",
      "iter: 2168, grad: 0.01960, loss_in_train: 2.15280, loss_in_test: 35.65462\n",
      "iter: 2169, grad: 0.01959, loss_in_train: 2.15280, loss_in_test: 35.65463\n",
      "iter: 2170, grad: 0.01958, loss_in_train: 2.15280, loss_in_test: 35.65463\n",
      "iter: 2171, grad: 0.01957, loss_in_train: 2.15280, loss_in_test: 35.65464\n",
      "iter: 2172, grad: 0.01956, loss_in_train: 2.15280, loss_in_test: 35.65465\n",
      "iter: 2173, grad: 0.01955, loss_in_train: 2.15280, loss_in_test: 35.65465\n",
      "iter: 2174, grad: 0.01954, loss_in_train: 2.15280, loss_in_test: 35.65466\n",
      "iter: 2175, grad: 0.01954, loss_in_train: 2.15280, loss_in_test: 35.65467\n",
      "iter: 2176, grad: 0.01953, loss_in_train: 2.15280, loss_in_test: 35.65467\n",
      "iter: 2177, grad: 0.01952, loss_in_train: 2.15280, loss_in_test: 35.65468\n",
      "iter: 2178, grad: 0.01951, loss_in_train: 2.15280, loss_in_test: 35.65469\n",
      "iter: 2179, grad: 0.01950, loss_in_train: 2.15280, loss_in_test: 35.65469\n",
      "iter: 2180, grad: 0.01949, loss_in_train: 2.15280, loss_in_test: 35.65470\n",
      "iter: 2181, grad: 0.01948, loss_in_train: 2.15280, loss_in_test: 35.65471\n",
      "iter: 2182, grad: 0.01947, loss_in_train: 2.15280, loss_in_test: 35.65471\n",
      "iter: 2183, grad: 0.01946, loss_in_train: 2.15280, loss_in_test: 35.65472\n",
      "iter: 2184, grad: 0.01945, loss_in_train: 2.15280, loss_in_test: 35.65473\n",
      "iter: 2185, grad: 0.01944, loss_in_train: 2.15280, loss_in_test: 35.65473\n",
      "iter: 2186, grad: 0.01943, loss_in_train: 2.15280, loss_in_test: 35.65474\n",
      "iter: 2187, grad: 0.01942, loss_in_train: 2.15280, loss_in_test: 35.65475\n",
      "iter: 2188, grad: 0.01941, loss_in_train: 2.15280, loss_in_test: 35.65475\n",
      "iter: 2189, grad: 0.01940, loss_in_train: 2.15280, loss_in_test: 35.65476\n",
      "iter: 2190, grad: 0.01939, loss_in_train: 2.15280, loss_in_test: 35.65477\n",
      "iter: 2191, grad: 0.01938, loss_in_train: 2.15280, loss_in_test: 35.65477\n",
      "iter: 2192, grad: 0.01938, loss_in_train: 2.15280, loss_in_test: 35.65478\n",
      "iter: 2193, grad: 0.01937, loss_in_train: 2.15280, loss_in_test: 35.65479\n",
      "iter: 2194, grad: 0.01936, loss_in_train: 2.15280, loss_in_test: 35.65479\n",
      "iter: 2195, grad: 0.01935, loss_in_train: 2.15280, loss_in_test: 35.65480\n",
      "iter: 2196, grad: 0.01934, loss_in_train: 2.15280, loss_in_test: 35.65481\n",
      "iter: 2197, grad: 0.01933, loss_in_train: 2.15280, loss_in_test: 35.65481\n",
      "iter: 2198, grad: 0.01932, loss_in_train: 2.15280, loss_in_test: 35.65482\n",
      "iter: 2199, grad: 0.01931, loss_in_train: 2.15280, loss_in_test: 35.65483\n",
      "iter: 2200, grad: 0.01930, loss_in_train: 2.15280, loss_in_test: 35.65483\n",
      "iter: 2201, grad: 0.01929, loss_in_train: 2.15280, loss_in_test: 35.65484\n",
      "iter: 2202, grad: 0.01928, loss_in_train: 2.15280, loss_in_test: 35.65484\n",
      "iter: 2203, grad: 0.01928, loss_in_train: 2.15280, loss_in_test: 35.65485\n",
      "iter: 2204, grad: 0.01927, loss_in_train: 2.15280, loss_in_test: 35.65486\n",
      "iter: 2205, grad: 0.01926, loss_in_train: 2.15280, loss_in_test: 35.65486\n",
      "iter: 2206, grad: 0.01925, loss_in_train: 2.15280, loss_in_test: 35.65487\n",
      "iter: 2207, grad: 0.01924, loss_in_train: 2.15280, loss_in_test: 35.65488\n",
      "iter: 2208, grad: 0.01923, loss_in_train: 2.15280, loss_in_test: 35.65488\n",
      "iter: 2209, grad: 0.01922, loss_in_train: 2.15280, loss_in_test: 35.65489\n",
      "iter: 2210, grad: 0.01921, loss_in_train: 2.15280, loss_in_test: 35.65489\n",
      "iter: 2211, grad: 0.01920, loss_in_train: 2.15280, loss_in_test: 35.65490\n",
      "iter: 2212, grad: 0.01920, loss_in_train: 2.15280, loss_in_test: 35.65491\n",
      "iter: 2213, grad: 0.01919, loss_in_train: 2.15280, loss_in_test: 35.65491\n",
      "iter: 2214, grad: 0.01918, loss_in_train: 2.15280, loss_in_test: 35.65492\n",
      "iter: 2215, grad: 0.01917, loss_in_train: 2.15280, loss_in_test: 35.65493\n",
      "iter: 2216, grad: 0.01916, loss_in_train: 2.15280, loss_in_test: 35.65493\n",
      "iter: 2217, grad: 0.01915, loss_in_train: 2.15280, loss_in_test: 35.65494\n",
      "iter: 2218, grad: 0.01914, loss_in_train: 2.15280, loss_in_test: 35.65494\n",
      "iter: 2219, grad: 0.01914, loss_in_train: 2.15280, loss_in_test: 35.65495\n",
      "iter: 2220, grad: 0.01913, loss_in_train: 2.15280, loss_in_test: 35.65496\n",
      "iter: 2221, grad: 0.01912, loss_in_train: 2.15280, loss_in_test: 35.65496\n",
      "iter: 2222, grad: 0.01911, loss_in_train: 2.15280, loss_in_test: 35.65497\n",
      "iter: 2223, grad: 0.01910, loss_in_train: 2.15280, loss_in_test: 35.65497\n",
      "iter: 2224, grad: 0.01909, loss_in_train: 2.15280, loss_in_test: 35.65498\n",
      "iter: 2225, grad: 0.01908, loss_in_train: 2.15280, loss_in_test: 35.65499\n",
      "iter: 2226, grad: 0.01908, loss_in_train: 2.15280, loss_in_test: 35.65499\n",
      "iter: 2227, grad: 0.01907, loss_in_train: 2.15280, loss_in_test: 35.65500\n",
      "iter: 2228, grad: 0.01906, loss_in_train: 2.15280, loss_in_test: 35.65500\n",
      "iter: 2229, grad: 0.01905, loss_in_train: 2.15280, loss_in_test: 35.65501\n",
      "iter: 2230, grad: 0.01904, loss_in_train: 2.15280, loss_in_test: 35.65502\n",
      "iter: 2231, grad: 0.01903, loss_in_train: 2.15280, loss_in_test: 35.65502\n",
      "iter: 2232, grad: 0.01903, loss_in_train: 2.15280, loss_in_test: 35.65503\n",
      "iter: 2233, grad: 0.01902, loss_in_train: 2.15280, loss_in_test: 35.65503\n",
      "iter: 2234, grad: 0.01901, loss_in_train: 2.15280, loss_in_test: 35.65504\n",
      "iter: 2235, grad: 0.01900, loss_in_train: 2.15280, loss_in_test: 35.65505\n",
      "iter: 2236, grad: 0.01899, loss_in_train: 2.15280, loss_in_test: 35.65505\n",
      "iter: 2237, grad: 0.01898, loss_in_train: 2.15280, loss_in_test: 35.65506\n",
      "iter: 2238, grad: 0.01898, loss_in_train: 2.15280, loss_in_test: 35.65506\n",
      "iter: 2239, grad: 0.01897, loss_in_train: 2.15280, loss_in_test: 35.65507\n",
      "iter: 2240, grad: 0.01896, loss_in_train: 2.15280, loss_in_test: 35.65508\n",
      "iter: 2241, grad: 0.01895, loss_in_train: 2.15280, loss_in_test: 35.65508\n",
      "iter: 2242, grad: 0.01894, loss_in_train: 2.15280, loss_in_test: 35.65509\n",
      "iter: 2243, grad: 0.01894, loss_in_train: 2.15280, loss_in_test: 35.65509\n",
      "iter: 2244, grad: 0.01893, loss_in_train: 2.15280, loss_in_test: 35.65510\n",
      "iter: 2245, grad: 0.01892, loss_in_train: 2.15280, loss_in_test: 35.65510\n",
      "iter: 2246, grad: 0.01891, loss_in_train: 2.15280, loss_in_test: 35.65511\n",
      "iter: 2247, grad: 0.01890, loss_in_train: 2.15280, loss_in_test: 35.65512\n",
      "iter: 2248, grad: 0.01889, loss_in_train: 2.15280, loss_in_test: 35.65512\n",
      "iter: 2249, grad: 0.01889, loss_in_train: 2.15280, loss_in_test: 35.65513\n",
      "iter: 2250, grad: 0.01888, loss_in_train: 2.15280, loss_in_test: 35.65513\n",
      "iter: 2251, grad: 0.01887, loss_in_train: 2.15280, loss_in_test: 35.65514\n",
      "iter: 2252, grad: 0.01886, loss_in_train: 2.15280, loss_in_test: 35.65514\n",
      "iter: 2253, grad: 0.01886, loss_in_train: 2.15280, loss_in_test: 35.65515\n",
      "iter: 2254, grad: 0.01885, loss_in_train: 2.15280, loss_in_test: 35.65515\n",
      "iter: 2255, grad: 0.01884, loss_in_train: 2.15280, loss_in_test: 35.65516\n",
      "iter: 2256, grad: 0.01883, loss_in_train: 2.15280, loss_in_test: 35.65517\n",
      "iter: 2257, grad: 0.01882, loss_in_train: 2.15280, loss_in_test: 35.65517\n",
      "iter: 2258, grad: 0.01882, loss_in_train: 2.15280, loss_in_test: 35.65518\n",
      "iter: 2259, grad: 0.01881, loss_in_train: 2.15280, loss_in_test: 35.65518\n",
      "iter: 2260, grad: 0.01880, loss_in_train: 2.15280, loss_in_test: 35.65519\n",
      "iter: 2261, grad: 0.01879, loss_in_train: 2.15280, loss_in_test: 35.65519\n",
      "iter: 2262, grad: 0.01878, loss_in_train: 2.15280, loss_in_test: 35.65520\n",
      "iter: 2263, grad: 0.01878, loss_in_train: 2.15280, loss_in_test: 35.65520\n",
      "iter: 2264, grad: 0.01877, loss_in_train: 2.15280, loss_in_test: 35.65521\n",
      "iter: 2265, grad: 0.01876, loss_in_train: 2.15280, loss_in_test: 35.65522\n",
      "iter: 2266, grad: 0.01875, loss_in_train: 2.15280, loss_in_test: 35.65522\n",
      "iter: 2267, grad: 0.01875, loss_in_train: 2.15280, loss_in_test: 35.65523\n",
      "iter: 2268, grad: 0.01874, loss_in_train: 2.15280, loss_in_test: 35.65523\n",
      "iter: 2269, grad: 0.01873, loss_in_train: 2.15280, loss_in_test: 35.65524\n",
      "iter: 2270, grad: 0.01872, loss_in_train: 2.15280, loss_in_test: 35.65524\n",
      "iter: 2271, grad: 0.01872, loss_in_train: 2.15280, loss_in_test: 35.65525\n",
      "iter: 2272, grad: 0.01871, loss_in_train: 2.15280, loss_in_test: 35.65525\n",
      "iter: 2273, grad: 0.01870, loss_in_train: 2.15280, loss_in_test: 35.65526\n",
      "iter: 2274, grad: 0.01869, loss_in_train: 2.15280, loss_in_test: 35.65526\n",
      "iter: 2275, grad: 0.01869, loss_in_train: 2.15280, loss_in_test: 35.65527\n",
      "iter: 2276, grad: 0.01868, loss_in_train: 2.15280, loss_in_test: 35.65528\n",
      "iter: 2277, grad: 0.01867, loss_in_train: 2.15280, loss_in_test: 35.65528\n",
      "iter: 2278, grad: 0.01866, loss_in_train: 2.15280, loss_in_test: 35.65529\n",
      "iter: 2279, grad: 0.01866, loss_in_train: 2.15280, loss_in_test: 35.65529\n",
      "iter: 2280, grad: 0.01865, loss_in_train: 2.15280, loss_in_test: 35.65530\n",
      "iter: 2281, grad: 0.01864, loss_in_train: 2.15280, loss_in_test: 35.65530\n",
      "iter: 2282, grad: 0.01863, loss_in_train: 2.15280, loss_in_test: 35.65531\n",
      "iter: 2283, grad: 0.01863, loss_in_train: 2.15280, loss_in_test: 35.65531\n",
      "iter: 2284, grad: 0.01862, loss_in_train: 2.15280, loss_in_test: 35.65532\n",
      "iter: 2285, grad: 0.01861, loss_in_train: 2.15280, loss_in_test: 35.65532\n",
      "iter: 2286, grad: 0.01860, loss_in_train: 2.15280, loss_in_test: 35.65533\n",
      "iter: 2287, grad: 0.01860, loss_in_train: 2.15280, loss_in_test: 35.65533\n",
      "iter: 2288, grad: 0.01859, loss_in_train: 2.15280, loss_in_test: 35.65534\n",
      "iter: 2289, grad: 0.01858, loss_in_train: 2.15280, loss_in_test: 35.65534\n",
      "iter: 2290, grad: 0.01858, loss_in_train: 2.15280, loss_in_test: 35.65535\n",
      "iter: 2291, grad: 0.01857, loss_in_train: 2.15280, loss_in_test: 35.65535\n",
      "iter: 2292, grad: 0.01856, loss_in_train: 2.15280, loss_in_test: 35.65536\n",
      "iter: 2293, grad: 0.01855, loss_in_train: 2.15280, loss_in_test: 35.65536\n",
      "iter: 2294, grad: 0.01855, loss_in_train: 2.15280, loss_in_test: 35.65537\n",
      "iter: 2295, grad: 0.01854, loss_in_train: 2.15280, loss_in_test: 35.65537\n",
      "iter: 2296, grad: 0.01853, loss_in_train: 2.15280, loss_in_test: 35.65538\n",
      "iter: 2297, grad: 0.01853, loss_in_train: 2.15280, loss_in_test: 35.65538\n",
      "iter: 2298, grad: 0.01852, loss_in_train: 2.15280, loss_in_test: 35.65539\n",
      "iter: 2299, grad: 0.01851, loss_in_train: 2.15280, loss_in_test: 35.65539\n",
      "iter: 2300, grad: 0.01850, loss_in_train: 2.15280, loss_in_test: 35.65540\n",
      "iter: 2301, grad: 0.01850, loss_in_train: 2.15280, loss_in_test: 35.65540\n",
      "iter: 2302, grad: 0.01849, loss_in_train: 2.15280, loss_in_test: 35.65541\n",
      "iter: 2303, grad: 0.01848, loss_in_train: 2.15280, loss_in_test: 35.65541\n",
      "iter: 2304, grad: 0.01848, loss_in_train: 2.15280, loss_in_test: 35.65542\n",
      "iter: 2305, grad: 0.01847, loss_in_train: 2.15280, loss_in_test: 35.65542\n",
      "iter: 2306, grad: 0.01846, loss_in_train: 2.15280, loss_in_test: 35.65543\n",
      "iter: 2307, grad: 0.01846, loss_in_train: 2.15280, loss_in_test: 35.65543\n",
      "iter: 2308, grad: 0.01845, loss_in_train: 2.15280, loss_in_test: 35.65544\n",
      "iter: 2309, grad: 0.01844, loss_in_train: 2.15280, loss_in_test: 35.65544\n",
      "iter: 2310, grad: 0.01844, loss_in_train: 2.15280, loss_in_test: 35.65545\n",
      "iter: 2311, grad: 0.01843, loss_in_train: 2.15280, loss_in_test: 35.65545\n",
      "iter: 2312, grad: 0.01842, loss_in_train: 2.15280, loss_in_test: 35.65546\n",
      "iter: 2313, grad: 0.01842, loss_in_train: 2.15280, loss_in_test: 35.65546\n",
      "iter: 2314, grad: 0.01841, loss_in_train: 2.15280, loss_in_test: 35.65547\n",
      "iter: 2315, grad: 0.01840, loss_in_train: 2.15280, loss_in_test: 35.65547\n",
      "iter: 2316, grad: 0.01839, loss_in_train: 2.15280, loss_in_test: 35.65548\n",
      "iter: 2317, grad: 0.01839, loss_in_train: 2.15280, loss_in_test: 35.65548\n",
      "iter: 2318, grad: 0.01838, loss_in_train: 2.15280, loss_in_test: 35.65549\n",
      "iter: 2319, grad: 0.01837, loss_in_train: 2.15280, loss_in_test: 35.65549\n",
      "iter: 2320, grad: 0.01837, loss_in_train: 2.15280, loss_in_test: 35.65550\n",
      "iter: 2321, grad: 0.01836, loss_in_train: 2.15280, loss_in_test: 35.65550\n",
      "iter: 2322, grad: 0.01835, loss_in_train: 2.15280, loss_in_test: 35.65551\n",
      "iter: 2323, grad: 0.01835, loss_in_train: 2.15280, loss_in_test: 35.65551\n",
      "iter: 2324, grad: 0.01834, loss_in_train: 2.15280, loss_in_test: 35.65552\n",
      "iter: 2325, grad: 0.01833, loss_in_train: 2.15280, loss_in_test: 35.65552\n",
      "iter: 2326, grad: 0.01833, loss_in_train: 2.15280, loss_in_test: 35.65553\n",
      "iter: 2327, grad: 0.01832, loss_in_train: 2.15280, loss_in_test: 35.65553\n",
      "iter: 2328, grad: 0.01832, loss_in_train: 2.15280, loss_in_test: 35.65553\n",
      "iter: 2329, grad: 0.01831, loss_in_train: 2.15280, loss_in_test: 35.65554\n",
      "iter: 2330, grad: 0.01830, loss_in_train: 2.15280, loss_in_test: 35.65554\n",
      "iter: 2331, grad: 0.01830, loss_in_train: 2.15280, loss_in_test: 35.65555\n",
      "iter: 2332, grad: 0.01829, loss_in_train: 2.15280, loss_in_test: 35.65555\n",
      "iter: 2333, grad: 0.01828, loss_in_train: 2.15280, loss_in_test: 35.65556\n",
      "iter: 2334, grad: 0.01828, loss_in_train: 2.15280, loss_in_test: 35.65556\n",
      "iter: 2335, grad: 0.01827, loss_in_train: 2.15280, loss_in_test: 35.65557\n",
      "iter: 2336, grad: 0.01826, loss_in_train: 2.15280, loss_in_test: 35.65557\n",
      "iter: 2337, grad: 0.01826, loss_in_train: 2.15280, loss_in_test: 35.65558\n",
      "iter: 2338, grad: 0.01825, loss_in_train: 2.15280, loss_in_test: 35.65558\n",
      "iter: 2339, grad: 0.01824, loss_in_train: 2.15280, loss_in_test: 35.65559\n",
      "iter: 2340, grad: 0.01824, loss_in_train: 2.15280, loss_in_test: 35.65559\n",
      "iter: 2341, grad: 0.01823, loss_in_train: 2.15280, loss_in_test: 35.65559\n",
      "iter: 2342, grad: 0.01823, loss_in_train: 2.15280, loss_in_test: 35.65560\n",
      "iter: 2343, grad: 0.01822, loss_in_train: 2.15280, loss_in_test: 35.65560\n",
      "iter: 2344, grad: 0.01821, loss_in_train: 2.15280, loss_in_test: 35.65561\n",
      "iter: 2345, grad: 0.01821, loss_in_train: 2.15280, loss_in_test: 35.65561\n",
      "iter: 2346, grad: 0.01820, loss_in_train: 2.15280, loss_in_test: 35.65562\n",
      "iter: 2347, grad: 0.01819, loss_in_train: 2.15280, loss_in_test: 35.65562\n",
      "iter: 2348, grad: 0.01819, loss_in_train: 2.15280, loss_in_test: 35.65563\n",
      "iter: 2349, grad: 0.01818, loss_in_train: 2.15280, loss_in_test: 35.65563\n",
      "iter: 2350, grad: 0.01818, loss_in_train: 2.15280, loss_in_test: 35.65564\n",
      "iter: 2351, grad: 0.01817, loss_in_train: 2.15280, loss_in_test: 35.65564\n",
      "iter: 2352, grad: 0.01816, loss_in_train: 2.15280, loss_in_test: 35.65564\n",
      "iter: 2353, grad: 0.01816, loss_in_train: 2.15280, loss_in_test: 35.65565\n",
      "iter: 2354, grad: 0.01815, loss_in_train: 2.15280, loss_in_test: 35.65565\n",
      "iter: 2355, grad: 0.01814, loss_in_train: 2.15280, loss_in_test: 35.65566\n",
      "iter: 2356, grad: 0.01814, loss_in_train: 2.15280, loss_in_test: 35.65566\n",
      "iter: 2357, grad: 0.01813, loss_in_train: 2.15280, loss_in_test: 35.65567\n",
      "iter: 2358, grad: 0.01813, loss_in_train: 2.15280, loss_in_test: 35.65567\n",
      "iter: 2359, grad: 0.01812, loss_in_train: 2.15280, loss_in_test: 35.65567\n",
      "iter: 2360, grad: 0.01811, loss_in_train: 2.15280, loss_in_test: 35.65568\n",
      "iter: 2361, grad: 0.01811, loss_in_train: 2.15280, loss_in_test: 35.65568\n",
      "iter: 2362, grad: 0.01810, loss_in_train: 2.15280, loss_in_test: 35.65569\n",
      "iter: 2363, grad: 0.01810, loss_in_train: 2.15280, loss_in_test: 35.65569\n",
      "iter: 2364, grad: 0.01809, loss_in_train: 2.15280, loss_in_test: 35.65570\n",
      "iter: 2365, grad: 0.01808, loss_in_train: 2.15280, loss_in_test: 35.65570\n",
      "iter: 2366, grad: 0.01808, loss_in_train: 2.15280, loss_in_test: 35.65570\n",
      "iter: 2367, grad: 0.01807, loss_in_train: 2.15280, loss_in_test: 35.65571\n",
      "iter: 2368, grad: 0.01807, loss_in_train: 2.15280, loss_in_test: 35.65571\n",
      "iter: 2369, grad: 0.01806, loss_in_train: 2.15280, loss_in_test: 35.65572\n",
      "iter: 2370, grad: 0.01805, loss_in_train: 2.15280, loss_in_test: 35.65572\n",
      "iter: 2371, grad: 0.01805, loss_in_train: 2.15280, loss_in_test: 35.65573\n",
      "iter: 2372, grad: 0.01804, loss_in_train: 2.15280, loss_in_test: 35.65573\n",
      "iter: 2373, grad: 0.01804, loss_in_train: 2.15280, loss_in_test: 35.65573\n",
      "iter: 2374, grad: 0.01803, loss_in_train: 2.15280, loss_in_test: 35.65574\n",
      "iter: 2375, grad: 0.01803, loss_in_train: 2.15280, loss_in_test: 35.65574\n",
      "iter: 2376, grad: 0.01802, loss_in_train: 2.15280, loss_in_test: 35.65575\n",
      "iter: 2377, grad: 0.01801, loss_in_train: 2.15280, loss_in_test: 35.65575\n",
      "iter: 2378, grad: 0.01801, loss_in_train: 2.15280, loss_in_test: 35.65576\n",
      "iter: 2379, grad: 0.01800, loss_in_train: 2.15280, loss_in_test: 35.65576\n",
      "iter: 2380, grad: 0.01800, loss_in_train: 2.15280, loss_in_test: 35.65576\n",
      "iter: 2381, grad: 0.01799, loss_in_train: 2.15280, loss_in_test: 35.65577\n",
      "iter: 2382, grad: 0.01798, loss_in_train: 2.15280, loss_in_test: 35.65577\n",
      "iter: 2383, grad: 0.01798, loss_in_train: 2.15280, loss_in_test: 35.65578\n",
      "iter: 2384, grad: 0.01797, loss_in_train: 2.15280, loss_in_test: 35.65578\n",
      "iter: 2385, grad: 0.01797, loss_in_train: 2.15280, loss_in_test: 35.65578\n",
      "iter: 2386, grad: 0.01796, loss_in_train: 2.15280, loss_in_test: 35.65579\n",
      "iter: 2387, grad: 0.01796, loss_in_train: 2.15280, loss_in_test: 35.65579\n",
      "iter: 2388, grad: 0.01795, loss_in_train: 2.15280, loss_in_test: 35.65580\n",
      "iter: 2389, grad: 0.01795, loss_in_train: 2.15280, loss_in_test: 35.65580\n",
      "iter: 2390, grad: 0.01794, loss_in_train: 2.15280, loss_in_test: 35.65580\n",
      "iter: 2391, grad: 0.01793, loss_in_train: 2.15280, loss_in_test: 35.65581\n",
      "iter: 2392, grad: 0.01793, loss_in_train: 2.15280, loss_in_test: 35.65581\n",
      "iter: 2393, grad: 0.01792, loss_in_train: 2.15280, loss_in_test: 35.65582\n",
      "iter: 2394, grad: 0.01792, loss_in_train: 2.15280, loss_in_test: 35.65582\n",
      "iter: 2395, grad: 0.01791, loss_in_train: 2.15280, loss_in_test: 35.65582\n",
      "iter: 2396, grad: 0.01791, loss_in_train: 2.15280, loss_in_test: 35.65583\n",
      "iter: 2397, grad: 0.01790, loss_in_train: 2.15280, loss_in_test: 35.65583\n",
      "iter: 2398, grad: 0.01790, loss_in_train: 2.15280, loss_in_test: 35.65584\n",
      "iter: 2399, grad: 0.01789, loss_in_train: 2.15280, loss_in_test: 35.65584\n",
      "iter: 2400, grad: 0.01788, loss_in_train: 2.15280, loss_in_test: 35.65584\n",
      "iter: 2401, grad: 0.01788, loss_in_train: 2.15280, loss_in_test: 35.65585\n",
      "iter: 2402, grad: 0.01787, loss_in_train: 2.15280, loss_in_test: 35.65585\n",
      "iter: 2403, grad: 0.01787, loss_in_train: 2.15280, loss_in_test: 35.65586\n",
      "iter: 2404, grad: 0.01786, loss_in_train: 2.15280, loss_in_test: 35.65586\n",
      "iter: 2405, grad: 0.01786, loss_in_train: 2.15280, loss_in_test: 35.65586\n",
      "iter: 2406, grad: 0.01785, loss_in_train: 2.15280, loss_in_test: 35.65587\n",
      "iter: 2407, grad: 0.01785, loss_in_train: 2.15280, loss_in_test: 35.65587\n",
      "iter: 2408, grad: 0.01784, loss_in_train: 2.15280, loss_in_test: 35.65588\n",
      "iter: 2409, grad: 0.01784, loss_in_train: 2.15280, loss_in_test: 35.65588\n",
      "iter: 2410, grad: 0.01783, loss_in_train: 2.15280, loss_in_test: 35.65588\n",
      "iter: 2411, grad: 0.01782, loss_in_train: 2.15280, loss_in_test: 35.65589\n",
      "iter: 2412, grad: 0.01782, loss_in_train: 2.15280, loss_in_test: 35.65589\n",
      "iter: 2413, grad: 0.01781, loss_in_train: 2.15280, loss_in_test: 35.65589\n",
      "iter: 2414, grad: 0.01781, loss_in_train: 2.15280, loss_in_test: 35.65590\n",
      "iter: 2415, grad: 0.01780, loss_in_train: 2.15280, loss_in_test: 35.65590\n",
      "iter: 2416, grad: 0.01780, loss_in_train: 2.15280, loss_in_test: 35.65591\n",
      "iter: 2417, grad: 0.01779, loss_in_train: 2.15280, loss_in_test: 35.65591\n",
      "iter: 2418, grad: 0.01779, loss_in_train: 2.15280, loss_in_test: 35.65591\n",
      "iter: 2419, grad: 0.01778, loss_in_train: 2.15280, loss_in_test: 35.65592\n",
      "iter: 2420, grad: 0.01778, loss_in_train: 2.15280, loss_in_test: 35.65592\n",
      "iter: 2421, grad: 0.01777, loss_in_train: 2.15280, loss_in_test: 35.65592\n",
      "iter: 2422, grad: 0.01777, loss_in_train: 2.15280, loss_in_test: 35.65593\n",
      "iter: 2423, grad: 0.01776, loss_in_train: 2.15280, loss_in_test: 35.65593\n",
      "iter: 2424, grad: 0.01776, loss_in_train: 2.15280, loss_in_test: 35.65594\n",
      "iter: 2425, grad: 0.01775, loss_in_train: 2.15280, loss_in_test: 35.65594\n",
      "iter: 2426, grad: 0.01775, loss_in_train: 2.15280, loss_in_test: 35.65594\n",
      "iter: 2427, grad: 0.01774, loss_in_train: 2.15280, loss_in_test: 35.65595\n",
      "iter: 2428, grad: 0.01774, loss_in_train: 2.15280, loss_in_test: 35.65595\n",
      "iter: 2429, grad: 0.01773, loss_in_train: 2.15280, loss_in_test: 35.65595\n",
      "iter: 2430, grad: 0.01773, loss_in_train: 2.15280, loss_in_test: 35.65596\n",
      "iter: 2431, grad: 0.01772, loss_in_train: 2.15280, loss_in_test: 35.65596\n",
      "iter: 2432, grad: 0.01772, loss_in_train: 2.15280, loss_in_test: 35.65597\n",
      "iter: 2433, grad: 0.01771, loss_in_train: 2.15280, loss_in_test: 35.65597\n",
      "iter: 2434, grad: 0.01771, loss_in_train: 2.15280, loss_in_test: 35.65597\n",
      "iter: 2435, grad: 0.01770, loss_in_train: 2.15280, loss_in_test: 35.65598\n",
      "iter: 2436, grad: 0.01770, loss_in_train: 2.15280, loss_in_test: 35.65598\n",
      "iter: 2437, grad: 0.01769, loss_in_train: 2.15280, loss_in_test: 35.65598\n",
      "iter: 2438, grad: 0.01769, loss_in_train: 2.15280, loss_in_test: 35.65599\n",
      "iter: 2439, grad: 0.01768, loss_in_train: 2.15280, loss_in_test: 35.65599\n",
      "iter: 2440, grad: 0.01768, loss_in_train: 2.15280, loss_in_test: 35.65599\n",
      "iter: 2441, grad: 0.01767, loss_in_train: 2.15280, loss_in_test: 35.65600\n",
      "iter: 2442, grad: 0.01767, loss_in_train: 2.15280, loss_in_test: 35.65600\n",
      "iter: 2443, grad: 0.01766, loss_in_train: 2.15280, loss_in_test: 35.65601\n",
      "iter: 2444, grad: 0.01766, loss_in_train: 2.15280, loss_in_test: 35.65601\n",
      "iter: 2445, grad: 0.01765, loss_in_train: 2.15280, loss_in_test: 35.65601\n",
      "iter: 2446, grad: 0.01765, loss_in_train: 2.15280, loss_in_test: 35.65602\n",
      "iter: 2447, grad: 0.01764, loss_in_train: 2.15280, loss_in_test: 35.65602\n",
      "iter: 2448, grad: 0.01764, loss_in_train: 2.15280, loss_in_test: 35.65602\n",
      "iter: 2449, grad: 0.01763, loss_in_train: 2.15280, loss_in_test: 35.65603\n",
      "iter: 2450, grad: 0.01763, loss_in_train: 2.15280, loss_in_test: 35.65603\n",
      "iter: 2451, grad: 0.01762, loss_in_train: 2.15280, loss_in_test: 35.65603\n",
      "iter: 2452, grad: 0.01762, loss_in_train: 2.15280, loss_in_test: 35.65604\n",
      "iter: 2453, grad: 0.01761, loss_in_train: 2.15280, loss_in_test: 35.65604\n",
      "iter: 2454, grad: 0.01761, loss_in_train: 2.15280, loss_in_test: 35.65604\n",
      "iter: 2455, grad: 0.01760, loss_in_train: 2.15280, loss_in_test: 35.65605\n",
      "iter: 2456, grad: 0.01760, loss_in_train: 2.15280, loss_in_test: 35.65605\n",
      "iter: 2457, grad: 0.01759, loss_in_train: 2.15280, loss_in_test: 35.65605\n",
      "iter: 2458, grad: 0.01759, loss_in_train: 2.15280, loss_in_test: 35.65606\n",
      "iter: 2459, grad: 0.01758, loss_in_train: 2.15280, loss_in_test: 35.65606\n",
      "iter: 2460, grad: 0.01758, loss_in_train: 2.15280, loss_in_test: 35.65606\n",
      "iter: 2461, grad: 0.01757, loss_in_train: 2.15280, loss_in_test: 35.65607\n",
      "iter: 2462, grad: 0.01757, loss_in_train: 2.15280, loss_in_test: 35.65607\n",
      "iter: 2463, grad: 0.01757, loss_in_train: 2.15280, loss_in_test: 35.65607\n",
      "iter: 2464, grad: 0.01756, loss_in_train: 2.15280, loss_in_test: 35.65608\n",
      "iter: 2465, grad: 0.01756, loss_in_train: 2.15280, loss_in_test: 35.65608\n",
      "iter: 2466, grad: 0.01755, loss_in_train: 2.15280, loss_in_test: 35.65608\n",
      "iter: 2467, grad: 0.01755, loss_in_train: 2.15280, loss_in_test: 35.65609\n",
      "iter: 2468, grad: 0.01754, loss_in_train: 2.15280, loss_in_test: 35.65609\n",
      "iter: 2469, grad: 0.01754, loss_in_train: 2.15280, loss_in_test: 35.65609\n",
      "iter: 2470, grad: 0.01753, loss_in_train: 2.15280, loss_in_test: 35.65610\n",
      "iter: 2471, grad: 0.01753, loss_in_train: 2.15280, loss_in_test: 35.65610\n",
      "iter: 2472, grad: 0.01752, loss_in_train: 2.15280, loss_in_test: 35.65610\n",
      "iter: 2473, grad: 0.01752, loss_in_train: 2.15280, loss_in_test: 35.65611\n",
      "iter: 2474, grad: 0.01751, loss_in_train: 2.15280, loss_in_test: 35.65611\n",
      "iter: 2475, grad: 0.01751, loss_in_train: 2.15280, loss_in_test: 35.65611\n",
      "iter: 2476, grad: 0.01751, loss_in_train: 2.15280, loss_in_test: 35.65612\n",
      "iter: 2477, grad: 0.01750, loss_in_train: 2.15280, loss_in_test: 35.65612\n",
      "iter: 2478, grad: 0.01750, loss_in_train: 2.15280, loss_in_test: 35.65612\n",
      "iter: 2479, grad: 0.01749, loss_in_train: 2.15280, loss_in_test: 35.65613\n",
      "iter: 2480, grad: 0.01749, loss_in_train: 2.15280, loss_in_test: 35.65613\n",
      "iter: 2481, grad: 0.01748, loss_in_train: 2.15280, loss_in_test: 35.65613\n",
      "iter: 2482, grad: 0.01748, loss_in_train: 2.15280, loss_in_test: 35.65614\n",
      "iter: 2483, grad: 0.01747, loss_in_train: 2.15280, loss_in_test: 35.65614\n",
      "iter: 2484, grad: 0.01747, loss_in_train: 2.15280, loss_in_test: 35.65614\n",
      "iter: 2485, grad: 0.01747, loss_in_train: 2.15280, loss_in_test: 35.65615\n",
      "iter: 2486, grad: 0.01746, loss_in_train: 2.15280, loss_in_test: 35.65615\n",
      "iter: 2487, grad: 0.01746, loss_in_train: 2.15280, loss_in_test: 35.65615\n",
      "iter: 2488, grad: 0.01745, loss_in_train: 2.15280, loss_in_test: 35.65616\n",
      "iter: 2489, grad: 0.01745, loss_in_train: 2.15280, loss_in_test: 35.65616\n",
      "iter: 2490, grad: 0.01744, loss_in_train: 2.15280, loss_in_test: 35.65616\n",
      "iter: 2491, grad: 0.01744, loss_in_train: 2.15280, loss_in_test: 35.65617\n",
      "iter: 2492, grad: 0.01744, loss_in_train: 2.15280, loss_in_test: 35.65617\n",
      "iter: 2493, grad: 0.01743, loss_in_train: 2.15280, loss_in_test: 35.65617\n",
      "iter: 2494, grad: 0.01743, loss_in_train: 2.15280, loss_in_test: 35.65617\n",
      "iter: 2495, grad: 0.01742, loss_in_train: 2.15280, loss_in_test: 35.65618\n",
      "iter: 2496, grad: 0.01742, loss_in_train: 2.15280, loss_in_test: 35.65618\n",
      "iter: 2497, grad: 0.01741, loss_in_train: 2.15280, loss_in_test: 35.65618\n",
      "iter: 2498, grad: 0.01741, loss_in_train: 2.15280, loss_in_test: 35.65619\n",
      "iter: 2499, grad: 0.01740, loss_in_train: 2.15280, loss_in_test: 35.65619\n",
      "iter: 2500, grad: 0.01740, loss_in_train: 2.15280, loss_in_test: 35.65619\n",
      "iter: 2501, grad: 0.01740, loss_in_train: 2.15280, loss_in_test: 35.65620\n",
      "iter: 2502, grad: 0.01739, loss_in_train: 2.15280, loss_in_test: 35.65620\n",
      "iter: 2503, grad: 0.01739, loss_in_train: 2.15280, loss_in_test: 35.65620\n",
      "iter: 2504, grad: 0.01738, loss_in_train: 2.15280, loss_in_test: 35.65621\n",
      "iter: 2505, grad: 0.01738, loss_in_train: 2.15280, loss_in_test: 35.65621\n",
      "iter: 2506, grad: 0.01738, loss_in_train: 2.15280, loss_in_test: 35.65621\n",
      "iter: 2507, grad: 0.01737, loss_in_train: 2.15280, loss_in_test: 35.65622\n",
      "iter: 2508, grad: 0.01737, loss_in_train: 2.15280, loss_in_test: 35.65622\n",
      "iter: 2509, grad: 0.01736, loss_in_train: 2.15280, loss_in_test: 35.65622\n",
      "iter: 2510, grad: 0.01736, loss_in_train: 2.15280, loss_in_test: 35.65622\n",
      "iter: 2511, grad: 0.01735, loss_in_train: 2.15280, loss_in_test: 35.65623\n",
      "iter: 2512, grad: 0.01735, loss_in_train: 2.15280, loss_in_test: 35.65623\n",
      "iter: 2513, grad: 0.01735, loss_in_train: 2.15280, loss_in_test: 35.65623\n",
      "iter: 2514, grad: 0.01734, loss_in_train: 2.15280, loss_in_test: 35.65624\n",
      "iter: 2515, grad: 0.01734, loss_in_train: 2.15280, loss_in_test: 35.65624\n",
      "iter: 2516, grad: 0.01733, loss_in_train: 2.15280, loss_in_test: 35.65624\n",
      "iter: 2517, grad: 0.01733, loss_in_train: 2.15280, loss_in_test: 35.65624\n",
      "iter: 2518, grad: 0.01733, loss_in_train: 2.15280, loss_in_test: 35.65625\n",
      "iter: 2519, grad: 0.01732, loss_in_train: 2.15280, loss_in_test: 35.65625\n",
      "iter: 2520, grad: 0.01732, loss_in_train: 2.15280, loss_in_test: 35.65625\n",
      "iter: 2521, grad: 0.01731, loss_in_train: 2.15280, loss_in_test: 35.65626\n",
      "iter: 2522, grad: 0.01731, loss_in_train: 2.15280, loss_in_test: 35.65626\n",
      "iter: 2523, grad: 0.01731, loss_in_train: 2.15280, loss_in_test: 35.65626\n",
      "iter: 2524, grad: 0.01730, loss_in_train: 2.15280, loss_in_test: 35.65627\n",
      "iter: 2525, grad: 0.01730, loss_in_train: 2.15280, loss_in_test: 35.65627\n",
      "iter: 2526, grad: 0.01729, loss_in_train: 2.15280, loss_in_test: 35.65627\n",
      "iter: 2527, grad: 0.01729, loss_in_train: 2.15280, loss_in_test: 35.65627\n",
      "iter: 2528, grad: 0.01729, loss_in_train: 2.15280, loss_in_test: 35.65628\n",
      "iter: 2529, grad: 0.01728, loss_in_train: 2.15280, loss_in_test: 35.65628\n",
      "iter: 2530, grad: 0.01728, loss_in_train: 2.15280, loss_in_test: 35.65628\n",
      "iter: 2531, grad: 0.01727, loss_in_train: 2.15280, loss_in_test: 35.65629\n",
      "iter: 2532, grad: 0.01727, loss_in_train: 2.15280, loss_in_test: 35.65629\n",
      "iter: 2533, grad: 0.01727, loss_in_train: 2.15280, loss_in_test: 35.65629\n",
      "iter: 2534, grad: 0.01726, loss_in_train: 2.15280, loss_in_test: 35.65629\n",
      "iter: 2535, grad: 0.01726, loss_in_train: 2.15280, loss_in_test: 35.65630\n",
      "iter: 2536, grad: 0.01725, loss_in_train: 2.15280, loss_in_test: 35.65630\n",
      "iter: 2537, grad: 0.01725, loss_in_train: 2.15280, loss_in_test: 35.65630\n",
      "iter: 2538, grad: 0.01725, loss_in_train: 2.15280, loss_in_test: 35.65631\n",
      "iter: 2539, grad: 0.01724, loss_in_train: 2.15280, loss_in_test: 35.65631\n",
      "iter: 2540, grad: 0.01724, loss_in_train: 2.15280, loss_in_test: 35.65631\n",
      "iter: 2541, grad: 0.01723, loss_in_train: 2.15280, loss_in_test: 35.65631\n",
      "iter: 2542, grad: 0.01723, loss_in_train: 2.15280, loss_in_test: 35.65632\n",
      "iter: 2543, grad: 0.01723, loss_in_train: 2.15280, loss_in_test: 35.65632\n",
      "iter: 2544, grad: 0.01722, loss_in_train: 2.15280, loss_in_test: 35.65632\n",
      "iter: 2545, grad: 0.01722, loss_in_train: 2.15280, loss_in_test: 35.65632\n",
      "iter: 2546, grad: 0.01722, loss_in_train: 2.15280, loss_in_test: 35.65633\n",
      "iter: 2547, grad: 0.01721, loss_in_train: 2.15280, loss_in_test: 35.65633\n",
      "iter: 2548, grad: 0.01721, loss_in_train: 2.15280, loss_in_test: 35.65633\n",
      "iter: 2549, grad: 0.01720, loss_in_train: 2.15280, loss_in_test: 35.65634\n",
      "iter: 2550, grad: 0.01720, loss_in_train: 2.15280, loss_in_test: 35.65634\n",
      "iter: 2551, grad: 0.01720, loss_in_train: 2.15280, loss_in_test: 35.65634\n",
      "iter: 2552, grad: 0.01719, loss_in_train: 2.15280, loss_in_test: 35.65634\n",
      "iter: 2553, grad: 0.01719, loss_in_train: 2.15280, loss_in_test: 35.65635\n",
      "iter: 2554, grad: 0.01719, loss_in_train: 2.15280, loss_in_test: 35.65635\n",
      "iter: 2555, grad: 0.01718, loss_in_train: 2.15280, loss_in_test: 35.65635\n",
      "iter: 2556, grad: 0.01718, loss_in_train: 2.15280, loss_in_test: 35.65635\n",
      "iter: 2557, grad: 0.01717, loss_in_train: 2.15280, loss_in_test: 35.65636\n",
      "iter: 2558, grad: 0.01717, loss_in_train: 2.15280, loss_in_test: 35.65636\n",
      "iter: 2559, grad: 0.01717, loss_in_train: 2.15280, loss_in_test: 35.65636\n",
      "iter: 2560, grad: 0.01716, loss_in_train: 2.15280, loss_in_test: 35.65637\n",
      "iter: 2561, grad: 0.01716, loss_in_train: 2.15280, loss_in_test: 35.65637\n",
      "iter: 2562, grad: 0.01716, loss_in_train: 2.15280, loss_in_test: 35.65637\n",
      "iter: 2563, grad: 0.01715, loss_in_train: 2.15280, loss_in_test: 35.65637\n",
      "iter: 2564, grad: 0.01715, loss_in_train: 2.15280, loss_in_test: 35.65638\n",
      "iter: 2565, grad: 0.01715, loss_in_train: 2.15280, loss_in_test: 35.65638\n",
      "iter: 2566, grad: 0.01714, loss_in_train: 2.15280, loss_in_test: 35.65638\n",
      "iter: 2567, grad: 0.01714, loss_in_train: 2.15280, loss_in_test: 35.65638\n",
      "iter: 2568, grad: 0.01713, loss_in_train: 2.15280, loss_in_test: 35.65639\n",
      "iter: 2569, grad: 0.01713, loss_in_train: 2.15280, loss_in_test: 35.65639\n",
      "iter: 2570, grad: 0.01713, loss_in_train: 2.15280, loss_in_test: 35.65639\n",
      "iter: 2571, grad: 0.01712, loss_in_train: 2.15280, loss_in_test: 35.65639\n",
      "iter: 2572, grad: 0.01712, loss_in_train: 2.15280, loss_in_test: 35.65640\n",
      "iter: 2573, grad: 0.01712, loss_in_train: 2.15280, loss_in_test: 35.65640\n",
      "iter: 2574, grad: 0.01711, loss_in_train: 2.15280, loss_in_test: 35.65640\n",
      "iter: 2575, grad: 0.01711, loss_in_train: 2.15280, loss_in_test: 35.65640\n",
      "iter: 2576, grad: 0.01711, loss_in_train: 2.15280, loss_in_test: 35.65641\n",
      "iter: 2577, grad: 0.01710, loss_in_train: 2.15280, loss_in_test: 35.65641\n",
      "iter: 2578, grad: 0.01710, loss_in_train: 2.15280, loss_in_test: 35.65641\n",
      "iter: 2579, grad: 0.01710, loss_in_train: 2.15280, loss_in_test: 35.65642\n",
      "iter: 2580, grad: 0.01709, loss_in_train: 2.15280, loss_in_test: 35.65642\n",
      "iter: 2581, grad: 0.01709, loss_in_train: 2.15280, loss_in_test: 35.65642\n",
      "iter: 2582, grad: 0.01709, loss_in_train: 2.15280, loss_in_test: 35.65642\n",
      "iter: 2583, grad: 0.01708, loss_in_train: 2.15280, loss_in_test: 35.65643\n",
      "iter: 2584, grad: 0.01708, loss_in_train: 2.15280, loss_in_test: 35.65643\n",
      "iter: 2585, grad: 0.01707, loss_in_train: 2.15280, loss_in_test: 35.65643\n",
      "iter: 2586, grad: 0.01707, loss_in_train: 2.15280, loss_in_test: 35.65643\n",
      "iter: 2587, grad: 0.01707, loss_in_train: 2.15280, loss_in_test: 35.65644\n",
      "iter: 2588, grad: 0.01706, loss_in_train: 2.15280, loss_in_test: 35.65644\n",
      "iter: 2589, grad: 0.01706, loss_in_train: 2.15280, loss_in_test: 35.65644\n",
      "iter: 2590, grad: 0.01706, loss_in_train: 2.15280, loss_in_test: 35.65644\n",
      "iter: 2591, grad: 0.01705, loss_in_train: 2.15280, loss_in_test: 35.65645\n",
      "iter: 2592, grad: 0.01705, loss_in_train: 2.15280, loss_in_test: 35.65645\n",
      "iter: 2593, grad: 0.01705, loss_in_train: 2.15280, loss_in_test: 35.65645\n",
      "iter: 2594, grad: 0.01704, loss_in_train: 2.15280, loss_in_test: 35.65645\n",
      "iter: 2595, grad: 0.01704, loss_in_train: 2.15280, loss_in_test: 35.65645\n",
      "iter: 2596, grad: 0.01704, loss_in_train: 2.15280, loss_in_test: 35.65646\n",
      "iter: 2597, grad: 0.01703, loss_in_train: 2.15280, loss_in_test: 35.65646\n",
      "iter: 2598, grad: 0.01703, loss_in_train: 2.15280, loss_in_test: 35.65646\n",
      "iter: 2599, grad: 0.01703, loss_in_train: 2.15280, loss_in_test: 35.65646\n",
      "iter: 2600, grad: 0.01702, loss_in_train: 2.15280, loss_in_test: 35.65647\n",
      "iter: 2601, grad: 0.01702, loss_in_train: 2.15280, loss_in_test: 35.65647\n",
      "iter: 2602, grad: 0.01702, loss_in_train: 2.15280, loss_in_test: 35.65647\n",
      "iter: 2603, grad: 0.01701, loss_in_train: 2.15280, loss_in_test: 35.65647\n",
      "iter: 2604, grad: 0.01701, loss_in_train: 2.15280, loss_in_test: 35.65648\n",
      "iter: 2605, grad: 0.01701, loss_in_train: 2.15280, loss_in_test: 35.65648\n",
      "iter: 2606, grad: 0.01700, loss_in_train: 2.15280, loss_in_test: 35.65648\n",
      "iter: 2607, grad: 0.01700, loss_in_train: 2.15280, loss_in_test: 35.65648\n",
      "iter: 2608, grad: 0.01700, loss_in_train: 2.15280, loss_in_test: 35.65649\n",
      "iter: 2609, grad: 0.01699, loss_in_train: 2.15280, loss_in_test: 35.65649\n",
      "iter: 2610, grad: 0.01699, loss_in_train: 2.15280, loss_in_test: 35.65649\n",
      "iter: 2611, grad: 0.01699, loss_in_train: 2.15280, loss_in_test: 35.65649\n",
      "iter: 2612, grad: 0.01699, loss_in_train: 2.15280, loss_in_test: 35.65650\n",
      "iter: 2613, grad: 0.01698, loss_in_train: 2.15280, loss_in_test: 35.65650\n",
      "iter: 2614, grad: 0.01698, loss_in_train: 2.15280, loss_in_test: 35.65650\n",
      "iter: 2615, grad: 0.01698, loss_in_train: 2.15280, loss_in_test: 35.65650\n",
      "iter: 2616, grad: 0.01697, loss_in_train: 2.15280, loss_in_test: 35.65650\n",
      "iter: 2617, grad: 0.01697, loss_in_train: 2.15280, loss_in_test: 35.65651\n",
      "iter: 2618, grad: 0.01697, loss_in_train: 2.15280, loss_in_test: 35.65651\n",
      "iter: 2619, grad: 0.01696, loss_in_train: 2.15280, loss_in_test: 35.65651\n",
      "iter: 2620, grad: 0.01696, loss_in_train: 2.15280, loss_in_test: 35.65651\n",
      "iter: 2621, grad: 0.01696, loss_in_train: 2.15280, loss_in_test: 35.65652\n",
      "iter: 2622, grad: 0.01695, loss_in_train: 2.15280, loss_in_test: 35.65652\n",
      "iter: 2623, grad: 0.01695, loss_in_train: 2.15280, loss_in_test: 35.65652\n",
      "iter: 2624, grad: 0.01695, loss_in_train: 2.15280, loss_in_test: 35.65652\n",
      "iter: 2625, grad: 0.01694, loss_in_train: 2.15280, loss_in_test: 35.65653\n",
      "iter: 2626, grad: 0.01694, loss_in_train: 2.15280, loss_in_test: 35.65653\n",
      "iter: 2627, grad: 0.01694, loss_in_train: 2.15280, loss_in_test: 35.65653\n",
      "iter: 2628, grad: 0.01693, loss_in_train: 2.15280, loss_in_test: 35.65653\n",
      "iter: 2629, grad: 0.01693, loss_in_train: 2.15280, loss_in_test: 35.65653\n",
      "iter: 2630, grad: 0.01693, loss_in_train: 2.15280, loss_in_test: 35.65654\n",
      "iter: 2631, grad: 0.01693, loss_in_train: 2.15280, loss_in_test: 35.65654\n",
      "iter: 2632, grad: 0.01692, loss_in_train: 2.15280, loss_in_test: 35.65654\n",
      "iter: 2633, grad: 0.01692, loss_in_train: 2.15280, loss_in_test: 35.65654\n",
      "iter: 2634, grad: 0.01692, loss_in_train: 2.15280, loss_in_test: 35.65655\n",
      "iter: 2635, grad: 0.01691, loss_in_train: 2.15280, loss_in_test: 35.65655\n",
      "iter: 2636, grad: 0.01691, loss_in_train: 2.15280, loss_in_test: 35.65655\n",
      "iter: 2637, grad: 0.01691, loss_in_train: 2.15280, loss_in_test: 35.65655\n",
      "iter: 2638, grad: 0.01690, loss_in_train: 2.15280, loss_in_test: 35.65655\n",
      "iter: 2639, grad: 0.01690, loss_in_train: 2.15280, loss_in_test: 35.65656\n",
      "iter: 2640, grad: 0.01690, loss_in_train: 2.15280, loss_in_test: 35.65656\n",
      "iter: 2641, grad: 0.01690, loss_in_train: 2.15280, loss_in_test: 35.65656\n",
      "iter: 2642, grad: 0.01689, loss_in_train: 2.15280, loss_in_test: 35.65656\n",
      "iter: 2643, grad: 0.01689, loss_in_train: 2.15280, loss_in_test: 35.65657\n",
      "iter: 2644, grad: 0.01689, loss_in_train: 2.15280, loss_in_test: 35.65657\n",
      "iter: 2645, grad: 0.01688, loss_in_train: 2.15280, loss_in_test: 35.65657\n",
      "iter: 2646, grad: 0.01688, loss_in_train: 2.15280, loss_in_test: 35.65657\n",
      "iter: 2647, grad: 0.01688, loss_in_train: 2.15280, loss_in_test: 35.65657\n",
      "iter: 2648, grad: 0.01687, loss_in_train: 2.15280, loss_in_test: 35.65658\n",
      "iter: 2649, grad: 0.01687, loss_in_train: 2.15280, loss_in_test: 35.65658\n",
      "iter: 2650, grad: 0.01687, loss_in_train: 2.15280, loss_in_test: 35.65658\n",
      "iter: 2651, grad: 0.01687, loss_in_train: 2.15280, loss_in_test: 35.65658\n",
      "iter: 2652, grad: 0.01686, loss_in_train: 2.15280, loss_in_test: 35.65658\n",
      "iter: 2653, grad: 0.01686, loss_in_train: 2.15280, loss_in_test: 35.65659\n",
      "iter: 2654, grad: 0.01686, loss_in_train: 2.15280, loss_in_test: 35.65659\n",
      "iter: 2655, grad: 0.01685, loss_in_train: 2.15280, loss_in_test: 35.65659\n",
      "iter: 2656, grad: 0.01685, loss_in_train: 2.15280, loss_in_test: 35.65659\n",
      "iter: 2657, grad: 0.01685, loss_in_train: 2.15280, loss_in_test: 35.65660\n",
      "iter: 2658, grad: 0.01685, loss_in_train: 2.15280, loss_in_test: 35.65660\n",
      "iter: 2659, grad: 0.01684, loss_in_train: 2.15280, loss_in_test: 35.65660\n",
      "iter: 2660, grad: 0.01684, loss_in_train: 2.15280, loss_in_test: 35.65660\n",
      "iter: 2661, grad: 0.01684, loss_in_train: 2.15280, loss_in_test: 35.65660\n",
      "iter: 2662, grad: 0.01683, loss_in_train: 2.15280, loss_in_test: 35.65661\n",
      "iter: 2663, grad: 0.01683, loss_in_train: 2.15280, loss_in_test: 35.65661\n",
      "iter: 2664, grad: 0.01683, loss_in_train: 2.15280, loss_in_test: 35.65661\n",
      "iter: 2665, grad: 0.01683, loss_in_train: 2.15280, loss_in_test: 35.65661\n",
      "iter: 2666, grad: 0.01682, loss_in_train: 2.15280, loss_in_test: 35.65661\n",
      "iter: 2667, grad: 0.01682, loss_in_train: 2.15280, loss_in_test: 35.65662\n",
      "iter: 2668, grad: 0.01682, loss_in_train: 2.15280, loss_in_test: 35.65662\n",
      "iter: 2669, grad: 0.01681, loss_in_train: 2.15280, loss_in_test: 35.65662\n",
      "iter: 2670, grad: 0.01681, loss_in_train: 2.15280, loss_in_test: 35.65662\n",
      "iter: 2671, grad: 0.01681, loss_in_train: 2.15280, loss_in_test: 35.65662\n",
      "iter: 2672, grad: 0.01681, loss_in_train: 2.15280, loss_in_test: 35.65663\n",
      "iter: 2673, grad: 0.01680, loss_in_train: 2.15280, loss_in_test: 35.65663\n",
      "iter: 2674, grad: 0.01680, loss_in_train: 2.15280, loss_in_test: 35.65663\n",
      "iter: 2675, grad: 0.01680, loss_in_train: 2.15280, loss_in_test: 35.65663\n",
      "iter: 2676, grad: 0.01680, loss_in_train: 2.15280, loss_in_test: 35.65663\n",
      "iter: 2677, grad: 0.01679, loss_in_train: 2.15280, loss_in_test: 35.65664\n",
      "iter: 2678, grad: 0.01679, loss_in_train: 2.15280, loss_in_test: 35.65664\n",
      "iter: 2679, grad: 0.01679, loss_in_train: 2.15280, loss_in_test: 35.65664\n",
      "iter: 2680, grad: 0.01678, loss_in_train: 2.15280, loss_in_test: 35.65664\n",
      "iter: 2681, grad: 0.01678, loss_in_train: 2.15280, loss_in_test: 35.65664\n",
      "iter: 2682, grad: 0.01678, loss_in_train: 2.15280, loss_in_test: 35.65665\n",
      "iter: 2683, grad: 0.01678, loss_in_train: 2.15280, loss_in_test: 35.65665\n",
      "iter: 2684, grad: 0.01677, loss_in_train: 2.15280, loss_in_test: 35.65665\n",
      "iter: 2685, grad: 0.01677, loss_in_train: 2.15280, loss_in_test: 35.65665\n",
      "iter: 2686, grad: 0.01677, loss_in_train: 2.15280, loss_in_test: 35.65665\n",
      "iter: 2687, grad: 0.01677, loss_in_train: 2.15280, loss_in_test: 35.65666\n",
      "iter: 2688, grad: 0.01676, loss_in_train: 2.15280, loss_in_test: 35.65666\n",
      "iter: 2689, grad: 0.01676, loss_in_train: 2.15280, loss_in_test: 35.65666\n",
      "iter: 2690, grad: 0.01676, loss_in_train: 2.15280, loss_in_test: 35.65666\n",
      "iter: 2691, grad: 0.01675, loss_in_train: 2.15280, loss_in_test: 35.65666\n",
      "iter: 2692, grad: 0.01675, loss_in_train: 2.15280, loss_in_test: 35.65667\n",
      "iter: 2693, grad: 0.01675, loss_in_train: 2.15280, loss_in_test: 35.65667\n",
      "iter: 2694, grad: 0.01675, loss_in_train: 2.15280, loss_in_test: 35.65667\n",
      "iter: 2695, grad: 0.01674, loss_in_train: 2.15280, loss_in_test: 35.65667\n",
      "iter: 2696, grad: 0.01674, loss_in_train: 2.15280, loss_in_test: 35.65667\n",
      "iter: 2697, grad: 0.01674, loss_in_train: 2.15280, loss_in_test: 35.65668\n",
      "iter: 2698, grad: 0.01674, loss_in_train: 2.15280, loss_in_test: 35.65668\n",
      "iter: 2699, grad: 0.01673, loss_in_train: 2.15280, loss_in_test: 35.65668\n",
      "iter: 2700, grad: 0.01673, loss_in_train: 2.15280, loss_in_test: 35.65668\n",
      "iter: 2701, grad: 0.01673, loss_in_train: 2.15280, loss_in_test: 35.65668\n",
      "iter: 2702, grad: 0.01673, loss_in_train: 2.15280, loss_in_test: 35.65668\n",
      "iter: 2703, grad: 0.01672, loss_in_train: 2.15280, loss_in_test: 35.65669\n",
      "iter: 2704, grad: 0.01672, loss_in_train: 2.15280, loss_in_test: 35.65669\n",
      "iter: 2705, grad: 0.01672, loss_in_train: 2.15280, loss_in_test: 35.65669\n",
      "iter: 2706, grad: 0.01672, loss_in_train: 2.15280, loss_in_test: 35.65669\n",
      "iter: 2707, grad: 0.01671, loss_in_train: 2.15280, loss_in_test: 35.65669\n",
      "iter: 2708, grad: 0.01671, loss_in_train: 2.15280, loss_in_test: 35.65670\n",
      "iter: 2709, grad: 0.01671, loss_in_train: 2.15280, loss_in_test: 35.65670\n",
      "iter: 2710, grad: 0.01671, loss_in_train: 2.15280, loss_in_test: 35.65670\n",
      "iter: 2711, grad: 0.01670, loss_in_train: 2.15280, loss_in_test: 35.65670\n",
      "iter: 2712, grad: 0.01670, loss_in_train: 2.15280, loss_in_test: 35.65670\n",
      "iter: 2713, grad: 0.01670, loss_in_train: 2.15280, loss_in_test: 35.65670\n",
      "iter: 2714, grad: 0.01670, loss_in_train: 2.15280, loss_in_test: 35.65671\n",
      "iter: 2715, grad: 0.01669, loss_in_train: 2.15280, loss_in_test: 35.65671\n",
      "iter: 2716, grad: 0.01669, loss_in_train: 2.15280, loss_in_test: 35.65671\n",
      "iter: 2717, grad: 0.01669, loss_in_train: 2.15280, loss_in_test: 35.65671\n",
      "iter: 2718, grad: 0.01669, loss_in_train: 2.15280, loss_in_test: 35.65671\n",
      "iter: 2719, grad: 0.01668, loss_in_train: 2.15280, loss_in_test: 35.65672\n",
      "iter: 2720, grad: 0.01668, loss_in_train: 2.15280, loss_in_test: 35.65672\n",
      "iter: 2721, grad: 0.01668, loss_in_train: 2.15280, loss_in_test: 35.65672\n",
      "iter: 2722, grad: 0.01668, loss_in_train: 2.15280, loss_in_test: 35.65672\n",
      "iter: 2723, grad: 0.01667, loss_in_train: 2.15280, loss_in_test: 35.65672\n",
      "iter: 2724, grad: 0.01667, loss_in_train: 2.15280, loss_in_test: 35.65672\n",
      "iter: 2725, grad: 0.01667, loss_in_train: 2.15280, loss_in_test: 35.65673\n",
      "iter: 2726, grad: 0.01667, loss_in_train: 2.15280, loss_in_test: 35.65673\n",
      "iter: 2727, grad: 0.01666, loss_in_train: 2.15280, loss_in_test: 35.65673\n",
      "iter: 2728, grad: 0.01666, loss_in_train: 2.15280, loss_in_test: 35.65673\n",
      "iter: 2729, grad: 0.01666, loss_in_train: 2.15280, loss_in_test: 35.65673\n",
      "iter: 2730, grad: 0.01666, loss_in_train: 2.15280, loss_in_test: 35.65674\n",
      "iter: 2731, grad: 0.01665, loss_in_train: 2.15280, loss_in_test: 35.65674\n",
      "iter: 2732, grad: 0.01665, loss_in_train: 2.15280, loss_in_test: 35.65674\n",
      "iter: 2733, grad: 0.01665, loss_in_train: 2.15280, loss_in_test: 35.65674\n",
      "iter: 2734, grad: 0.01665, loss_in_train: 2.15280, loss_in_test: 35.65674\n",
      "iter: 2735, grad: 0.01664, loss_in_train: 2.15280, loss_in_test: 35.65674\n",
      "iter: 2736, grad: 0.01664, loss_in_train: 2.15280, loss_in_test: 35.65675\n",
      "iter: 2737, grad: 0.01664, loss_in_train: 2.15280, loss_in_test: 35.65675\n",
      "iter: 2738, grad: 0.01664, loss_in_train: 2.15280, loss_in_test: 35.65675\n",
      "iter: 2739, grad: 0.01664, loss_in_train: 2.15280, loss_in_test: 35.65675\n",
      "iter: 2740, grad: 0.01663, loss_in_train: 2.15280, loss_in_test: 35.65675\n",
      "iter: 2741, grad: 0.01663, loss_in_train: 2.15280, loss_in_test: 35.65675\n",
      "iter: 2742, grad: 0.01663, loss_in_train: 2.15280, loss_in_test: 35.65676\n",
      "iter: 2743, grad: 0.01663, loss_in_train: 2.15280, loss_in_test: 35.65676\n",
      "iter: 2744, grad: 0.01662, loss_in_train: 2.15280, loss_in_test: 35.65676\n",
      "iter: 2745, grad: 0.01662, loss_in_train: 2.15280, loss_in_test: 35.65676\n",
      "iter: 2746, grad: 0.01662, loss_in_train: 2.15280, loss_in_test: 35.65676\n",
      "iter: 2747, grad: 0.01662, loss_in_train: 2.15280, loss_in_test: 35.65676\n",
      "iter: 2748, grad: 0.01661, loss_in_train: 2.15280, loss_in_test: 35.65677\n",
      "iter: 2749, grad: 0.01661, loss_in_train: 2.15280, loss_in_test: 35.65677\n",
      "iter: 2750, grad: 0.01661, loss_in_train: 2.15280, loss_in_test: 35.65677\n",
      "iter: 2751, grad: 0.01661, loss_in_train: 2.15280, loss_in_test: 35.65677\n",
      "iter: 2752, grad: 0.01661, loss_in_train: 2.15280, loss_in_test: 35.65677\n",
      "iter: 2753, grad: 0.01660, loss_in_train: 2.15280, loss_in_test: 35.65677\n",
      "iter: 2754, grad: 0.01660, loss_in_train: 2.15280, loss_in_test: 35.65678\n",
      "iter: 2755, grad: 0.01660, loss_in_train: 2.15280, loss_in_test: 35.65678\n",
      "iter: 2756, grad: 0.01660, loss_in_train: 2.15280, loss_in_test: 35.65678\n",
      "iter: 2757, grad: 0.01659, loss_in_train: 2.15280, loss_in_test: 35.65678\n",
      "iter: 2758, grad: 0.01659, loss_in_train: 2.15280, loss_in_test: 35.65678\n",
      "iter: 2759, grad: 0.01659, loss_in_train: 2.15280, loss_in_test: 35.65678\n",
      "iter: 2760, grad: 0.01659, loss_in_train: 2.15280, loss_in_test: 35.65679\n",
      "iter: 2761, grad: 0.01659, loss_in_train: 2.15280, loss_in_test: 35.65679\n",
      "iter: 2762, grad: 0.01658, loss_in_train: 2.15280, loss_in_test: 35.65679\n",
      "iter: 2763, grad: 0.01658, loss_in_train: 2.15280, loss_in_test: 35.65679\n",
      "iter: 2764, grad: 0.01658, loss_in_train: 2.15280, loss_in_test: 35.65679\n",
      "iter: 2765, grad: 0.01658, loss_in_train: 2.15280, loss_in_test: 35.65679\n",
      "iter: 2766, grad: 0.01657, loss_in_train: 2.15280, loss_in_test: 35.65680\n",
      "iter: 2767, grad: 0.01657, loss_in_train: 2.15280, loss_in_test: 35.65680\n",
      "iter: 2768, grad: 0.01657, loss_in_train: 2.15280, loss_in_test: 35.65680\n",
      "iter: 2769, grad: 0.01657, loss_in_train: 2.15280, loss_in_test: 35.65680\n",
      "iter: 2770, grad: 0.01657, loss_in_train: 2.15280, loss_in_test: 35.65680\n",
      "iter: 2771, grad: 0.01656, loss_in_train: 2.15280, loss_in_test: 35.65680\n",
      "iter: 2772, grad: 0.01656, loss_in_train: 2.15280, loss_in_test: 35.65681\n",
      "iter: 2773, grad: 0.01656, loss_in_train: 2.15280, loss_in_test: 35.65681\n",
      "iter: 2774, grad: 0.01656, loss_in_train: 2.15280, loss_in_test: 35.65681\n",
      "iter: 2775, grad: 0.01655, loss_in_train: 2.15280, loss_in_test: 35.65681\n",
      "iter: 2776, grad: 0.01655, loss_in_train: 2.15280, loss_in_test: 35.65681\n",
      "iter: 2777, grad: 0.01655, loss_in_train: 2.15280, loss_in_test: 35.65681\n",
      "iter: 2778, grad: 0.01655, loss_in_train: 2.15280, loss_in_test: 35.65681\n",
      "iter: 2779, grad: 0.01655, loss_in_train: 2.15280, loss_in_test: 35.65682\n",
      "iter: 2780, grad: 0.01654, loss_in_train: 2.15280, loss_in_test: 35.65682\n",
      "iter: 2781, grad: 0.01654, loss_in_train: 2.15280, loss_in_test: 35.65682\n",
      "iter: 2782, grad: 0.01654, loss_in_train: 2.15280, loss_in_test: 35.65682\n",
      "iter: 2783, grad: 0.01654, loss_in_train: 2.15280, loss_in_test: 35.65682\n",
      "iter: 2784, grad: 0.01654, loss_in_train: 2.15280, loss_in_test: 35.65682\n",
      "iter: 2785, grad: 0.01653, loss_in_train: 2.15280, loss_in_test: 35.65683\n",
      "iter: 2786, grad: 0.01653, loss_in_train: 2.15280, loss_in_test: 35.65683\n",
      "iter: 2787, grad: 0.01653, loss_in_train: 2.15280, loss_in_test: 35.65683\n",
      "iter: 2788, grad: 0.01653, loss_in_train: 2.15280, loss_in_test: 35.65683\n",
      "iter: 2789, grad: 0.01653, loss_in_train: 2.15280, loss_in_test: 35.65683\n",
      "iter: 2790, grad: 0.01652, loss_in_train: 2.15280, loss_in_test: 35.65683\n",
      "iter: 2791, grad: 0.01652, loss_in_train: 2.15280, loss_in_test: 35.65683\n",
      "iter: 2792, grad: 0.01652, loss_in_train: 2.15280, loss_in_test: 35.65684\n",
      "iter: 2793, grad: 0.01652, loss_in_train: 2.15280, loss_in_test: 35.65684\n",
      "iter: 2794, grad: 0.01652, loss_in_train: 2.15280, loss_in_test: 35.65684\n",
      "iter: 2795, grad: 0.01651, loss_in_train: 2.15280, loss_in_test: 35.65684\n",
      "iter: 2796, grad: 0.01651, loss_in_train: 2.15280, loss_in_test: 35.65684\n",
      "iter: 2797, grad: 0.01651, loss_in_train: 2.15280, loss_in_test: 35.65684\n",
      "iter: 2798, grad: 0.01651, loss_in_train: 2.15280, loss_in_test: 35.65684\n",
      "iter: 2799, grad: 0.01651, loss_in_train: 2.15280, loss_in_test: 35.65685\n",
      "iter: 2800, grad: 0.01650, loss_in_train: 2.15280, loss_in_test: 35.65685\n",
      "iter: 2801, grad: 0.01650, loss_in_train: 2.15280, loss_in_test: 35.65685\n",
      "iter: 2802, grad: 0.01650, loss_in_train: 2.15280, loss_in_test: 35.65685\n",
      "iter: 2803, grad: 0.01650, loss_in_train: 2.15280, loss_in_test: 35.65685\n",
      "iter: 2804, grad: 0.01650, loss_in_train: 2.15280, loss_in_test: 35.65685\n",
      "iter: 2805, grad: 0.01649, loss_in_train: 2.15280, loss_in_test: 35.65686\n",
      "iter: 2806, grad: 0.01649, loss_in_train: 2.15280, loss_in_test: 35.65686\n",
      "iter: 2807, grad: 0.01649, loss_in_train: 2.15280, loss_in_test: 35.65686\n",
      "iter: 2808, grad: 0.01649, loss_in_train: 2.15280, loss_in_test: 35.65686\n",
      "iter: 2809, grad: 0.01649, loss_in_train: 2.15280, loss_in_test: 35.65686\n",
      "iter: 2810, grad: 0.01648, loss_in_train: 2.15280, loss_in_test: 35.65686\n",
      "iter: 2811, grad: 0.01648, loss_in_train: 2.15280, loss_in_test: 35.65686\n",
      "iter: 2812, grad: 0.01648, loss_in_train: 2.15280, loss_in_test: 35.65687\n",
      "iter: 2813, grad: 0.01648, loss_in_train: 2.15280, loss_in_test: 35.65687\n",
      "iter: 2814, grad: 0.01648, loss_in_train: 2.15280, loss_in_test: 35.65687\n",
      "iter: 2815, grad: 0.01647, loss_in_train: 2.15280, loss_in_test: 35.65687\n",
      "iter: 2816, grad: 0.01647, loss_in_train: 2.15280, loss_in_test: 35.65687\n",
      "iter: 2817, grad: 0.01647, loss_in_train: 2.15280, loss_in_test: 35.65687\n",
      "iter: 2818, grad: 0.01647, loss_in_train: 2.15280, loss_in_test: 35.65687\n",
      "iter: 2819, grad: 0.01647, loss_in_train: 2.15280, loss_in_test: 35.65688\n",
      "iter: 2820, grad: 0.01646, loss_in_train: 2.15280, loss_in_test: 35.65688\n",
      "iter: 2821, grad: 0.01646, loss_in_train: 2.15280, loss_in_test: 35.65688\n",
      "iter: 2822, grad: 0.01646, loss_in_train: 2.15280, loss_in_test: 35.65688\n",
      "iter: 2823, grad: 0.01646, loss_in_train: 2.15280, loss_in_test: 35.65688\n",
      "iter: 2824, grad: 0.01646, loss_in_train: 2.15280, loss_in_test: 35.65688\n",
      "iter: 2825, grad: 0.01645, loss_in_train: 2.15280, loss_in_test: 35.65688\n",
      "iter: 2826, grad: 0.01645, loss_in_train: 2.15280, loss_in_test: 35.65688\n",
      "iter: 2827, grad: 0.01645, loss_in_train: 2.15280, loss_in_test: 35.65689\n",
      "iter: 2828, grad: 0.01645, loss_in_train: 2.15280, loss_in_test: 35.65689\n",
      "iter: 2829, grad: 0.01645, loss_in_train: 2.15280, loss_in_test: 35.65689\n",
      "iter: 2830, grad: 0.01645, loss_in_train: 2.15280, loss_in_test: 35.65689\n",
      "iter: 2831, grad: 0.01644, loss_in_train: 2.15280, loss_in_test: 35.65689\n",
      "iter: 2832, grad: 0.01644, loss_in_train: 2.15280, loss_in_test: 35.65689\n",
      "iter: 2833, grad: 0.01644, loss_in_train: 2.15280, loss_in_test: 35.65689\n",
      "iter: 2834, grad: 0.01644, loss_in_train: 2.15280, loss_in_test: 35.65690\n",
      "iter: 2835, grad: 0.01644, loss_in_train: 2.15280, loss_in_test: 35.65690\n",
      "iter: 2836, grad: 0.01643, loss_in_train: 2.15280, loss_in_test: 35.65690\n",
      "iter: 2837, grad: 0.01643, loss_in_train: 2.15280, loss_in_test: 35.65690\n",
      "iter: 2838, grad: 0.01643, loss_in_train: 2.15280, loss_in_test: 35.65690\n",
      "iter: 2839, grad: 0.01643, loss_in_train: 2.15280, loss_in_test: 35.65690\n",
      "iter: 2840, grad: 0.01643, loss_in_train: 2.15280, loss_in_test: 35.65690\n",
      "iter: 2841, grad: 0.01642, loss_in_train: 2.15280, loss_in_test: 35.65691\n",
      "iter: 2842, grad: 0.01642, loss_in_train: 2.15280, loss_in_test: 35.65691\n",
      "iter: 2843, grad: 0.01642, loss_in_train: 2.15280, loss_in_test: 35.65691\n",
      "iter: 2844, grad: 0.01642, loss_in_train: 2.15280, loss_in_test: 35.65691\n",
      "iter: 2845, grad: 0.01642, loss_in_train: 2.15280, loss_in_test: 35.65691\n",
      "iter: 2846, grad: 0.01642, loss_in_train: 2.15280, loss_in_test: 35.65691\n",
      "iter: 2847, grad: 0.01641, loss_in_train: 2.15280, loss_in_test: 35.65691\n",
      "iter: 2848, grad: 0.01641, loss_in_train: 2.15280, loss_in_test: 35.65691\n",
      "iter: 2849, grad: 0.01641, loss_in_train: 2.15280, loss_in_test: 35.65692\n",
      "iter: 2850, grad: 0.01641, loss_in_train: 2.15280, loss_in_test: 35.65692\n",
      "iter: 2851, grad: 0.01641, loss_in_train: 2.15280, loss_in_test: 35.65692\n",
      "iter: 2852, grad: 0.01641, loss_in_train: 2.15280, loss_in_test: 35.65692\n",
      "iter: 2853, grad: 0.01640, loss_in_train: 2.15280, loss_in_test: 35.65692\n",
      "iter: 2854, grad: 0.01640, loss_in_train: 2.15280, loss_in_test: 35.65692\n",
      "iter: 2855, grad: 0.01640, loss_in_train: 2.15280, loss_in_test: 35.65692\n",
      "iter: 2856, grad: 0.01640, loss_in_train: 2.15280, loss_in_test: 35.65692\n",
      "iter: 2857, grad: 0.01640, loss_in_train: 2.15280, loss_in_test: 35.65693\n",
      "iter: 2858, grad: 0.01639, loss_in_train: 2.15280, loss_in_test: 35.65693\n",
      "iter: 2859, grad: 0.01639, loss_in_train: 2.15280, loss_in_test: 35.65693\n",
      "iter: 2860, grad: 0.01639, loss_in_train: 2.15280, loss_in_test: 35.65693\n",
      "iter: 2861, grad: 0.01639, loss_in_train: 2.15280, loss_in_test: 35.65693\n",
      "iter: 2862, grad: 0.01639, loss_in_train: 2.15280, loss_in_test: 35.65693\n",
      "iter: 2863, grad: 0.01639, loss_in_train: 2.15280, loss_in_test: 35.65693\n",
      "iter: 2864, grad: 0.01638, loss_in_train: 2.15280, loss_in_test: 35.65693\n",
      "iter: 2865, grad: 0.01638, loss_in_train: 2.15280, loss_in_test: 35.65694\n",
      "iter: 2866, grad: 0.01638, loss_in_train: 2.15280, loss_in_test: 35.65694\n",
      "iter: 2867, grad: 0.01638, loss_in_train: 2.15280, loss_in_test: 35.65694\n",
      "iter: 2868, grad: 0.01638, loss_in_train: 2.15280, loss_in_test: 35.65694\n",
      "iter: 2869, grad: 0.01638, loss_in_train: 2.15280, loss_in_test: 35.65694\n",
      "iter: 2870, grad: 0.01637, loss_in_train: 2.15280, loss_in_test: 35.65694\n",
      "iter: 2871, grad: 0.01637, loss_in_train: 2.15280, loss_in_test: 35.65694\n",
      "iter: 2872, grad: 0.01637, loss_in_train: 2.15280, loss_in_test: 35.65694\n",
      "iter: 2873, grad: 0.01637, loss_in_train: 2.15280, loss_in_test: 35.65695\n",
      "iter: 2874, grad: 0.01637, loss_in_train: 2.15280, loss_in_test: 35.65695\n",
      "iter: 2875, grad: 0.01637, loss_in_train: 2.15280, loss_in_test: 35.65695\n",
      "iter: 2876, grad: 0.01636, loss_in_train: 2.15280, loss_in_test: 35.65695\n",
      "iter: 2877, grad: 0.01636, loss_in_train: 2.15280, loss_in_test: 35.65695\n",
      "iter: 2878, grad: 0.01636, loss_in_train: 2.15280, loss_in_test: 35.65695\n",
      "iter: 2879, grad: 0.01636, loss_in_train: 2.15280, loss_in_test: 35.65695\n",
      "iter: 2880, grad: 0.01636, loss_in_train: 2.15280, loss_in_test: 35.65695\n",
      "iter: 2881, grad: 0.01636, loss_in_train: 2.15280, loss_in_test: 35.65696\n",
      "iter: 2882, grad: 0.01635, loss_in_train: 2.15280, loss_in_test: 35.65696\n",
      "iter: 2883, grad: 0.01635, loss_in_train: 2.15280, loss_in_test: 35.65696\n",
      "iter: 2884, grad: 0.01635, loss_in_train: 2.15280, loss_in_test: 35.65696\n",
      "iter: 2885, grad: 0.01635, loss_in_train: 2.15280, loss_in_test: 35.65696\n",
      "iter: 2886, grad: 0.01635, loss_in_train: 2.15280, loss_in_test: 35.65696\n",
      "iter: 2887, grad: 0.01635, loss_in_train: 2.15280, loss_in_test: 35.65696\n",
      "iter: 2888, grad: 0.01634, loss_in_train: 2.15280, loss_in_test: 35.65696\n",
      "iter: 2889, grad: 0.01634, loss_in_train: 2.15280, loss_in_test: 35.65697\n",
      "iter: 2890, grad: 0.01634, loss_in_train: 2.15280, loss_in_test: 35.65697\n",
      "iter: 2891, grad: 0.01634, loss_in_train: 2.15280, loss_in_test: 35.65697\n",
      "iter: 2892, grad: 0.01634, loss_in_train: 2.15280, loss_in_test: 35.65697\n",
      "iter: 2893, grad: 0.01634, loss_in_train: 2.15280, loss_in_test: 35.65697\n",
      "iter: 2894, grad: 0.01633, loss_in_train: 2.15280, loss_in_test: 35.65697\n",
      "iter: 2895, grad: 0.01633, loss_in_train: 2.15280, loss_in_test: 35.65697\n",
      "iter: 2896, grad: 0.01633, loss_in_train: 2.15280, loss_in_test: 35.65697\n",
      "iter: 2897, grad: 0.01633, loss_in_train: 2.15280, loss_in_test: 35.65697\n",
      "iter: 2898, grad: 0.01633, loss_in_train: 2.15280, loss_in_test: 35.65698\n",
      "iter: 2899, grad: 0.01633, loss_in_train: 2.15280, loss_in_test: 35.65698\n",
      "iter: 2900, grad: 0.01633, loss_in_train: 2.15280, loss_in_test: 35.65698\n",
      "iter: 2901, grad: 0.01632, loss_in_train: 2.15280, loss_in_test: 35.65698\n",
      "iter: 2902, grad: 0.01632, loss_in_train: 2.15280, loss_in_test: 35.65698\n",
      "iter: 2903, grad: 0.01632, loss_in_train: 2.15280, loss_in_test: 35.65698\n",
      "iter: 2904, grad: 0.01632, loss_in_train: 2.15280, loss_in_test: 35.65698\n",
      "iter: 2905, grad: 0.01632, loss_in_train: 2.15280, loss_in_test: 35.65698\n",
      "iter: 2906, grad: 0.01632, loss_in_train: 2.15280, loss_in_test: 35.65698\n",
      "iter: 2907, grad: 0.01631, loss_in_train: 2.15280, loss_in_test: 35.65699\n",
      "iter: 2908, grad: 0.01631, loss_in_train: 2.15280, loss_in_test: 35.65699\n",
      "iter: 2909, grad: 0.01631, loss_in_train: 2.15280, loss_in_test: 35.65699\n",
      "iter: 2910, grad: 0.01631, loss_in_train: 2.15280, loss_in_test: 35.65699\n",
      "iter: 2911, grad: 0.01631, loss_in_train: 2.15280, loss_in_test: 35.65699\n",
      "iter: 2912, grad: 0.01631, loss_in_train: 2.15280, loss_in_test: 35.65699\n",
      "iter: 2913, grad: 0.01631, loss_in_train: 2.15280, loss_in_test: 35.65699\n",
      "iter: 2914, grad: 0.01630, loss_in_train: 2.15280, loss_in_test: 35.65699\n",
      "iter: 2915, grad: 0.01630, loss_in_train: 2.15280, loss_in_test: 35.65699\n",
      "iter: 2916, grad: 0.01630, loss_in_train: 2.15280, loss_in_test: 35.65700\n",
      "iter: 2917, grad: 0.01630, loss_in_train: 2.15280, loss_in_test: 35.65700\n",
      "iter: 2918, grad: 0.01630, loss_in_train: 2.15280, loss_in_test: 35.65700\n",
      "iter: 2919, grad: 0.01630, loss_in_train: 2.15280, loss_in_test: 35.65700\n",
      "iter: 2920, grad: 0.01629, loss_in_train: 2.15280, loss_in_test: 35.65700\n",
      "iter: 2921, grad: 0.01629, loss_in_train: 2.15280, loss_in_test: 35.65700\n",
      "iter: 2922, grad: 0.01629, loss_in_train: 2.15280, loss_in_test: 35.65700\n",
      "iter: 2923, grad: 0.01629, loss_in_train: 2.15280, loss_in_test: 35.65700\n",
      "iter: 2924, grad: 0.01629, loss_in_train: 2.15280, loss_in_test: 35.65700\n",
      "iter: 2925, grad: 0.01629, loss_in_train: 2.15280, loss_in_test: 35.65701\n",
      "iter: 2926, grad: 0.01629, loss_in_train: 2.15280, loss_in_test: 35.65701\n",
      "iter: 2927, grad: 0.01628, loss_in_train: 2.15280, loss_in_test: 35.65701\n",
      "iter: 2928, grad: 0.01628, loss_in_train: 2.15280, loss_in_test: 35.65701\n",
      "iter: 2929, grad: 0.01628, loss_in_train: 2.15280, loss_in_test: 35.65701\n",
      "iter: 2930, grad: 0.01628, loss_in_train: 2.15280, loss_in_test: 35.65701\n",
      "iter: 2931, grad: 0.01628, loss_in_train: 2.15280, loss_in_test: 35.65701\n",
      "iter: 2932, grad: 0.01628, loss_in_train: 2.15280, loss_in_test: 35.65701\n",
      "iter: 2933, grad: 0.01628, loss_in_train: 2.15280, loss_in_test: 35.65701\n",
      "iter: 2934, grad: 0.01627, loss_in_train: 2.15280, loss_in_test: 35.65702\n",
      "iter: 2935, grad: 0.01627, loss_in_train: 2.15280, loss_in_test: 35.65702\n",
      "iter: 2936, grad: 0.01627, loss_in_train: 2.15280, loss_in_test: 35.65702\n",
      "iter: 2937, grad: 0.01627, loss_in_train: 2.15280, loss_in_test: 35.65702\n",
      "iter: 2938, grad: 0.01627, loss_in_train: 2.15280, loss_in_test: 35.65702\n",
      "iter: 2939, grad: 0.01627, loss_in_train: 2.15280, loss_in_test: 35.65702\n",
      "iter: 2940, grad: 0.01627, loss_in_train: 2.15280, loss_in_test: 35.65702\n",
      "iter: 2941, grad: 0.01626, loss_in_train: 2.15280, loss_in_test: 35.65702\n",
      "iter: 2942, grad: 0.01626, loss_in_train: 2.15280, loss_in_test: 35.65702\n",
      "iter: 2943, grad: 0.01626, loss_in_train: 2.15280, loss_in_test: 35.65702\n",
      "iter: 2944, grad: 0.01626, loss_in_train: 2.15280, loss_in_test: 35.65703\n",
      "iter: 2945, grad: 0.01626, loss_in_train: 2.15280, loss_in_test: 35.65703\n",
      "iter: 2946, grad: 0.01626, loss_in_train: 2.15280, loss_in_test: 35.65703\n",
      "iter: 2947, grad: 0.01626, loss_in_train: 2.15280, loss_in_test: 35.65703\n",
      "iter: 2948, grad: 0.01625, loss_in_train: 2.15280, loss_in_test: 35.65703\n",
      "iter: 2949, grad: 0.01625, loss_in_train: 2.15280, loss_in_test: 35.65703\n",
      "iter: 2950, grad: 0.01625, loss_in_train: 2.15280, loss_in_test: 35.65703\n",
      "iter: 2951, grad: 0.01625, loss_in_train: 2.15280, loss_in_test: 35.65703\n",
      "iter: 2952, grad: 0.01625, loss_in_train: 2.15280, loss_in_test: 35.65703\n",
      "iter: 2953, grad: 0.01625, loss_in_train: 2.15280, loss_in_test: 35.65704\n",
      "iter: 2954, grad: 0.01625, loss_in_train: 2.15280, loss_in_test: 35.65704\n",
      "iter: 2955, grad: 0.01625, loss_in_train: 2.15280, loss_in_test: 35.65704\n",
      "iter: 2956, grad: 0.01624, loss_in_train: 2.15280, loss_in_test: 35.65704\n",
      "iter: 2957, grad: 0.01624, loss_in_train: 2.15280, loss_in_test: 35.65704\n",
      "iter: 2958, grad: 0.01624, loss_in_train: 2.15280, loss_in_test: 35.65704\n",
      "iter: 2959, grad: 0.01624, loss_in_train: 2.15280, loss_in_test: 35.65704\n",
      "iter: 2960, grad: 0.01624, loss_in_train: 2.15280, loss_in_test: 35.65704\n",
      "iter: 2961, grad: 0.01624, loss_in_train: 2.15280, loss_in_test: 35.65704\n",
      "iter: 2962, grad: 0.01624, loss_in_train: 2.15280, loss_in_test: 35.65704\n",
      "iter: 2963, grad: 0.01623, loss_in_train: 2.15280, loss_in_test: 35.65705\n",
      "iter: 2964, grad: 0.01623, loss_in_train: 2.15280, loss_in_test: 35.65705\n",
      "iter: 2965, grad: 0.01623, loss_in_train: 2.15280, loss_in_test: 35.65705\n",
      "iter: 2966, grad: 0.01623, loss_in_train: 2.15280, loss_in_test: 35.65705\n",
      "iter: 2967, grad: 0.01623, loss_in_train: 2.15280, loss_in_test: 35.65705\n",
      "iter: 2968, grad: 0.01623, loss_in_train: 2.15280, loss_in_test: 35.65705\n",
      "iter: 2969, grad: 0.01623, loss_in_train: 2.15280, loss_in_test: 35.65705\n",
      "iter: 2970, grad: 0.01623, loss_in_train: 2.15280, loss_in_test: 35.65705\n",
      "iter: 2971, grad: 0.01622, loss_in_train: 2.15280, loss_in_test: 35.65705\n",
      "iter: 2972, grad: 0.01622, loss_in_train: 2.15280, loss_in_test: 35.65705\n",
      "iter: 2973, grad: 0.01622, loss_in_train: 2.15280, loss_in_test: 35.65705\n",
      "iter: 2974, grad: 0.01622, loss_in_train: 2.15280, loss_in_test: 35.65706\n",
      "iter: 2975, grad: 0.01622, loss_in_train: 2.15280, loss_in_test: 35.65706\n",
      "iter: 2976, grad: 0.01622, loss_in_train: 2.15280, loss_in_test: 35.65706\n",
      "iter: 2977, grad: 0.01622, loss_in_train: 2.15280, loss_in_test: 35.65706\n",
      "iter: 2978, grad: 0.01621, loss_in_train: 2.15280, loss_in_test: 35.65706\n",
      "iter: 2979, grad: 0.01621, loss_in_train: 2.15280, loss_in_test: 35.65706\n",
      "iter: 2980, grad: 0.01621, loss_in_train: 2.15280, loss_in_test: 35.65706\n",
      "iter: 2981, grad: 0.01621, loss_in_train: 2.15280, loss_in_test: 35.65706\n",
      "iter: 2982, grad: 0.01621, loss_in_train: 2.15280, loss_in_test: 35.65706\n",
      "iter: 2983, grad: 0.01621, loss_in_train: 2.15280, loss_in_test: 35.65706\n",
      "iter: 2984, grad: 0.01621, loss_in_train: 2.15280, loss_in_test: 35.65707\n",
      "iter: 2985, grad: 0.01621, loss_in_train: 2.15280, loss_in_test: 35.65707\n",
      "iter: 2986, grad: 0.01620, loss_in_train: 2.15280, loss_in_test: 35.65707\n",
      "iter: 2987, grad: 0.01620, loss_in_train: 2.15280, loss_in_test: 35.65707\n",
      "iter: 2988, grad: 0.01620, loss_in_train: 2.15280, loss_in_test: 35.65707\n",
      "iter: 2989, grad: 0.01620, loss_in_train: 2.15280, loss_in_test: 35.65707\n",
      "iter: 2990, grad: 0.01620, loss_in_train: 2.15280, loss_in_test: 35.65707\n",
      "iter: 2991, grad: 0.01620, loss_in_train: 2.15280, loss_in_test: 35.65707\n",
      "iter: 2992, grad: 0.01620, loss_in_train: 2.15280, loss_in_test: 35.65707\n",
      "iter: 2993, grad: 0.01620, loss_in_train: 2.15280, loss_in_test: 35.65707\n",
      "iter: 2994, grad: 0.01619, loss_in_train: 2.15280, loss_in_test: 35.65707\n",
      "iter: 2995, grad: 0.01619, loss_in_train: 2.15280, loss_in_test: 35.65708\n",
      "iter: 2996, grad: 0.01619, loss_in_train: 2.15280, loss_in_test: 35.65708\n",
      "iter: 2997, grad: 0.01619, loss_in_train: 2.15280, loss_in_test: 35.65708\n",
      "iter: 2998, grad: 0.01619, loss_in_train: 2.15280, loss_in_test: 35.65708\n",
      "iter: 2999, grad: 0.01619, loss_in_train: 2.15280, loss_in_test: 35.65708\n",
      "iter: 3000, grad: 0.01619, loss_in_train: 2.15280, loss_in_test: 35.65708\n",
      "iter: 3001, grad: 0.01619, loss_in_train: 2.15280, loss_in_test: 35.65708\n",
      "iter: 3002, grad: 0.01618, loss_in_train: 2.15280, loss_in_test: 35.65708\n",
      "iter: 3003, grad: 0.01618, loss_in_train: 2.15280, loss_in_test: 35.65708\n",
      "iter: 3004, grad: 0.01618, loss_in_train: 2.15280, loss_in_test: 35.65708\n",
      "iter: 3005, grad: 0.01618, loss_in_train: 2.15280, loss_in_test: 35.65708\n",
      "iter: 3006, grad: 0.01618, loss_in_train: 2.15280, loss_in_test: 35.65709\n",
      "iter: 3007, grad: 0.01618, loss_in_train: 2.15280, loss_in_test: 35.65709\n",
      "iter: 3008, grad: 0.01618, loss_in_train: 2.15280, loss_in_test: 35.65709\n",
      "iter: 3009, grad: 0.01618, loss_in_train: 2.15280, loss_in_test: 35.65709\n",
      "iter: 3010, grad: 0.01618, loss_in_train: 2.15280, loss_in_test: 35.65709\n",
      "iter: 3011, grad: 0.01617, loss_in_train: 2.15280, loss_in_test: 35.65709\n",
      "iter: 3012, grad: 0.01617, loss_in_train: 2.15280, loss_in_test: 35.65709\n",
      "iter: 3013, grad: 0.01617, loss_in_train: 2.15280, loss_in_test: 35.65709\n",
      "iter: 3014, grad: 0.01617, loss_in_train: 2.15280, loss_in_test: 35.65709\n",
      "iter: 3015, grad: 0.01617, loss_in_train: 2.15280, loss_in_test: 35.65709\n",
      "iter: 3016, grad: 0.01617, loss_in_train: 2.15280, loss_in_test: 35.65709\n",
      "iter: 3017, grad: 0.01617, loss_in_train: 2.15280, loss_in_test: 35.65709\n",
      "iter: 3018, grad: 0.01617, loss_in_train: 2.15280, loss_in_test: 35.65710\n",
      "iter: 3019, grad: 0.01616, loss_in_train: 2.15280, loss_in_test: 35.65710\n",
      "iter: 3020, grad: 0.01616, loss_in_train: 2.15280, loss_in_test: 35.65710\n",
      "iter: 3021, grad: 0.01616, loss_in_train: 2.15280, loss_in_test: 35.65710\n",
      "iter: 3022, grad: 0.01616, loss_in_train: 2.15280, loss_in_test: 35.65710\n",
      "iter: 3023, grad: 0.01616, loss_in_train: 2.15280, loss_in_test: 35.65710\n",
      "iter: 3024, grad: 0.01616, loss_in_train: 2.15280, loss_in_test: 35.65710\n",
      "iter: 3025, grad: 0.01616, loss_in_train: 2.15280, loss_in_test: 35.65710\n",
      "iter: 3026, grad: 0.01616, loss_in_train: 2.15280, loss_in_test: 35.65710\n",
      "iter: 3027, grad: 0.01616, loss_in_train: 2.15280, loss_in_test: 35.65710\n",
      "iter: 3028, grad: 0.01615, loss_in_train: 2.15280, loss_in_test: 35.65710\n",
      "iter: 3029, grad: 0.01615, loss_in_train: 2.15280, loss_in_test: 35.65710\n",
      "iter: 3030, grad: 0.01615, loss_in_train: 2.15280, loss_in_test: 35.65711\n",
      "iter: 3031, grad: 0.01615, loss_in_train: 2.15280, loss_in_test: 35.65711\n",
      "iter: 3032, grad: 0.01615, loss_in_train: 2.15280, loss_in_test: 35.65711\n",
      "iter: 3033, grad: 0.01615, loss_in_train: 2.15280, loss_in_test: 35.65711\n",
      "iter: 3034, grad: 0.01615, loss_in_train: 2.15280, loss_in_test: 35.65711\n",
      "iter: 3035, grad: 0.01615, loss_in_train: 2.15280, loss_in_test: 35.65711\n",
      "iter: 3036, grad: 0.01615, loss_in_train: 2.15280, loss_in_test: 35.65711\n",
      "iter: 3037, grad: 0.01614, loss_in_train: 2.15280, loss_in_test: 35.65711\n",
      "iter: 3038, grad: 0.01614, loss_in_train: 2.15280, loss_in_test: 35.65711\n",
      "iter: 3039, grad: 0.01614, loss_in_train: 2.15280, loss_in_test: 35.65711\n",
      "iter: 3040, grad: 0.01614, loss_in_train: 2.15280, loss_in_test: 35.65711\n",
      "iter: 3041, grad: 0.01614, loss_in_train: 2.15280, loss_in_test: 35.65711\n",
      "iter: 3042, grad: 0.01614, loss_in_train: 2.15280, loss_in_test: 35.65712\n",
      "iter: 3043, grad: 0.01614, loss_in_train: 2.15280, loss_in_test: 35.65712\n",
      "iter: 3044, grad: 0.01614, loss_in_train: 2.15280, loss_in_test: 35.65712\n",
      "iter: 3045, grad: 0.01614, loss_in_train: 2.15280, loss_in_test: 35.65712\n",
      "iter: 3046, grad: 0.01613, loss_in_train: 2.15280, loss_in_test: 35.65712\n",
      "iter: 3047, grad: 0.01613, loss_in_train: 2.15280, loss_in_test: 35.65712\n",
      "iter: 3048, grad: 0.01613, loss_in_train: 2.15280, loss_in_test: 35.65712\n",
      "iter: 3049, grad: 0.01613, loss_in_train: 2.15280, loss_in_test: 35.65712\n",
      "iter: 3050, grad: 0.01613, loss_in_train: 2.15280, loss_in_test: 35.65712\n",
      "iter: 3051, grad: 0.01613, loss_in_train: 2.15280, loss_in_test: 35.65712\n",
      "iter: 3052, grad: 0.01613, loss_in_train: 2.15280, loss_in_test: 35.65712\n",
      "iter: 3053, grad: 0.01613, loss_in_train: 2.15280, loss_in_test: 35.65712\n",
      "iter: 3054, grad: 0.01613, loss_in_train: 2.15280, loss_in_test: 35.65712\n",
      "iter: 3055, grad: 0.01612, loss_in_train: 2.15280, loss_in_test: 35.65713\n",
      "iter: 3056, grad: 0.01612, loss_in_train: 2.15280, loss_in_test: 35.65713\n",
      "iter: 3057, grad: 0.01612, loss_in_train: 2.15280, loss_in_test: 35.65713\n",
      "iter: 3058, grad: 0.01612, loss_in_train: 2.15280, loss_in_test: 35.65713\n",
      "iter: 3059, grad: 0.01612, loss_in_train: 2.15280, loss_in_test: 35.65713\n",
      "iter: 3060, grad: 0.01612, loss_in_train: 2.15280, loss_in_test: 35.65713\n",
      "iter: 3061, grad: 0.01612, loss_in_train: 2.15280, loss_in_test: 35.65713\n",
      "iter: 3062, grad: 0.01612, loss_in_train: 2.15280, loss_in_test: 35.65713\n",
      "iter: 3063, grad: 0.01612, loss_in_train: 2.15280, loss_in_test: 35.65713\n",
      "iter: 3064, grad: 0.01612, loss_in_train: 2.15280, loss_in_test: 35.65713\n",
      "iter: 3065, grad: 0.01611, loss_in_train: 2.15280, loss_in_test: 35.65713\n",
      "iter: 3066, grad: 0.01611, loss_in_train: 2.15280, loss_in_test: 35.65713\n",
      "iter: 3067, grad: 0.01611, loss_in_train: 2.15280, loss_in_test: 35.65713\n",
      "iter: 3068, grad: 0.01611, loss_in_train: 2.15280, loss_in_test: 35.65714\n",
      "iter: 3069, grad: 0.01611, loss_in_train: 2.15280, loss_in_test: 35.65714\n",
      "iter: 3070, grad: 0.01611, loss_in_train: 2.15280, loss_in_test: 35.65714\n",
      "iter: 3071, grad: 0.01611, loss_in_train: 2.15280, loss_in_test: 35.65714\n",
      "iter: 3072, grad: 0.01611, loss_in_train: 2.15280, loss_in_test: 35.65714\n",
      "iter: 3073, grad: 0.01611, loss_in_train: 2.15280, loss_in_test: 35.65714\n",
      "iter: 3074, grad: 0.01610, loss_in_train: 2.15280, loss_in_test: 35.65714\n",
      "iter: 3075, grad: 0.01610, loss_in_train: 2.15280, loss_in_test: 35.65714\n",
      "iter: 3076, grad: 0.01610, loss_in_train: 2.15280, loss_in_test: 35.65714\n",
      "iter: 3077, grad: 0.01610, loss_in_train: 2.15280, loss_in_test: 35.65714\n",
      "iter: 3078, grad: 0.01610, loss_in_train: 2.15280, loss_in_test: 35.65714\n",
      "iter: 3079, grad: 0.01610, loss_in_train: 2.15280, loss_in_test: 35.65714\n",
      "iter: 3080, grad: 0.01610, loss_in_train: 2.15280, loss_in_test: 35.65714\n",
      "iter: 3081, grad: 0.01610, loss_in_train: 2.15280, loss_in_test: 35.65715\n",
      "iter: 3082, grad: 0.01610, loss_in_train: 2.15280, loss_in_test: 35.65715\n",
      "iter: 3083, grad: 0.01610, loss_in_train: 2.15280, loss_in_test: 35.65715\n",
      "iter: 3084, grad: 0.01609, loss_in_train: 2.15280, loss_in_test: 35.65715\n",
      "iter: 3085, grad: 0.01609, loss_in_train: 2.15280, loss_in_test: 35.65715\n",
      "iter: 3086, grad: 0.01609, loss_in_train: 2.15280, loss_in_test: 35.65715\n",
      "iter: 3087, grad: 0.01609, loss_in_train: 2.15280, loss_in_test: 35.65715\n",
      "iter: 3088, grad: 0.01609, loss_in_train: 2.15280, loss_in_test: 35.65715\n",
      "iter: 3089, grad: 0.01609, loss_in_train: 2.15280, loss_in_test: 35.65715\n",
      "iter: 3090, grad: 0.01609, loss_in_train: 2.15280, loss_in_test: 35.65715\n",
      "iter: 3091, grad: 0.01609, loss_in_train: 2.15280, loss_in_test: 35.65715\n",
      "iter: 3092, grad: 0.01609, loss_in_train: 2.15280, loss_in_test: 35.65715\n",
      "iter: 3093, grad: 0.01609, loss_in_train: 2.15280, loss_in_test: 35.65715\n",
      "iter: 3094, grad: 0.01609, loss_in_train: 2.15280, loss_in_test: 35.65715\n",
      "iter: 3095, grad: 0.01608, loss_in_train: 2.15280, loss_in_test: 35.65716\n",
      "iter: 3096, grad: 0.01608, loss_in_train: 2.15280, loss_in_test: 35.65716\n",
      "iter: 3097, grad: 0.01608, loss_in_train: 2.15280, loss_in_test: 35.65716\n",
      "iter: 3098, grad: 0.01608, loss_in_train: 2.15280, loss_in_test: 35.65716\n",
      "iter: 3099, grad: 0.01608, loss_in_train: 2.15280, loss_in_test: 35.65716\n",
      "iter: 3100, grad: 0.01608, loss_in_train: 2.15280, loss_in_test: 35.65716\n",
      "iter: 3101, grad: 0.01608, loss_in_train: 2.15280, loss_in_test: 35.65716\n",
      "iter: 3102, grad: 0.01608, loss_in_train: 2.15280, loss_in_test: 35.65716\n",
      "iter: 3103, grad: 0.01608, loss_in_train: 2.15280, loss_in_test: 35.65716\n",
      "iter: 3104, grad: 0.01608, loss_in_train: 2.15280, loss_in_test: 35.65716\n",
      "iter: 3105, grad: 0.01607, loss_in_train: 2.15280, loss_in_test: 35.65716\n",
      "iter: 3106, grad: 0.01607, loss_in_train: 2.15280, loss_in_test: 35.65716\n",
      "iter: 3107, grad: 0.01607, loss_in_train: 2.15280, loss_in_test: 35.65716\n",
      "iter: 3108, grad: 0.01607, loss_in_train: 2.15280, loss_in_test: 35.65716\n",
      "iter: 3109, grad: 0.01607, loss_in_train: 2.15280, loss_in_test: 35.65717\n",
      "iter: 3110, grad: 0.01607, loss_in_train: 2.15280, loss_in_test: 35.65717\n",
      "iter: 3111, grad: 0.01607, loss_in_train: 2.15280, loss_in_test: 35.65717\n",
      "iter: 3112, grad: 0.01607, loss_in_train: 2.15280, loss_in_test: 35.65717\n",
      "iter: 3113, grad: 0.01607, loss_in_train: 2.15280, loss_in_test: 35.65717\n",
      "iter: 3114, grad: 0.01607, loss_in_train: 2.15280, loss_in_test: 35.65717\n",
      "iter: 3115, grad: 0.01607, loss_in_train: 2.15280, loss_in_test: 35.65717\n",
      "iter: 3116, grad: 0.01606, loss_in_train: 2.15280, loss_in_test: 35.65717\n",
      "iter: 3117, grad: 0.01606, loss_in_train: 2.15280, loss_in_test: 35.65717\n",
      "iter: 3118, grad: 0.01606, loss_in_train: 2.15280, loss_in_test: 35.65717\n",
      "iter: 3119, grad: 0.01606, loss_in_train: 2.15280, loss_in_test: 35.65717\n",
      "iter: 3120, grad: 0.01606, loss_in_train: 2.15280, loss_in_test: 35.65717\n",
      "iter: 3121, grad: 0.01606, loss_in_train: 2.15280, loss_in_test: 35.65717\n",
      "iter: 3122, grad: 0.01606, loss_in_train: 2.15280, loss_in_test: 35.65717\n",
      "iter: 3123, grad: 0.01606, loss_in_train: 2.15280, loss_in_test: 35.65717\n",
      "iter: 3124, grad: 0.01606, loss_in_train: 2.15280, loss_in_test: 35.65718\n",
      "iter: 3125, grad: 0.01606, loss_in_train: 2.15280, loss_in_test: 35.65718\n",
      "iter: 3126, grad: 0.01606, loss_in_train: 2.15280, loss_in_test: 35.65718\n",
      "iter: 3127, grad: 0.01605, loss_in_train: 2.15280, loss_in_test: 35.65718\n",
      "iter: 3128, grad: 0.01605, loss_in_train: 2.15280, loss_in_test: 35.65718\n",
      "iter: 3129, grad: 0.01605, loss_in_train: 2.15280, loss_in_test: 35.65718\n",
      "iter: 3130, grad: 0.01605, loss_in_train: 2.15280, loss_in_test: 35.65718\n",
      "iter: 3131, grad: 0.01605, loss_in_train: 2.15280, loss_in_test: 35.65718\n",
      "iter: 3132, grad: 0.01605, loss_in_train: 2.15280, loss_in_test: 35.65718\n",
      "iter: 3133, grad: 0.01605, loss_in_train: 2.15280, loss_in_test: 35.65718\n",
      "iter: 3134, grad: 0.01605, loss_in_train: 2.15280, loss_in_test: 35.65718\n",
      "iter: 3135, grad: 0.01605, loss_in_train: 2.15280, loss_in_test: 35.65718\n",
      "iter: 3136, grad: 0.01605, loss_in_train: 2.15280, loss_in_test: 35.65718\n",
      "iter: 3137, grad: 0.01605, loss_in_train: 2.15280, loss_in_test: 35.65718\n",
      "iter: 3138, grad: 0.01605, loss_in_train: 2.15280, loss_in_test: 35.65718\n",
      "iter: 3139, grad: 0.01604, loss_in_train: 2.15280, loss_in_test: 35.65718\n",
      "iter: 3140, grad: 0.01604, loss_in_train: 2.15280, loss_in_test: 35.65719\n",
      "iter: 3141, grad: 0.01604, loss_in_train: 2.15280, loss_in_test: 35.65719\n",
      "iter: 3142, grad: 0.01604, loss_in_train: 2.15280, loss_in_test: 35.65719\n",
      "iter: 3143, grad: 0.01604, loss_in_train: 2.15280, loss_in_test: 35.65719\n",
      "iter: 3144, grad: 0.01604, loss_in_train: 2.15280, loss_in_test: 35.65719\n",
      "iter: 3145, grad: 0.01604, loss_in_train: 2.15280, loss_in_test: 35.65719\n",
      "iter: 3146, grad: 0.01604, loss_in_train: 2.15280, loss_in_test: 35.65719\n",
      "iter: 3147, grad: 0.01604, loss_in_train: 2.15280, loss_in_test: 35.65719\n",
      "iter: 3148, grad: 0.01604, loss_in_train: 2.15280, loss_in_test: 35.65719\n",
      "iter: 3149, grad: 0.01604, loss_in_train: 2.15280, loss_in_test: 35.65719\n",
      "iter: 3150, grad: 0.01603, loss_in_train: 2.15280, loss_in_test: 35.65719\n",
      "iter: 3151, grad: 0.01603, loss_in_train: 2.15280, loss_in_test: 35.65719\n",
      "iter: 3152, grad: 0.01603, loss_in_train: 2.15280, loss_in_test: 35.65719\n",
      "iter: 3153, grad: 0.01603, loss_in_train: 2.15280, loss_in_test: 35.65719\n",
      "iter: 3154, grad: 0.01603, loss_in_train: 2.15280, loss_in_test: 35.65719\n",
      "iter: 3155, grad: 0.01603, loss_in_train: 2.15280, loss_in_test: 35.65719\n",
      "iter: 3156, grad: 0.01603, loss_in_train: 2.15280, loss_in_test: 35.65720\n",
      "iter: 3157, grad: 0.01603, loss_in_train: 2.15280, loss_in_test: 35.65720\n",
      "iter: 3158, grad: 0.01603, loss_in_train: 2.15280, loss_in_test: 35.65720\n",
      "iter: 3159, grad: 0.01603, loss_in_train: 2.15280, loss_in_test: 35.65720\n",
      "iter: 3160, grad: 0.01603, loss_in_train: 2.15280, loss_in_test: 35.65720\n",
      "iter: 3161, grad: 0.01603, loss_in_train: 2.15280, loss_in_test: 35.65720\n",
      "iter: 3162, grad: 0.01603, loss_in_train: 2.15280, loss_in_test: 35.65720\n",
      "iter: 3163, grad: 0.01602, loss_in_train: 2.15280, loss_in_test: 35.65720\n",
      "iter: 3164, grad: 0.01602, loss_in_train: 2.15280, loss_in_test: 35.65720\n",
      "iter: 3165, grad: 0.01602, loss_in_train: 2.15280, loss_in_test: 35.65720\n",
      "iter: 3166, grad: 0.01602, loss_in_train: 2.15280, loss_in_test: 35.65720\n",
      "iter: 3167, grad: 0.01602, loss_in_train: 2.15280, loss_in_test: 35.65720\n",
      "iter: 3168, grad: 0.01602, loss_in_train: 2.15280, loss_in_test: 35.65720\n",
      "iter: 3169, grad: 0.01602, loss_in_train: 2.15280, loss_in_test: 35.65720\n",
      "iter: 3170, grad: 0.01602, loss_in_train: 2.15280, loss_in_test: 35.65720\n",
      "iter: 3171, grad: 0.01602, loss_in_train: 2.15280, loss_in_test: 35.65720\n",
      "iter: 3172, grad: 0.01602, loss_in_train: 2.15280, loss_in_test: 35.65721\n",
      "iter: 3173, grad: 0.01602, loss_in_train: 2.15280, loss_in_test: 35.65721\n",
      "iter: 3174, grad: 0.01602, loss_in_train: 2.15280, loss_in_test: 35.65721\n",
      "iter: 3175, grad: 0.01601, loss_in_train: 2.15280, loss_in_test: 35.65721\n",
      "iter: 3176, grad: 0.01601, loss_in_train: 2.15280, loss_in_test: 35.65721\n",
      "iter: 3177, grad: 0.01601, loss_in_train: 2.15280, loss_in_test: 35.65721\n",
      "iter: 3178, grad: 0.01601, loss_in_train: 2.15280, loss_in_test: 35.65721\n",
      "iter: 3179, grad: 0.01601, loss_in_train: 2.15280, loss_in_test: 35.65721\n",
      "iter: 3180, grad: 0.01601, loss_in_train: 2.15280, loss_in_test: 35.65721\n",
      "iter: 3181, grad: 0.01601, loss_in_train: 2.15280, loss_in_test: 35.65721\n",
      "iter: 3182, grad: 0.01601, loss_in_train: 2.15280, loss_in_test: 35.65721\n",
      "iter: 3183, grad: 0.01601, loss_in_train: 2.15280, loss_in_test: 35.65721\n",
      "iter: 3184, grad: 0.01601, loss_in_train: 2.15280, loss_in_test: 35.65721\n",
      "iter: 3185, grad: 0.01601, loss_in_train: 2.15280, loss_in_test: 35.65721\n",
      "iter: 3186, grad: 0.01601, loss_in_train: 2.15280, loss_in_test: 35.65721\n",
      "iter: 3187, grad: 0.01601, loss_in_train: 2.15280, loss_in_test: 35.65721\n",
      "iter: 3188, grad: 0.01600, loss_in_train: 2.15280, loss_in_test: 35.65721\n",
      "iter: 3189, grad: 0.01600, loss_in_train: 2.15280, loss_in_test: 35.65721\n",
      "iter: 3190, grad: 0.01600, loss_in_train: 2.15280, loss_in_test: 35.65722\n",
      "iter: 3191, grad: 0.01600, loss_in_train: 2.15280, loss_in_test: 35.65722\n",
      "iter: 3192, grad: 0.01600, loss_in_train: 2.15280, loss_in_test: 35.65722\n",
      "iter: 3193, grad: 0.01600, loss_in_train: 2.15280, loss_in_test: 35.65722\n",
      "iter: 3194, grad: 0.01600, loss_in_train: 2.15280, loss_in_test: 35.65722\n",
      "iter: 3195, grad: 0.01600, loss_in_train: 2.15280, loss_in_test: 35.65722\n",
      "iter: 3196, grad: 0.01600, loss_in_train: 2.15280, loss_in_test: 35.65722\n",
      "iter: 3197, grad: 0.01600, loss_in_train: 2.15280, loss_in_test: 35.65722\n",
      "iter: 3198, grad: 0.01600, loss_in_train: 2.15280, loss_in_test: 35.65722\n",
      "iter: 3199, grad: 0.01600, loss_in_train: 2.15280, loss_in_test: 35.65722\n",
      "iter: 3200, grad: 0.01600, loss_in_train: 2.15280, loss_in_test: 35.65722\n",
      "iter: 3201, grad: 0.01599, loss_in_train: 2.15280, loss_in_test: 35.65722\n",
      "iter: 3202, grad: 0.01599, loss_in_train: 2.15280, loss_in_test: 35.65722\n",
      "iter: 3203, grad: 0.01599, loss_in_train: 2.15280, loss_in_test: 35.65722\n",
      "iter: 3204, grad: 0.01599, loss_in_train: 2.15280, loss_in_test: 35.65722\n",
      "iter: 3205, grad: 0.01599, loss_in_train: 2.15280, loss_in_test: 35.65722\n",
      "iter: 3206, grad: 0.01599, loss_in_train: 2.15280, loss_in_test: 35.65722\n",
      "iter: 3207, grad: 0.01599, loss_in_train: 2.15280, loss_in_test: 35.65722\n",
      "iter: 3208, grad: 0.01599, loss_in_train: 2.15280, loss_in_test: 35.65723\n",
      "iter: 3209, grad: 0.01599, loss_in_train: 2.15280, loss_in_test: 35.65723\n",
      "iter: 3210, grad: 0.01599, loss_in_train: 2.15280, loss_in_test: 35.65723\n",
      "iter: 3211, grad: 0.01599, loss_in_train: 2.15280, loss_in_test: 35.65723\n",
      "iter: 3212, grad: 0.01599, loss_in_train: 2.15280, loss_in_test: 35.65723\n",
      "iter: 3213, grad: 0.01599, loss_in_train: 2.15280, loss_in_test: 35.65723\n",
      "iter: 3214, grad: 0.01599, loss_in_train: 2.15280, loss_in_test: 35.65723\n",
      "iter: 3215, grad: 0.01598, loss_in_train: 2.15280, loss_in_test: 35.65723\n",
      "iter: 3216, grad: 0.01598, loss_in_train: 2.15280, loss_in_test: 35.65723\n",
      "iter: 3217, grad: 0.01598, loss_in_train: 2.15280, loss_in_test: 35.65723\n",
      "iter: 3218, grad: 0.01598, loss_in_train: 2.15280, loss_in_test: 35.65723\n",
      "iter: 3219, grad: 0.01598, loss_in_train: 2.15280, loss_in_test: 35.65723\n",
      "iter: 3220, grad: 0.01598, loss_in_train: 2.15280, loss_in_test: 35.65723\n",
      "iter: 3221, grad: 0.01598, loss_in_train: 2.15280, loss_in_test: 35.65723\n",
      "iter: 3222, grad: 0.01598, loss_in_train: 2.15280, loss_in_test: 35.65723\n",
      "iter: 3223, grad: 0.01598, loss_in_train: 2.15280, loss_in_test: 35.65723\n",
      "iter: 3224, grad: 0.01598, loss_in_train: 2.15280, loss_in_test: 35.65723\n",
      "iter: 3225, grad: 0.01598, loss_in_train: 2.15280, loss_in_test: 35.65723\n",
      "iter: 3226, grad: 0.01598, loss_in_train: 2.15280, loss_in_test: 35.65723\n",
      "iter: 3227, grad: 0.01598, loss_in_train: 2.15280, loss_in_test: 35.65724\n",
      "iter: 3228, grad: 0.01598, loss_in_train: 2.15280, loss_in_test: 35.65724\n",
      "iter: 3229, grad: 0.01598, loss_in_train: 2.15280, loss_in_test: 35.65724\n",
      "iter: 3230, grad: 0.01597, loss_in_train: 2.15280, loss_in_test: 35.65724\n",
      "iter: 3231, grad: 0.01597, loss_in_train: 2.15280, loss_in_test: 35.65724\n",
      "iter: 3232, grad: 0.01597, loss_in_train: 2.15280, loss_in_test: 35.65724\n",
      "iter: 3233, grad: 0.01597, loss_in_train: 2.15280, loss_in_test: 35.65724\n",
      "iter: 3234, grad: 0.01597, loss_in_train: 2.15280, loss_in_test: 35.65724\n",
      "iter: 3235, grad: 0.01597, loss_in_train: 2.15280, loss_in_test: 35.65724\n",
      "iter: 3236, grad: 0.01597, loss_in_train: 2.15280, loss_in_test: 35.65724\n",
      "iter: 3237, grad: 0.01597, loss_in_train: 2.15280, loss_in_test: 35.65724\n",
      "iter: 3238, grad: 0.01597, loss_in_train: 2.15280, loss_in_test: 35.65724\n",
      "iter: 3239, grad: 0.01597, loss_in_train: 2.15280, loss_in_test: 35.65724\n",
      "iter: 3240, grad: 0.01597, loss_in_train: 2.15280, loss_in_test: 35.65724\n",
      "iter: 3241, grad: 0.01597, loss_in_train: 2.15280, loss_in_test: 35.65724\n",
      "iter: 3242, grad: 0.01597, loss_in_train: 2.15280, loss_in_test: 35.65724\n",
      "iter: 3243, grad: 0.01597, loss_in_train: 2.15280, loss_in_test: 35.65724\n",
      "iter: 3244, grad: 0.01596, loss_in_train: 2.15280, loss_in_test: 35.65724\n",
      "iter: 3245, grad: 0.01596, loss_in_train: 2.15280, loss_in_test: 35.65724\n",
      "iter: 3246, grad: 0.01596, loss_in_train: 2.15280, loss_in_test: 35.65724\n",
      "iter: 3247, grad: 0.01596, loss_in_train: 2.15280, loss_in_test: 35.65724\n",
      "iter: 3248, grad: 0.01596, loss_in_train: 2.15280, loss_in_test: 35.65725\n",
      "iter: 3249, grad: 0.01596, loss_in_train: 2.15280, loss_in_test: 35.65725\n",
      "iter: 3250, grad: 0.01596, loss_in_train: 2.15280, loss_in_test: 35.65725\n",
      "iter: 3251, grad: 0.01596, loss_in_train: 2.15280, loss_in_test: 35.65725\n",
      "iter: 3252, grad: 0.01596, loss_in_train: 2.15280, loss_in_test: 35.65725\n",
      "iter: 3253, grad: 0.01596, loss_in_train: 2.15280, loss_in_test: 35.65725\n",
      "iter: 3254, grad: 0.01596, loss_in_train: 2.15280, loss_in_test: 35.65725\n",
      "iter: 3255, grad: 0.01596, loss_in_train: 2.15280, loss_in_test: 35.65725\n",
      "iter: 3256, grad: 0.01596, loss_in_train: 2.15280, loss_in_test: 35.65725\n",
      "iter: 3257, grad: 0.01596, loss_in_train: 2.15280, loss_in_test: 35.65725\n",
      "iter: 3258, grad: 0.01596, loss_in_train: 2.15280, loss_in_test: 35.65725\n",
      "iter: 3259, grad: 0.01596, loss_in_train: 2.15280, loss_in_test: 35.65725\n",
      "iter: 3260, grad: 0.01595, loss_in_train: 2.15280, loss_in_test: 35.65725\n",
      "iter: 3261, grad: 0.01595, loss_in_train: 2.15280, loss_in_test: 35.65725\n",
      "iter: 3262, grad: 0.01595, loss_in_train: 2.15280, loss_in_test: 35.65725\n",
      "iter: 3263, grad: 0.01595, loss_in_train: 2.15280, loss_in_test: 35.65725\n",
      "iter: 3264, grad: 0.01595, loss_in_train: 2.15280, loss_in_test: 35.65725\n",
      "iter: 3265, grad: 0.01595, loss_in_train: 2.15280, loss_in_test: 35.65725\n",
      "iter: 3266, grad: 0.01595, loss_in_train: 2.15280, loss_in_test: 35.65725\n",
      "iter: 3267, grad: 0.01595, loss_in_train: 2.15280, loss_in_test: 35.65725\n",
      "iter: 3268, grad: 0.01595, loss_in_train: 2.15280, loss_in_test: 35.65725\n",
      "iter: 3269, grad: 0.01595, loss_in_train: 2.15280, loss_in_test: 35.65726\n",
      "iter: 3270, grad: 0.01595, loss_in_train: 2.15280, loss_in_test: 35.65726\n",
      "iter: 3271, grad: 0.01595, loss_in_train: 2.15280, loss_in_test: 35.65726\n",
      "iter: 3272, grad: 0.01595, loss_in_train: 2.15280, loss_in_test: 35.65726\n",
      "iter: 3273, grad: 0.01595, loss_in_train: 2.15280, loss_in_test: 35.65726\n",
      "iter: 3274, grad: 0.01595, loss_in_train: 2.15280, loss_in_test: 35.65726\n",
      "iter: 3275, grad: 0.01595, loss_in_train: 2.15280, loss_in_test: 35.65726\n",
      "iter: 3276, grad: 0.01594, loss_in_train: 2.15280, loss_in_test: 35.65726\n",
      "iter: 3277, grad: 0.01594, loss_in_train: 2.15280, loss_in_test: 35.65726\n",
      "iter: 3278, grad: 0.01594, loss_in_train: 2.15280, loss_in_test: 35.65726\n",
      "iter: 3279, grad: 0.01594, loss_in_train: 2.15280, loss_in_test: 35.65726\n",
      "iter: 3280, grad: 0.01594, loss_in_train: 2.15280, loss_in_test: 35.65726\n",
      "iter: 3281, grad: 0.01594, loss_in_train: 2.15280, loss_in_test: 35.65726\n",
      "iter: 3282, grad: 0.01594, loss_in_train: 2.15280, loss_in_test: 35.65726\n",
      "iter: 3283, grad: 0.01594, loss_in_train: 2.15280, loss_in_test: 35.65726\n",
      "iter: 3284, grad: 0.01594, loss_in_train: 2.15280, loss_in_test: 35.65726\n",
      "iter: 3285, grad: 0.01594, loss_in_train: 2.15280, loss_in_test: 35.65726\n",
      "iter: 3286, grad: 0.01594, loss_in_train: 2.15280, loss_in_test: 35.65726\n",
      "iter: 3287, grad: 0.01594, loss_in_train: 2.15280, loss_in_test: 35.65726\n",
      "iter: 3288, grad: 0.01594, loss_in_train: 2.15280, loss_in_test: 35.65726\n",
      "iter: 3289, grad: 0.01594, loss_in_train: 2.15280, loss_in_test: 35.65726\n",
      "iter: 3290, grad: 0.01594, loss_in_train: 2.15280, loss_in_test: 35.65726\n",
      "iter: 3291, grad: 0.01594, loss_in_train: 2.15280, loss_in_test: 35.65727\n",
      "iter: 3292, grad: 0.01594, loss_in_train: 2.15280, loss_in_test: 35.65727\n",
      "iter: 3293, grad: 0.01593, loss_in_train: 2.15280, loss_in_test: 35.65727\n",
      "iter: 3294, grad: 0.01593, loss_in_train: 2.15280, loss_in_test: 35.65727\n",
      "iter: 3295, grad: 0.01593, loss_in_train: 2.15280, loss_in_test: 35.65727\n",
      "iter: 3296, grad: 0.01593, loss_in_train: 2.15280, loss_in_test: 35.65727\n",
      "iter: 3297, grad: 0.01593, loss_in_train: 2.15280, loss_in_test: 35.65727\n",
      "iter: 3298, grad: 0.01593, loss_in_train: 2.15280, loss_in_test: 35.65727\n",
      "iter: 3299, grad: 0.01593, loss_in_train: 2.15280, loss_in_test: 35.65727\n",
      "iter: 3300, grad: 0.01593, loss_in_train: 2.15280, loss_in_test: 35.65727\n",
      "iter: 3301, grad: 0.01593, loss_in_train: 2.15280, loss_in_test: 35.65727\n",
      "iter: 3302, grad: 0.01593, loss_in_train: 2.15280, loss_in_test: 35.65727\n",
      "iter: 3303, grad: 0.01593, loss_in_train: 2.15280, loss_in_test: 35.65727\n",
      "iter: 3304, grad: 0.01593, loss_in_train: 2.15280, loss_in_test: 35.65727\n",
      "iter: 3305, grad: 0.01593, loss_in_train: 2.15280, loss_in_test: 35.65727\n",
      "iter: 3306, grad: 0.01593, loss_in_train: 2.15280, loss_in_test: 35.65727\n",
      "iter: 3307, grad: 0.01593, loss_in_train: 2.15280, loss_in_test: 35.65727\n",
      "iter: 3308, grad: 0.01593, loss_in_train: 2.15280, loss_in_test: 35.65727\n",
      "iter: 3309, grad: 0.01593, loss_in_train: 2.15280, loss_in_test: 35.65727\n",
      "iter: 3310, grad: 0.01592, loss_in_train: 2.15280, loss_in_test: 35.65727\n",
      "iter: 3311, grad: 0.01592, loss_in_train: 2.15280, loss_in_test: 35.65727\n",
      "iter: 3312, grad: 0.01592, loss_in_train: 2.15280, loss_in_test: 35.65727\n",
      "iter: 3313, grad: 0.01592, loss_in_train: 2.15280, loss_in_test: 35.65727\n",
      "iter: 3314, grad: 0.01592, loss_in_train: 2.15280, loss_in_test: 35.65727\n",
      "iter: 3315, grad: 0.01592, loss_in_train: 2.15280, loss_in_test: 35.65728\n",
      "iter: 3316, grad: 0.01592, loss_in_train: 2.15280, loss_in_test: 35.65728\n",
      "iter: 3317, grad: 0.01592, loss_in_train: 2.15280, loss_in_test: 35.65728\n",
      "iter: 3318, grad: 0.01592, loss_in_train: 2.15280, loss_in_test: 35.65728\n",
      "iter: 3319, grad: 0.01592, loss_in_train: 2.15280, loss_in_test: 35.65728\n",
      "iter: 3320, grad: 0.01592, loss_in_train: 2.15280, loss_in_test: 35.65728\n",
      "iter: 3321, grad: 0.01592, loss_in_train: 2.15280, loss_in_test: 35.65728\n",
      "iter: 3322, grad: 0.01592, loss_in_train: 2.15280, loss_in_test: 35.65728\n",
      "iter: 3323, grad: 0.01592, loss_in_train: 2.15280, loss_in_test: 35.65728\n",
      "iter: 3324, grad: 0.01592, loss_in_train: 2.15280, loss_in_test: 35.65728\n",
      "iter: 3325, grad: 0.01592, loss_in_train: 2.15280, loss_in_test: 35.65728\n",
      "iter: 3326, grad: 0.01592, loss_in_train: 2.15280, loss_in_test: 35.65728\n",
      "iter: 3327, grad: 0.01592, loss_in_train: 2.15280, loss_in_test: 35.65728\n",
      "iter: 3328, grad: 0.01591, loss_in_train: 2.15280, loss_in_test: 35.65728\n",
      "iter: 3329, grad: 0.01591, loss_in_train: 2.15280, loss_in_test: 35.65728\n",
      "iter: 3330, grad: 0.01591, loss_in_train: 2.15280, loss_in_test: 35.65728\n",
      "iter: 3331, grad: 0.01591, loss_in_train: 2.15280, loss_in_test: 35.65728\n",
      "iter: 3332, grad: 0.01591, loss_in_train: 2.15280, loss_in_test: 35.65728\n",
      "iter: 3333, grad: 0.01591, loss_in_train: 2.15280, loss_in_test: 35.65728\n",
      "iter: 3334, grad: 0.01591, loss_in_train: 2.15280, loss_in_test: 35.65728\n",
      "iter: 3335, grad: 0.01591, loss_in_train: 2.15280, loss_in_test: 35.65728\n",
      "iter: 3336, grad: 0.01591, loss_in_train: 2.15280, loss_in_test: 35.65728\n",
      "iter: 3337, grad: 0.01591, loss_in_train: 2.15280, loss_in_test: 35.65728\n",
      "iter: 3338, grad: 0.01591, loss_in_train: 2.15280, loss_in_test: 35.65728\n",
      "iter: 3339, grad: 0.01591, loss_in_train: 2.15280, loss_in_test: 35.65728\n",
      "iter: 3340, grad: 0.01591, loss_in_train: 2.15280, loss_in_test: 35.65729\n",
      "iter: 3341, grad: 0.01591, loss_in_train: 2.15280, loss_in_test: 35.65729\n",
      "iter: 3342, grad: 0.01591, loss_in_train: 2.15280, loss_in_test: 35.65729\n",
      "iter: 3343, grad: 0.01591, loss_in_train: 2.15280, loss_in_test: 35.65729\n",
      "iter: 3344, grad: 0.01591, loss_in_train: 2.15280, loss_in_test: 35.65729\n",
      "iter: 3345, grad: 0.01591, loss_in_train: 2.15280, loss_in_test: 35.65729\n",
      "iter: 3346, grad: 0.01591, loss_in_train: 2.15280, loss_in_test: 35.65729\n",
      "iter: 3347, grad: 0.01590, loss_in_train: 2.15280, loss_in_test: 35.65729\n",
      "iter: 3348, grad: 0.01590, loss_in_train: 2.15280, loss_in_test: 35.65729\n",
      "iter: 3349, grad: 0.01590, loss_in_train: 2.15280, loss_in_test: 35.65729\n",
      "iter: 3350, grad: 0.01590, loss_in_train: 2.15280, loss_in_test: 35.65729\n",
      "iter: 3351, grad: 0.01590, loss_in_train: 2.15280, loss_in_test: 35.65729\n",
      "iter: 3352, grad: 0.01590, loss_in_train: 2.15280, loss_in_test: 35.65729\n",
      "iter: 3353, grad: 0.01590, loss_in_train: 2.15280, loss_in_test: 35.65729\n",
      "iter: 3354, grad: 0.01590, loss_in_train: 2.15280, loss_in_test: 35.65729\n",
      "iter: 3355, grad: 0.01590, loss_in_train: 2.15280, loss_in_test: 35.65729\n",
      "iter: 3356, grad: 0.01590, loss_in_train: 2.15280, loss_in_test: 35.65729\n",
      "iter: 3357, grad: 0.01590, loss_in_train: 2.15280, loss_in_test: 35.65729\n",
      "iter: 3358, grad: 0.01590, loss_in_train: 2.15280, loss_in_test: 35.65729\n",
      "iter: 3359, grad: 0.01590, loss_in_train: 2.15280, loss_in_test: 35.65729\n",
      "iter: 3360, grad: 0.01590, loss_in_train: 2.15280, loss_in_test: 35.65729\n",
      "iter: 3361, grad: 0.01590, loss_in_train: 2.15280, loss_in_test: 35.65729\n",
      "iter: 3362, grad: 0.01590, loss_in_train: 2.15280, loss_in_test: 35.65729\n",
      "iter: 3363, grad: 0.01590, loss_in_train: 2.15280, loss_in_test: 35.65729\n",
      "iter: 3364, grad: 0.01590, loss_in_train: 2.15280, loss_in_test: 35.65729\n",
      "iter: 3365, grad: 0.01590, loss_in_train: 2.15280, loss_in_test: 35.65729\n",
      "iter: 3366, grad: 0.01590, loss_in_train: 2.15280, loss_in_test: 35.65729\n",
      "iter: 3367, grad: 0.01590, loss_in_train: 2.15280, loss_in_test: 35.65730\n",
      "iter: 3368, grad: 0.01589, loss_in_train: 2.15280, loss_in_test: 35.65730\n",
      "iter: 3369, grad: 0.01589, loss_in_train: 2.15280, loss_in_test: 35.65730\n",
      "iter: 3370, grad: 0.01589, loss_in_train: 2.15280, loss_in_test: 35.65730\n",
      "iter: 3371, grad: 0.01589, loss_in_train: 2.15280, loss_in_test: 35.65730\n",
      "iter: 3372, grad: 0.01589, loss_in_train: 2.15280, loss_in_test: 35.65730\n",
      "iter: 3373, grad: 0.01589, loss_in_train: 2.15280, loss_in_test: 35.65730\n",
      "iter: 3374, grad: 0.01589, loss_in_train: 2.15280, loss_in_test: 35.65730\n",
      "iter: 3375, grad: 0.01589, loss_in_train: 2.15280, loss_in_test: 35.65730\n",
      "iter: 3376, grad: 0.01589, loss_in_train: 2.15280, loss_in_test: 35.65730\n",
      "iter: 3377, grad: 0.01589, loss_in_train: 2.15280, loss_in_test: 35.65730\n",
      "iter: 3378, grad: 0.01589, loss_in_train: 2.15280, loss_in_test: 35.65730\n",
      "iter: 3379, grad: 0.01589, loss_in_train: 2.15280, loss_in_test: 35.65730\n",
      "iter: 3380, grad: 0.01589, loss_in_train: 2.15280, loss_in_test: 35.65730\n",
      "iter: 3381, grad: 0.01589, loss_in_train: 2.15280, loss_in_test: 35.65730\n",
      "iter: 3382, grad: 0.01589, loss_in_train: 2.15280, loss_in_test: 35.65730\n",
      "iter: 3383, grad: 0.01589, loss_in_train: 2.15280, loss_in_test: 35.65730\n",
      "iter: 3384, grad: 0.01589, loss_in_train: 2.15280, loss_in_test: 35.65730\n",
      "iter: 3385, grad: 0.01589, loss_in_train: 2.15280, loss_in_test: 35.65730\n",
      "iter: 3386, grad: 0.01589, loss_in_train: 2.15280, loss_in_test: 35.65730\n",
      "iter: 3387, grad: 0.01589, loss_in_train: 2.15280, loss_in_test: 35.65730\n",
      "iter: 3388, grad: 0.01589, loss_in_train: 2.15280, loss_in_test: 35.65730\n",
      "iter: 3389, grad: 0.01588, loss_in_train: 2.15280, loss_in_test: 35.65730\n",
      "iter: 3390, grad: 0.01588, loss_in_train: 2.15280, loss_in_test: 35.65730\n",
      "iter: 3391, grad: 0.01588, loss_in_train: 2.15280, loss_in_test: 35.65730\n",
      "iter: 3392, grad: 0.01588, loss_in_train: 2.15280, loss_in_test: 35.65730\n",
      "iter: 3393, grad: 0.01588, loss_in_train: 2.15280, loss_in_test: 35.65730\n",
      "iter: 3394, grad: 0.01588, loss_in_train: 2.15280, loss_in_test: 35.65730\n",
      "iter: 3395, grad: 0.01588, loss_in_train: 2.15280, loss_in_test: 35.65730\n",
      "iter: 3396, grad: 0.01588, loss_in_train: 2.15280, loss_in_test: 35.65731\n",
      "iter: 3397, grad: 0.01588, loss_in_train: 2.15280, loss_in_test: 35.65731\n",
      "iter: 3398, grad: 0.01588, loss_in_train: 2.15280, loss_in_test: 35.65731\n",
      "iter: 3399, grad: 0.01588, loss_in_train: 2.15280, loss_in_test: 35.65731\n",
      "iter: 3400, grad: 0.01588, loss_in_train: 2.15280, loss_in_test: 35.65731\n",
      "iter: 3401, grad: 0.01588, loss_in_train: 2.15280, loss_in_test: 35.65731\n",
      "iter: 3402, grad: 0.01588, loss_in_train: 2.15280, loss_in_test: 35.65731\n",
      "iter: 3403, grad: 0.01588, loss_in_train: 2.15280, loss_in_test: 35.65731\n",
      "iter: 3404, grad: 0.01588, loss_in_train: 2.15280, loss_in_test: 35.65731\n",
      "iter: 3405, grad: 0.01588, loss_in_train: 2.15280, loss_in_test: 35.65731\n",
      "iter: 3406, grad: 0.01588, loss_in_train: 2.15280, loss_in_test: 35.65731\n",
      "iter: 3407, grad: 0.01588, loss_in_train: 2.15280, loss_in_test: 35.65731\n",
      "iter: 3408, grad: 0.01588, loss_in_train: 2.15280, loss_in_test: 35.65731\n",
      "iter: 3409, grad: 0.01588, loss_in_train: 2.15280, loss_in_test: 35.65731\n",
      "iter: 3410, grad: 0.01588, loss_in_train: 2.15280, loss_in_test: 35.65731\n",
      "iter: 3411, grad: 0.01587, loss_in_train: 2.15280, loss_in_test: 35.65731\n",
      "iter: 3412, grad: 0.01587, loss_in_train: 2.15280, loss_in_test: 35.65731\n",
      "iter: 3413, grad: 0.01587, loss_in_train: 2.15280, loss_in_test: 35.65731\n",
      "iter: 3414, grad: 0.01587, loss_in_train: 2.15280, loss_in_test: 35.65731\n",
      "iter: 3415, grad: 0.01587, loss_in_train: 2.15280, loss_in_test: 35.65731\n",
      "iter: 3416, grad: 0.01587, loss_in_train: 2.15280, loss_in_test: 35.65731\n",
      "iter: 3417, grad: 0.01587, loss_in_train: 2.15280, loss_in_test: 35.65731\n",
      "iter: 3418, grad: 0.01587, loss_in_train: 2.15280, loss_in_test: 35.65731\n",
      "iter: 3419, grad: 0.01587, loss_in_train: 2.15280, loss_in_test: 35.65731\n",
      "iter: 3420, grad: 0.01587, loss_in_train: 2.15280, loss_in_test: 35.65731\n",
      "iter: 3421, grad: 0.01587, loss_in_train: 2.15280, loss_in_test: 35.65731\n",
      "iter: 3422, grad: 0.01587, loss_in_train: 2.15280, loss_in_test: 35.65731\n",
      "iter: 3423, grad: 0.01587, loss_in_train: 2.15280, loss_in_test: 35.65731\n",
      "iter: 3424, grad: 0.01587, loss_in_train: 2.15280, loss_in_test: 35.65731\n",
      "iter: 3425, grad: 0.01587, loss_in_train: 2.15280, loss_in_test: 35.65731\n",
      "iter: 3426, grad: 0.01587, loss_in_train: 2.15280, loss_in_test: 35.65731\n",
      "iter: 3427, grad: 0.01587, loss_in_train: 2.15280, loss_in_test: 35.65732\n",
      "iter: 3428, grad: 0.01587, loss_in_train: 2.15280, loss_in_test: 35.65732\n",
      "iter: 3429, grad: 0.01587, loss_in_train: 2.15280, loss_in_test: 35.65732\n",
      "iter: 3430, grad: 0.01587, loss_in_train: 2.15280, loss_in_test: 35.65732\n",
      "iter: 3431, grad: 0.01587, loss_in_train: 2.15280, loss_in_test: 35.65732\n",
      "iter: 3432, grad: 0.01587, loss_in_train: 2.15280, loss_in_test: 35.65732\n",
      "iter: 3433, grad: 0.01587, loss_in_train: 2.15280, loss_in_test: 35.65732\n",
      "iter: 3434, grad: 0.01587, loss_in_train: 2.15280, loss_in_test: 35.65732\n",
      "iter: 3435, grad: 0.01586, loss_in_train: 2.15280, loss_in_test: 35.65732\n",
      "iter: 3436, grad: 0.01586, loss_in_train: 2.15280, loss_in_test: 35.65732\n",
      "iter: 3437, grad: 0.01586, loss_in_train: 2.15280, loss_in_test: 35.65732\n",
      "iter: 3438, grad: 0.01586, loss_in_train: 2.15280, loss_in_test: 35.65732\n",
      "iter: 3439, grad: 0.01586, loss_in_train: 2.15280, loss_in_test: 35.65732\n",
      "iter: 3440, grad: 0.01586, loss_in_train: 2.15280, loss_in_test: 35.65732\n",
      "iter: 3441, grad: 0.01586, loss_in_train: 2.15280, loss_in_test: 35.65732\n",
      "iter: 3442, grad: 0.01586, loss_in_train: 2.15280, loss_in_test: 35.65732\n",
      "iter: 3443, grad: 0.01586, loss_in_train: 2.15280, loss_in_test: 35.65732\n",
      "iter: 3444, grad: 0.01586, loss_in_train: 2.15280, loss_in_test: 35.65732\n",
      "iter: 3445, grad: 0.01586, loss_in_train: 2.15280, loss_in_test: 35.65732\n",
      "iter: 3446, grad: 0.01586, loss_in_train: 2.15280, loss_in_test: 35.65732\n",
      "iter: 3447, grad: 0.01586, loss_in_train: 2.15280, loss_in_test: 35.65732\n",
      "iter: 3448, grad: 0.01586, loss_in_train: 2.15280, loss_in_test: 35.65732\n",
      "iter: 3449, grad: 0.01586, loss_in_train: 2.15280, loss_in_test: 35.65732\n",
      "iter: 3450, grad: 0.01586, loss_in_train: 2.15280, loss_in_test: 35.65732\n",
      "iter: 3451, grad: 0.01586, loss_in_train: 2.15280, loss_in_test: 35.65732\n",
      "iter: 3452, grad: 0.01586, loss_in_train: 2.15280, loss_in_test: 35.65732\n",
      "iter: 3453, grad: 0.01586, loss_in_train: 2.15280, loss_in_test: 35.65732\n",
      "iter: 3454, grad: 0.01586, loss_in_train: 2.15280, loss_in_test: 35.65732\n",
      "iter: 3455, grad: 0.01586, loss_in_train: 2.15280, loss_in_test: 35.65732\n",
      "iter: 3456, grad: 0.01586, loss_in_train: 2.15280, loss_in_test: 35.65732\n",
      "iter: 3457, grad: 0.01586, loss_in_train: 2.15280, loss_in_test: 35.65732\n",
      "iter: 3458, grad: 0.01586, loss_in_train: 2.15280, loss_in_test: 35.65732\n",
      "iter: 3459, grad: 0.01586, loss_in_train: 2.15280, loss_in_test: 35.65732\n",
      "iter: 3460, grad: 0.01585, loss_in_train: 2.15280, loss_in_test: 35.65732\n",
      "iter: 3461, grad: 0.01585, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3462, grad: 0.01585, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3463, grad: 0.01585, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3464, grad: 0.01585, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3465, grad: 0.01585, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3466, grad: 0.01585, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3467, grad: 0.01585, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3468, grad: 0.01585, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3469, grad: 0.01585, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3470, grad: 0.01585, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3471, grad: 0.01585, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3472, grad: 0.01585, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3473, grad: 0.01585, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3474, grad: 0.01585, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3475, grad: 0.01585, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3476, grad: 0.01585, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3477, grad: 0.01585, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3478, grad: 0.01585, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3479, grad: 0.01585, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3480, grad: 0.01585, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3481, grad: 0.01585, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3482, grad: 0.01585, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3483, grad: 0.01585, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3484, grad: 0.01585, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3485, grad: 0.01585, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3486, grad: 0.01585, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3487, grad: 0.01584, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3488, grad: 0.01584, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3489, grad: 0.01584, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3490, grad: 0.01584, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3491, grad: 0.01584, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3492, grad: 0.01584, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3493, grad: 0.01584, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3494, grad: 0.01584, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3495, grad: 0.01584, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3496, grad: 0.01584, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3497, grad: 0.01584, loss_in_train: 2.15280, loss_in_test: 35.65733\n",
      "iter: 3498, grad: 0.01584, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3499, grad: 0.01584, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3500, grad: 0.01584, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3501, grad: 0.01584, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3502, grad: 0.01584, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3503, grad: 0.01584, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3504, grad: 0.01584, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3505, grad: 0.01584, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3506, grad: 0.01584, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3507, grad: 0.01584, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3508, grad: 0.01584, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3509, grad: 0.01584, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3510, grad: 0.01584, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3511, grad: 0.01584, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3512, grad: 0.01584, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3513, grad: 0.01584, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3514, grad: 0.01584, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3515, grad: 0.01583, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3516, grad: 0.01583, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3517, grad: 0.01583, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3518, grad: 0.01583, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3519, grad: 0.01583, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3520, grad: 0.01583, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3521, grad: 0.01583, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3522, grad: 0.01583, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3523, grad: 0.01583, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3524, grad: 0.01583, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3525, grad: 0.01583, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3526, grad: 0.01583, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3527, grad: 0.01583, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3528, grad: 0.01583, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3529, grad: 0.01583, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3530, grad: 0.01583, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3531, grad: 0.01583, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3532, grad: 0.01583, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3533, grad: 0.01583, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3534, grad: 0.01583, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3535, grad: 0.01583, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3536, grad: 0.01583, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3537, grad: 0.01583, loss_in_train: 2.15280, loss_in_test: 35.65734\n",
      "iter: 3538, grad: 0.01583, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3539, grad: 0.01583, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3540, grad: 0.01583, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3541, grad: 0.01583, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3542, grad: 0.01583, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3543, grad: 0.01583, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3544, grad: 0.01583, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3545, grad: 0.01583, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3546, grad: 0.01582, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3547, grad: 0.01582, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3548, grad: 0.01582, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3549, grad: 0.01582, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3550, grad: 0.01582, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3551, grad: 0.01582, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3552, grad: 0.01582, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3553, grad: 0.01582, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3554, grad: 0.01582, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3555, grad: 0.01582, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3556, grad: 0.01582, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3557, grad: 0.01582, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3558, grad: 0.01582, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3559, grad: 0.01582, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3560, grad: 0.01582, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3561, grad: 0.01582, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3562, grad: 0.01582, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3563, grad: 0.01582, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3564, grad: 0.01582, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3565, grad: 0.01582, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3566, grad: 0.01582, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3567, grad: 0.01582, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3568, grad: 0.01582, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3569, grad: 0.01582, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3570, grad: 0.01582, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3571, grad: 0.01582, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3572, grad: 0.01582, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3573, grad: 0.01582, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3574, grad: 0.01582, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3575, grad: 0.01582, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3576, grad: 0.01582, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3577, grad: 0.01582, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3578, grad: 0.01582, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3579, grad: 0.01582, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3580, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3581, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3582, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3583, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65735\n",
      "iter: 3584, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3585, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3586, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3587, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3588, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3589, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3590, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3591, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3592, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3593, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3594, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3595, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3596, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3597, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3598, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3599, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3600, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3601, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3602, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3603, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3604, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3605, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3606, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3607, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3608, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3609, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3610, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3611, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3612, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3613, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3614, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3615, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3616, grad: 0.01581, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3617, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3618, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3619, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3620, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3621, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3622, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3623, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3624, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3625, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3626, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3627, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3628, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3629, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3630, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3631, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3632, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3633, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65736\n",
      "iter: 3634, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3635, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3636, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3637, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3638, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3639, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3640, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3641, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3642, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3643, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3644, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3645, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3646, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3647, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3648, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3649, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3650, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3651, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3652, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3653, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3654, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3655, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3656, grad: 0.01580, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3657, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3658, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3659, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3660, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3661, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3662, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3663, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3664, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3665, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3666, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3667, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3668, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3669, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3670, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3671, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3672, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3673, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3674, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3675, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3676, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3677, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3678, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3679, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3680, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3681, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3682, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3683, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3684, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3685, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3686, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3687, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3688, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3689, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3690, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3691, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3692, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65737\n",
      "iter: 3693, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3694, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3695, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3696, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3697, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3698, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3699, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3700, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3701, grad: 0.01579, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3702, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3703, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3704, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3705, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3706, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3707, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3708, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3709, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3710, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3711, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3712, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3713, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3714, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3715, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3716, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3717, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3718, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3719, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3720, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3721, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3722, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3723, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3724, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3725, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3726, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3727, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3728, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3729, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3730, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3731, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3732, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3733, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3734, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3735, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3736, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3737, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3738, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3739, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3740, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3741, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3742, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3743, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3744, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3745, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3746, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3747, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3748, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3749, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3750, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3751, grad: 0.01578, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3752, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3753, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3754, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3755, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3756, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3757, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3758, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3759, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3760, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65738\n",
      "iter: 3761, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3762, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3763, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3764, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3765, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3766, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3767, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3768, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3769, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3770, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3771, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3772, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3773, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3774, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3775, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3776, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3777, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3778, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3779, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3780, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3781, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3782, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3783, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3784, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3785, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3786, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3787, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3788, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3789, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3790, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3791, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3792, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3793, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3794, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3795, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3796, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3797, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3798, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3799, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3800, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3801, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3802, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3803, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3804, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3805, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3806, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3807, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3808, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3809, grad: 0.01577, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3810, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3811, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3812, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3813, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3814, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3815, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3816, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3817, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3818, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3819, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3820, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3821, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3822, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3823, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3824, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3825, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3826, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3827, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3828, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3829, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3830, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3831, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3832, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3833, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3834, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3835, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3836, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3837, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3838, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3839, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3840, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3841, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3842, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3843, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65739\n",
      "iter: 3844, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3845, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3846, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3847, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3848, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3849, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3850, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3851, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3852, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3853, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3854, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3855, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3856, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3857, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3858, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3859, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3860, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3861, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3862, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3863, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3864, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3865, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3866, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3867, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3868, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3869, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3870, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3871, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3872, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3873, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3874, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3875, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3876, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3877, grad: 0.01576, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3878, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3879, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3880, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3881, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3882, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3883, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3884, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3885, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3886, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3887, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3888, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3889, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3890, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3891, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3892, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3893, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3894, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3895, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3896, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3897, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3898, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3899, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3900, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3901, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3902, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3903, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3904, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3905, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3906, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3907, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3908, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3909, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3910, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3911, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3912, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3913, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3914, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3915, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3916, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3917, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3918, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3919, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3920, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3921, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3922, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3923, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3924, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3925, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3926, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3927, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3928, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3929, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3930, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3931, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3932, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3933, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3934, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3935, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3936, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3937, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3938, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3939, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3940, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3941, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3942, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3943, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3944, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3945, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3946, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3947, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65740\n",
      "iter: 3948, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3949, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3950, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3951, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3952, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3953, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3954, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3955, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3956, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3957, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3958, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3959, grad: 0.01575, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3960, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3961, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3962, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3963, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3964, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3965, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3966, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3967, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3968, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3969, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3970, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3971, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3972, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3973, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3974, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3975, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3976, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3977, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3978, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3979, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3980, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3981, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3982, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3983, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3984, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3985, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3986, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3987, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3988, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3989, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3990, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3991, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3992, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3993, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3994, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3995, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3996, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3997, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3998, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 3999, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4000, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4001, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4002, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4003, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4004, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4005, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4006, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4007, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4008, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4009, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4010, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4011, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4012, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4013, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4014, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4015, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4016, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4017, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4018, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4019, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4020, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4021, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4022, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4023, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4024, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4025, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4026, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4027, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4028, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4029, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4030, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4031, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4032, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4033, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4034, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4035, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4036, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4037, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4038, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4039, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4040, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4041, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4042, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4043, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4044, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4045, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4046, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4047, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4048, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4049, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4050, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4051, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4052, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4053, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4054, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4055, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4056, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4057, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4058, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4059, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4060, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4061, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4062, grad: 0.01574, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4063, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4064, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4065, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4066, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4067, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4068, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4069, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4070, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4071, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4072, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4073, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4074, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4075, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4076, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4077, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4078, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4079, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4080, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4081, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4082, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4083, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4084, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4085, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4086, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4087, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4088, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65741\n",
      "iter: 4089, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4090, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4091, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4092, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4093, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4094, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4095, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4096, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4097, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4098, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4099, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4100, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4101, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4102, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4103, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4104, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4105, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4106, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4107, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4108, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4109, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4110, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4111, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4112, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4113, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4114, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4115, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4116, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4117, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4118, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4119, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4120, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4121, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4122, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4123, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4124, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4125, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4126, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4127, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4128, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4129, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4130, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4131, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4132, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4133, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4134, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4135, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4136, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4137, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4138, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4139, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4140, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4141, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4142, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4143, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4144, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4145, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4146, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4147, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4148, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4149, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4150, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4151, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4152, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4153, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4154, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4155, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4156, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4157, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4158, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4159, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4160, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4161, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4162, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4163, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4164, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4165, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4166, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4167, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4168, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4169, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4170, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4171, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4172, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4173, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4174, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4175, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4176, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4177, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4178, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4179, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4180, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4181, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4182, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4183, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4184, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4185, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4186, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4187, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4188, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4189, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4190, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4191, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4192, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4193, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n",
      "iter: 4194, grad: 0.01573, loss_in_train: 2.15280, loss_in_test: 35.65742\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [4]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m reg \u001B[38;5;241m=\u001B[39m GradientDescentLinerRegressionWithRegularization(learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-3\u001B[39m, _lambda\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, min_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.01\u001B[39m, seed\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1024\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m \u001B[43mreg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_x\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_y\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [2]\u001B[0m, in \u001B[0;36mGradientDescentLinerRegressionWithRegularization.fit\u001B[0;34m(self, x, y)\u001B[0m\n\u001B[1;32m     20\u001B[0m i \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m---> 22\u001B[0m     d_w \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_train_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_arr\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss())\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miter: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, grad: \u001B[39m\u001B[38;5;132;01m{:.5f}\u001B[39;00m\u001B[38;5;124m, loss_in_train: \u001B[39m\u001B[38;5;132;01m{:.5f}\u001B[39;00m\u001B[38;5;124m, loss_in_test: \u001B[39m\u001B[38;5;132;01m{:.5f}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(i, np\u001B[38;5;241m.\u001B[39maverage(d_w),\n\u001B[1;32m     25\u001B[0m                                                                                        \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss(is_test\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[1;32m     26\u001B[0m                                                                                        \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_arr[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]))\n",
      "Input \u001B[0;32mIn [2]\u001B[0m, in \u001B[0;36mGradientDescentLinerRegressionWithRegularization._train_step\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_train_step\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m---> 58\u001B[0m     d_w \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_calc_gradient\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     59\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mw \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mw \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlr \u001B[38;5;241m*\u001B[39m d_w\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mintercept_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mw[\u001B[38;5;241m0\u001B[39m]\n",
      "Input \u001B[0;32mIn [2]\u001B[0m, in \u001B[0;36mGradientDescentLinerRegressionWithRegularization._calc_gradient\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     52\u001B[0m d_w[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msum(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mx_b\u001B[38;5;241m.\u001B[39mdot(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mw) \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39my)\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mx_b_dim):\n\u001B[0;32m---> 54\u001B[0m     d_w[i] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msqueeze((\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx_b\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mw\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39my))\u001B[38;5;241m.\u001B[39mdot(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mx_b[:, i]\u001B[38;5;241m.\u001B[39mT)\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m d_w \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mx_sample \u001B[38;5;241m+\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lambda \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mx_sample \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mw)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "reg = GradientDescentLinerRegressionWithRegularization(learning_rate=1e-3, _lambda=100, min_grad=0.01, max_item=5000, seed=1024)\n",
    "reg.fit(train_x, train_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}