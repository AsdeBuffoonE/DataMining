{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def compute_entropy(y_class):\n",
    "    \"\"\"计算信息熵\"\"\"\n",
    "    y_unique = np.unique(y_class)\n",
    "    if y_unique.shape[0] == 1:  # 只有一个类别, 熵为0\n",
    "        return 0.\n",
    "    ety = 0.\n",
    "    for i in range(len(y_unique)):  # 取每个类别\n",
    "        p = np.sum(y_class == y_unique[i]) / len(y_class)\n",
    "        ety += p * np.log2(p)\n",
    "    return -ety\n",
    "\n",
    "\n",
    "def compute_condition_entropy(X_feature, y_class):\n",
    "    \"\"\"计算条件熵\"\"\"\n",
    "    f_unique = np.unique(X_feature)\n",
    "    result = 0.\n",
    "    for i in range(len(f_unique)):\n",
    "        index_x = (X_feature == f_unique[i])\n",
    "        p = np.sum(index_x) / len(y_class)\n",
    "        ety = compute_entropy(y_class[index_x])\n",
    "        result += p * ety\n",
    "    return result\n",
    "\n",
    "\n",
    "def compute_cost_in_leaf(labels):\n",
    "    \"\"\"\n",
    "    计算节点的损失\n",
    "    \"\"\"\n",
    "    y_count = np.bincount(labels)\n",
    "    n_samples = len(labels)\n",
    "    cost = 0\n",
    "    # 计算整个节点内部的损失值\n",
    "    for i in range(len(y_count)):\n",
    "        if y_count[i] == 0:\n",
    "            continue\n",
    "        cost += y_count[i] * np.log2(y_count[i] / n_samples)\n",
    "    return -cost\n",
    "\n",
    "\n",
    "def get_label(labels):\n",
    "    \"\"\"\n",
    "    根据给定标签，返回出现次数最多的类别\n",
    "    \"\"\"\n",
    "    r = {}\n",
    "    for i in range(len(labels)):\n",
    "        r[labels[i]] = r.setdefault(labels[i], 0) + 1\n",
    "    return sorted(r.items(), key=lambda x: x[1])[-1][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, ):\n",
    "        self.sample_index = None  # 保存当前节点中对应样本在数据集中的索引\n",
    "        self.values = None  # 保存每个类别的数量\n",
    "        self.features = None  # 保存当前节点状态时特征集中剩余特征\n",
    "        self.feature_id = -1  # 保存当前节点对应划分特征的id\n",
    "        self.label = None  # 保存当前节点对应的类别标签（叶子节点才有）\n",
    "        self.n_samples = 0  # 保存当前节点对应的样本数量\n",
    "        self.children = {}  # 保存当前节点对应的孩子节点\n",
    "        self.criterion_value = 0.\n",
    "        self.n_leaf = 0  # 以当前节点为根节点时其叶子节点的个数\n",
    "        self.leaf_costs = 0.\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<======================>\\n\" \\\n",
    "               f\"当前节点所有样本的索引({self.sample_index})\\n\" \\\n",
    "               f\"当前节点的样本数量({self.n_samples})\\n\" \\\n",
    "               f\"当前节点每个类别的样本数({self.values})\\n\" \\\n",
    "               f\"当前节点对应的信息增益（比）({round(self.criterion_value, 3)})\\n\" \\\n",
    "               f\"当前节点状态时特征集中剩余特征({self.features})\\n\" \\\n",
    "               f\"当前节点状态时划分特征ID({self.feature_id})\\n\" \\\n",
    "               f\"当前节点对应的类别标签为({self.label})\\n\" \\\n",
    "               f\"当前节点为根节点对应孩子节点数为({self.n_leaf})\\n\" \\\n",
    "               f\"当前节点为根节点对应孩子节点损失为({self.leaf_costs})\\n\" \\\n",
    "               f\"当前节点对应的孩子为({self.children.keys()})\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    def __init__(self, min_samples_split=2, criterion='id3', epsilon=1e-5, alpha=0.):\n",
    "        self._X = None\n",
    "        self.n_classes = None\n",
    "        self._y = None\n",
    "        self.root = None\n",
    "        self.min_samples_split = min_samples_split  # 用来控制是否停止分裂\n",
    "        self.epsilon = epsilon  # 用来控制是否停止分裂\n",
    "        self.criterion = criterion  # 划分标注，ID3还是C4.5\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"训练构建决策树\"\"\"\n",
    "        self._y = np.array(y).reshape(-1)\n",
    "        self.n_classes = len(np.bincount(y))  # 得到当前数据集的类别数量\n",
    "        feature_ids = [i for i in range(X.shape[1])]  # 得到特征的序号\n",
    "        self._X = np.hstack(([X, np.arange(len(X)).reshape(-1, 1)]))\n",
    "        self._build_tree(self._X, feature_ids)  # 递归构建决策树\n",
    "        self._pruning_leaf()\n",
    "        return self\n",
    "\n",
    "    def _predict_one_sample(self, x):\n",
    "        \"\"\"\n",
    "        预测单一样本\n",
    "        \"\"\"\n",
    "        current_node = self.root\n",
    "        while True:\n",
    "            current_feature_id = current_node.feature_id\n",
    "            current_feature = x[current_feature_id]\n",
    "\n",
    "            # 当前节点为叶子节点，或者由于数据集不充分当前节点的孩子节点不存在下一个划分节点的某一个取值\n",
    "            if len(current_node.children) < 1 or current_feature not in current_node.children:\n",
    "                return current_node.values\n",
    "            current_node = current_node.children[current_feature]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        预测样本\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for x in X:\n",
    "            results.append(self._predict_one_sample(x))\n",
    "        results = np.array(results)\n",
    "        logging.debug(f\"原始预测结果为:\\n{results}\")\n",
    "        y_pred = np.argmax(results, axis=1)\n",
    "        return y_pred\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"计算样本在测试集上准确率\"\"\"\n",
    "        pred = dt.predict(X)\n",
    "        print(pred)\n",
    "        score = 0\n",
    "        for i in range(len(pred)):\n",
    "            if pred[i] == y[i]:\n",
    "                score += 1\n",
    "\n",
    "        return score / len(pred)\n",
    "\n",
    "    def _split_criterion(self, ety, X_feature, y_class):\n",
    "        \"\"\"返回不同分类决策下的选择\"\"\"\n",
    "        c_ety = compute_condition_entropy(X_feature, y_class)\n",
    "        info_gains = ety - c_ety  # 计算信息增益\n",
    "        if self.criterion == \"id3\":\n",
    "            return info_gains\n",
    "        elif self.criterion == \"c45\":\n",
    "            f_ety = compute_entropy(X_feature)\n",
    "            if f_ety == 0:\n",
    "                return 0\n",
    "            else:\n",
    "                return info_gains / f_ety  # 计算信息增益率\n",
    "        else:\n",
    "            raise ValueError(f\"划分标准 self.criterion = {self.criterion}只能是 id3 和 c45 其中之一！\")\n",
    "\n",
    "    def _build_tree(self, data, f_ids):\n",
    "        \"\"\"构建决策树\"\"\"\n",
    "        x_ids = np.array(data[:, -1], dtype=int).reshape(-1)  # 得到每个样本在原始数据集中的索引\n",
    "        labels = self._y[x_ids]\n",
    "\n",
    "        # 定义当前节点并保存对应的节点信息\n",
    "        node = Node()\n",
    "        node.sample_index = x_ids\n",
    "        node.n_samples = len(labels)\n",
    "        node.values = np.bincount(labels, minlength=self.n_classes)\n",
    "        node.features = f_ids\n",
    "        # 为空则将当前节点作为根节点\n",
    "        if self.root is None:\n",
    "            self.root = node\n",
    "\n",
    "        y_unique = np.unique(labels)\n",
    "        # 最小样本数的判断条件\n",
    "        if y_unique.shape[0] == 1 or len(f_ids) < 1 or node.n_samples <= self.min_samples_split:\n",
    "            node.label = get_label(labels)  # 确定当前节点对应的类别\n",
    "            return node\n",
    "        ety = compute_entropy(labels)  # 计算当前节点所有样本对应的信息熵\n",
    "        node.criterion_value = ety\n",
    "\n",
    "        max_criterion = 0\n",
    "        best_feature_id = -1\n",
    "        # 遍历当前节点可进行选择的划分特征，然后计算各个特征划分下的信息增益（比）来选择最佳特征\n",
    "        for id in f_ids:\n",
    "            criterion = self._split_criterion(ety, data[:, id], labels)\n",
    "            if criterion > max_criterion:\n",
    "                max_criterion = criterion\n",
    "                best_feature_id = id\n",
    "        # 当前计算得到的最大划分指标是否小于阈值，如果是则直接返回当前节点\n",
    "        if max_criterion < self.epsilon:\n",
    "            node.label = get_label(labels)\n",
    "            return node\n",
    "        node.feature_id = best_feature_id\n",
    "        feature_values = np.unique(data[:, best_feature_id])  # 取最佳划分特征中特征的取值情况\n",
    "        candidate_ids = copy.copy(f_ids)  # f_ids是一个列表传入函数_build_tree时本质上是传入的地址，因此需要先进行复制\n",
    "        candidate_ids.remove(best_feature_id)\n",
    "        # 根据当前特征维度的取值，来取对应的样本索引，并递归的进行节点划分\n",
    "        for f in feature_values:\n",
    "            c = data[:, best_feature_id] == f\n",
    "            index = np.array([i for i in range(len(c)) if c[i] == True])\n",
    "            node.children[f] = self._build_tree(data[index], candidate_ids)\n",
    "        return node\n",
    "\n",
    "    def level_order(self, return_node=False):\n",
    "        \"\"\"进行决策树的遍历\"\"\"\n",
    "        root = self.root\n",
    "        if not root:\n",
    "            return []\n",
    "        queue = [root]\n",
    "        res = []\n",
    "        # 利用队列遍历所有节点\n",
    "        while queue:\n",
    "            tmp = []\n",
    "            for i in range(len(queue)):\n",
    "                node = queue.pop(0)\n",
    "                tmp.append(node)\n",
    "                for k, v in node.children.items():\n",
    "                    queue.append(v)\n",
    "            res.append(tmp)\n",
    "        if return_node:\n",
    "            return res  # 返回所有层次遍历的结果(各层节点的地址)\n",
    "\n",
    "        # 直接输出整个层次遍历的结果\n",
    "        for i, r in enumerate(res):\n",
    "            logging.debug(f\"第{i + 1}层的节点为：\")\n",
    "            for node in r:\n",
    "                logging.debug(node)\n",
    "\n",
    "    def _pruning_leaf(self):\n",
    "        \"\"\"进行剪枝\"\"\"\n",
    "        level_order_nodes = self.level_order(return_node=True)\n",
    "        # 获取得到层次遍历的所有结果\n",
    "        logging.debug(f\"正在进行剪枝操作……\")\n",
    "        for i in range(len(level_order_nodes) - 1, -1, -1):\n",
    "            # 从下往上依次遍历每一层节点\n",
    "            current_level_nodes = level_order_nodes[i]  # 取第i层的所有节点\n",
    "            for j in range(len(current_level_nodes)):\n",
    "                current_node = current_level_nodes[j]  # 取第i层的第j个节点\n",
    "                if len(current_node.children) == 0:  # 当前节点为叶子节点时\n",
    "                    current_node.n_leaf = 1  # 令其叶节点个数为1\n",
    "                else:\n",
    "                    for _, leaf_node in current_node.children.items():\n",
    "                        current_node.n_leaf += leaf_node.n_leaf  # 统计以当前节点为根节点时的叶子节点数量\n",
    "                if self._is_pruning_leaf(current_node):\n",
    "                    current_node.children = {}\n",
    "                    current_node.n_leaf = 1\n",
    "\n",
    "    def _is_pruning_leaf(self, node):\n",
    "        \"\"\"\n",
    "        判断是否对当前节点进行剪枝\n",
    "        \"\"\"\n",
    "        if len(node.children) < 1:  # 当前节点为叶子节点时，计算叶子节点对应的损失\n",
    "            node.leaf_costs = compute_cost_in_leaf(self._y[node.sample_index])\n",
    "            return False\n",
    "        parent_cost = compute_cost_in_leaf(self._y[node.sample_index])  # 剪枝后的损失\n",
    "        for (_, leaf_node) in node.children.items():  # 剪枝前累加所有叶子节点的损失\n",
    "            node.leaf_costs += leaf_node.leaf_costs\n",
    "        # logging.debug(f\"当前节点的损失为：{parent_cost} + {self.alpha}\")\n",
    "        # logging.debug(f\"当前节点的孩子节点损失和为：{node.leaf_costs} + {self.alpha} * {node.n_leaf}\")\n",
    "\n",
    "        #  当剪枝前的损失  >  剪枝后的损失， 则表示当前节点可以进行剪枝（减掉其所有孩子）\n",
    "        if node.leaf_costs + self.alpha * node.n_leaf > parent_cost + self.alpha:\n",
    "            return True\n",
    "        return False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class FeatureDiscretization(object):\n",
    "    def __init__(self, continuous_columns):\n",
    "        self.continuous_columns = continuous_columns\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        for col_name in self.continuous_columns:\n",
    "            col = x[col_name].to_numpy()\n",
    "            col_sorted = np.sort(col)\n",
    "            dividing_point_list = []\n",
    "            for i in np.arange(0, len(col_sorted) - 1):\n",
    "                avg = np.average([col_sorted[i], col_sorted[i + 1]])\n",
    "                if avg not in dividing_point_list:\n",
    "                    dividing_point_list.append(avg)\n",
    "            dividing_value = self.get_dividing_value(col, dividing_point_list, y)\n",
    "            if dividing_value is not None:\n",
    "                col_new = col.copy()\n",
    "                col_new[col >= dividing_value] = 1\n",
    "                col_new[col < dividing_value] = 0\n",
    "                x[col_name] = col_new\n",
    "        return x\n",
    "\n",
    "    def get_dividing_value(self, col, dividing_point_list, y):\n",
    "        gain_list = []\n",
    "        for value in dividing_point_list:\n",
    "            col_new = col.copy()\n",
    "            col_new[col >= value] = 1\n",
    "            col_new[col < value] = 0\n",
    "\n",
    "            # 计算信息增益率\n",
    "            ety = compute_entropy(col_new)\n",
    "            if ety == 0:\n",
    "                gain_list.append(0)\n",
    "            else:\n",
    "                gain_list.append((compute_entropy(y) - compute_condition_entropy(col_new, y)) / ety)\n",
    "        return dividing_point_list[np.argmax(gain_list)] if len(gain_list) != 0 else None"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 1 1 2 1 1 1 1 2 2 2 1 2 1 2 1 1 2 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 2 1\n",
      " 2 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 2 2 1 1 1 1 2 1 1 1 1 1\n",
      " 2 1 2 1 1 1 2 1 1 1 1 1 1 2 1 2 1 1 2 1 1 2 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1\n",
      " 1 1 2 1 1 2 1 1 1 2 1 1 1 2 1 1 1 1 2 1 2 1 1 1 1 1 2 1 1 1 1 1 2 1 1 1 1\n",
      " 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 2 1 1 1 1 1 2 2 2 1 2\n",
      " 1 2 1 2 1 2 2 2 1 1 2 1 2 1 2 1 1 1 2 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 2 2 1 1 1 1 1 1 1 2 2 1 1 1 2 1 2 1 1 1 1 1 1 2 1 1 2 1 1 1 1 2 1\n",
      " 1 1 1 1 1 1 1 1 1 2 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 1 1 2\n",
      " 1 1 1 1 1 2 2 1 2 1 1 2 2 1 1 1 1 2 1 2 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 2 2\n",
      " 2 2 2 1 2 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 2 1 2 1 2 1 2 1 1 1 1 2 1 1 1 2 1\n",
      " 1 1 1 1 1 2 1 1 2 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 2 1\n",
      " 1 1 2 1 1 2 1 2 1 2 1 1 2 1 1 1 1 2 1 1 1 1 1 1 2 1 1 1 2 1 1 1 2 1 1 1 2\n",
      " 2 1 2 1 1 2 1 1 1 1 2 1 1 2 1 1 1 1 1 1 1 1 2 1 1 1 2 2 2 1 2 1 1 1 1 1 1\n",
      " 1 1 1 1 2 1 1 1 1 1 2 1 1 1 1 2 1 1 1 2 1 1 2 2 2 1 2 1 1 2 1 1 1 1 1 1 2\n",
      " 1 1 1 2 2 1 1 1 1 1 2 1 1 2 1 1 1 2 1 1 2 1 2 1 2 2 1 2 1 1 2 1 1 1 2 1 1\n",
      " 2 2 2 2 2 1 2 1 2 1 1 2 1 1 2 2 1 1 1 1 1 1 1 2 1 2 1 1 2 1 1 1 1 2 1 1 1\n",
      " 1 2 2 2 2 2 2 1 1 2 2 2 1 1 1 2 1 1 2 2 1 1 2 1 1 1 2 1 1 2 2 1 2 1 1 2 1\n",
      " 1 1 2 1 2 2 1 1 1 1 2 2 1 2 1 1 2 1 2 2 2 1 2 2 2 1 1 2 1 1 1 1 2 1 1 1 1\n",
      " 1 1 2 1 1 1 1 1 2 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1\n",
      " 1 1 1 2 2 1 1 1 2 1 1 2 1 1 1 1 1 2 2 2 1 2 1 1 2 1 1 1 2 1 1 1 1 2 1 1 2\n",
      " 1 1 1 1 1 1 1 2 1 1 1 2 1 1 2 2 1 2 1 1 1 2 1 2 1 1 2 1 1 1 1 2 1 1 1 2 1\n",
      " 1 1 1 2 1 1 2 1 1 1 1 2 2 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 2 2\n",
      " 2 1 1 1 1 2 1 1 2 1 1 1 2 2 2 1 1 2 2 1 2 2 1 1 1 1 2 1 1 1 1 1 2 1 1 2 1\n",
      " 1 1 2 1 1 1 1 2 1 1 2 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 2 2 1 2\n",
      " 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 2 1 1 2 2 1 2 2 2 1 1 2 1 2\n",
      " 2 1 2 1 1 1 2 1 1 1 2 2 1 2 1 1 1 1 1 1 1 2 1 2 2 1 2 2 1 1 1 1 1 2 1 1 1\n",
      " 1 2 1 1 2 1 1 1 1 1 2 2 1 1 1 1 2 2 2 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2\n",
      " 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.973"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/german_clean.csv')\n",
    "x = data.iloc[:, :20]\n",
    "y = data.iloc[:, 20:].to_numpy().flatten()\n",
    "\n",
    "# 连续数据离散化\n",
    "continuous_columns = ['duration_new', 'credit_amount', 'age']\n",
    "transformer = FeatureDiscretization(continuous_columns=continuous_columns)\n",
    "x = transformer.fit(x, y)\n",
    "\n",
    "# 训练模型\n",
    "dt = DecisionTree(criterion='c45')\n",
    "dt.fit(x.to_numpy(), y)\n",
    "nodes = dt.level_order(return_node=True)  # 剪枝\n",
    "\n",
    "dt.score(x.to_numpy(), y)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
