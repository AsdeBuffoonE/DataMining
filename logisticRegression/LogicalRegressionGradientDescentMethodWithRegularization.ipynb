{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('data/telcoPreprocess.csv')\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, :41], data.iloc[:, 41:], test_size=0.3,\n",
    "                                                    random_state=42)\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# The sigmoid function\n",
    "def Sigmoid(z):\n",
    "    return float(1.0 / float((1.0 + np.exp(-1.0 * z))))\n",
    "\n",
    "\n",
    "def Hypothesis(theta, x):\n",
    "    z = 0\n",
    "    for i in range(len(theta)):\n",
    "        z += x[i] * theta[i]\n",
    "    return Sigmoid(z)\n",
    "\n",
    "\n",
    "def Cost_Function(X, Y, theta, m):\n",
    "    sumOfErrors = 0\n",
    "    for i in range(m):\n",
    "        xi = X[i]\n",
    "        hi = Hypothesis(theta, xi)\n",
    "        if Y[i] == 1:\n",
    "            error = Y[i] * np.log(hi)\n",
    "        elif Y[i] == 0:\n",
    "            error = (1 - Y[i]) * np.log(1 - hi)\n",
    "        sumOfErrors += error\n",
    "    const = -1 / m\n",
    "    J = const * sumOfErrors\n",
    "    print('cost is ', J)\n",
    "    return J\n",
    "\n",
    "\n",
    "def Cost_Function_Derivative(X, Y, theta, j, m, alpha, _lamda):\n",
    "    sumErrors = 0\n",
    "    for i in range(m):\n",
    "        xi = X[i]\n",
    "        xij = xi[j]\n",
    "        hi = Hypothesis(theta, X[i])\n",
    "        error = (hi - Y[i]) * xij\n",
    "        sumErrors += error\n",
    "    m = len(Y)\n",
    "    constant = float(alpha) / float(m) + (_lamda / float(m) * theta[j])\n",
    "    return constant * sumErrors\n",
    "\n",
    "\n",
    "def Gradient_Descent(X, Y, theta, m, alpha, _lamda):\n",
    "    new_theta = []\n",
    "    for j in range(len(theta)):\n",
    "        CFDerivative = Cost_Function_Derivative(X, Y, theta, j, m, alpha, _lamda)\n",
    "        new_theta_value = theta[j] - CFDerivative\n",
    "        new_theta.append(*new_theta_value)\n",
    "    return new_theta\n",
    "\n",
    "\n",
    "def Declare_Winner(theta):\n",
    "    score = 0\n",
    "    length = len(X_test)\n",
    "    for i in range(length):\n",
    "        prediction = round(Hypothesis(X_test[i], theta))\n",
    "        answer = y_test[i]\n",
    "        if prediction == answer:\n",
    "            score += 1\n",
    "\n",
    "    my_score = float(score) / float(length)\n",
    "    print('Your score: ', my_score)\n",
    "\n",
    "\n",
    "def Logistic_Regression(X, Y, alpha, theta, num_iters, _lamda=0.1):\n",
    "    m = len(Y)\n",
    "    r = np.diag([_lamda] * (X_train.shape[1] + 1))\n",
    "    r[0][0] = 0\n",
    "    for x in range(num_iters):\n",
    "        new_theta = Gradient_Descent(X, Y, theta, m, alpha, _lamda)\n",
    "        theta = new_theta\n",
    "        if x % 100 == 0:\n",
    "            Cost_Function(X, Y, theta, m)\n",
    "            print('theta ', theta)\n",
    "            print('cost is ', Cost_Function(X, Y, theta, m))\n",
    "    Declare_Winner(theta)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dp/wdk87p0d3ml1ysm634x3wlf80000gn/T/ipykernel_17609/1972323649.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  return float(1.0 / float((1.0 + np.exp(-1.0 * z))))\n",
      "/var/folders/dp/wdk87p0d3ml1ysm634x3wlf80000gn/T/ipykernel_17609/1972323649.py:19: RuntimeWarning: divide by zero encountered in log\n",
      "  error = Y[i] * np.log(hi)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost is  [inf]\n",
      "theta  [-0.045142857142857144, -1.105, -1.0264285714285715, -0.01092857142857143, -0.36300000000000004, -1.877857142857143, -0.044571428571428574, -0.3451428571428572, -0.002, -0.010571428571428572, -0.0485, -0.011000000000000001, -0.001142857142857143, -0.019642857142857146, -0.0038571428571428576, -0.35958571428571423, -0.3599107142857143, -0.07505357142857141, -0.3883571428571429, -0.16714285714285723, -20.57807142857143, -18.079353571428566, -8.047057142857144, -20.524482142857146, -10.32152500000001, -0.009928571428571429, -0.0033571428571428576, -0.0035714285714285718, -0.0018571428571428573, -0.011285714285714286, -0.012214285714285716, -0.011142857142857144, -0.01242857142857143, -0.0022142857142857146, -0.05704466221428569, -0.07078040642857164, -0.07749263599999988, -0.06055272549999999, -0.07749761564285705, -0.08730615400000002, -0.05335714285714286]\n",
      "cost is  [inf]\n",
      "cost is  [inf]\n",
      "Your score:  0.7533333333333333\n"
     ]
    }
   ],
   "source": [
    "initial_theta = np.zeros(41)\n",
    "alpha = 0.1\n",
    "iterations = 100\n",
    "Logistic_Regression(X_train, y_train, alpha, initial_theta, iterations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}